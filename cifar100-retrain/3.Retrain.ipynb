{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "847b3687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]\n",
      "torch: 2.6.0+cu124\n",
      "torchvision: 0.21.0+cu124\n",
      "Device: cuda\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os, math, random\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\" \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset, random_split, ConcatDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "print(\"python:\", os.sys.version.splitlines()[0])\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"torchvision:\", torchvision.__version__)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e9347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"./data\"\n",
    "IMG_SIZE = 224\n",
    "BATCH_M2 = 64\n",
    "NUM_WORKERS = 8\n",
    "PIN_MEMORY = True\n",
    "SELECTED_CSV = \"selected_low_confidence.csv\"\n",
    "VAL_RATIO = 0.10\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a6a3183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concat dataset length: 60000\n",
      "Loaded 22500 selected indices from selected_low_confidence.csv\n",
      "Selected subset size: 22500  -> train: 20250  val: 2250\n",
      "Train loader batches: 316  Val loader batches: 36\n"
     ]
    }
   ],
   "source": [
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_transform_m2 = T.Compose([\n",
    "    T.RandomResizedCrop(IMG_SIZE, scale=(0.08, 1.0), ratio=(0.75, 1.3333)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    # T.RandAugment(num_ops=2, magnitude=9),  # optional\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "\n",
    "train_ds_for_concat = datasets.CIFAR100(root=DATA_ROOT, train=True, download=False, transform=train_transform_m2)\n",
    "test_ds_for_concat  = datasets.CIFAR100(root=DATA_ROOT, train=False, download=False, transform=train_transform_m2)\n",
    "concat_aug = ConcatDataset([train_ds_for_concat, test_ds_for_concat])\n",
    "print(\"Concat dataset length:\", len(concat_aug))\n",
    "\n",
    "\n",
    "df_sel=pd.read_csv(SELECTED_CSV)\n",
    "sel_global_idx=df_sel[\"global_idx\"].astype(int).tolist()\n",
    "print(f\"Loaded {len(sel_global_idx)} selected indices from {SELECTED_CSV}\")\n",
    "\n",
    "selected_subset = Subset(concat_aug, sel_global_idx)\n",
    "n_total = len(selected_subset)\n",
    "n_val = max(1, int(round(VAL_RATIO * n_total)))\n",
    "n_train = n_total - n_val\n",
    "print(\"Selected subset size:\", n_total, \" -> train:\", n_train, \" val:\", n_val)\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "train_subset, val_subset = random_split(selected_subset, [n_train, n_val])\n",
    "\n",
    "train_loader_m2 = DataLoader(train_subset, batch_size=BATCH_M2, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=True)\n",
    "val_loader_m2   = DataLoader(val_subset,   batch_size=BATCH_M2, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "print(\"Train loader batches:\", len(train_loader_m2), \" Val loader batches:\", len(val_loader_m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87415504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/23ucc611/miniconda3/envs/ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and checkpoint: ./checkpoints_linear/best_checkpoint.pth\n",
      "Loaded model weights from checkpoint keys.\n"
     ]
    }
   ],
   "source": [
    "import os, time, math\n",
    "import numpy as np\n",
    "import torch, torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "\n",
    "MODEL_NAME = \"resnext101_32x8d\"\n",
    "CHECKPOINT_PATH = \"./checkpoints_linear/best_checkpoint.pth\"   # path to your original best model checkpoint\n",
    "M2_SAVE_DIR = \"m2_retrained_checkpoints\"\n",
    "os.makedirs(M2_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "M2_EPOCHS = 40\n",
    "USE_MIXUP_M2 = True\n",
    "MIXUP_ALPHA_M2 = 0.8\n",
    "BASE_LR = 0.01 * (BATCH_M2 / 256.0)\n",
    "MIN_LR = 1e-6\n",
    "WEIGHT_DECAY = 1e-4\n",
    "MOMENTUM = 0.9\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Loading model and checkpoint:\", CHECKPOINT_PATH)\n",
    "\n",
    "model = timm.create_model(MODEL_NAME, pretrained=False, num_classes=100)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "ck = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "\n",
    "if \"model_state\" in ck:\n",
    "    state_dict = ck[\"model_state\"]\n",
    "elif \"state_dict\" in ck:\n",
    "    state_dict = ck[\"state_dict\"]\n",
    "else:\n",
    "    try:\n",
    "        model.load_state_dict(ck)\n",
    "        print(\"Loaded raw state_dict from checkpoint.\")\n",
    "        state_dict = None\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Could not find model_state or state_dict in checkpoint. Inspect keys: \" + str(list(ck.keys())))\n",
    "\n",
    "\n",
    "if state_dict is not None:\n",
    "    new_state = {}\n",
    "    for k, v in state_dict.items():\n",
    "        new_k = k\n",
    "        if k.startswith(\"module.\"):\n",
    "            new_k = k[len(\"module.\"):]\n",
    "        new_state[new_k] = v\n",
    "    model.load_state_dict(new_state)\n",
    "    print(\"Loaded model weights from checkpoint keys.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6362f233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_57936/2057016899.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
      "/tmp/ipykernel_57936/2057016899.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 Batch 20/316 AvgLoss:3.0787 Time:11.4s LR:0.002500\n",
      "Epoch 1/40 Batch 40/316 AvgLoss:2.9974 Time:20.1s LR:0.002500\n",
      "Epoch 1/40 Batch 60/316 AvgLoss:2.9132 Time:28.7s LR:0.002500\n",
      "Epoch 1/40 Batch 80/316 AvgLoss:2.8637 Time:37.7s LR:0.002500\n",
      "Epoch 1/40 Batch 100/316 AvgLoss:2.7912 Time:46.5s LR:0.002500\n",
      "Epoch 1/40 Batch 120/316 AvgLoss:2.7663 Time:55.6s LR:0.002500\n",
      "Epoch 1/40 Batch 140/316 AvgLoss:2.7342 Time:64.2s LR:0.002500\n",
      "Epoch 1/40 Batch 160/316 AvgLoss:2.6946 Time:73.1s LR:0.002500\n",
      "Epoch 1/40 Batch 180/316 AvgLoss:2.7127 Time:82.1s LR:0.002500\n",
      "Epoch 1/40 Batch 200/316 AvgLoss:2.7076 Time:91.2s LR:0.002500\n",
      "Epoch 1/40 Batch 220/316 AvgLoss:2.6900 Time:100.0s LR:0.002500\n",
      "Epoch 1/40 Batch 240/316 AvgLoss:2.6599 Time:108.5s LR:0.002500\n",
      "Epoch 1/40 Batch 260/316 AvgLoss:2.6407 Time:117.1s LR:0.002500\n",
      "Epoch 1/40 Batch 280/316 AvgLoss:2.6165 Time:125.7s LR:0.002500\n",
      "Epoch 1/40 Batch 300/316 AvgLoss:2.6057 Time:134.4s LR:0.002500\n",
      "Epoch 1/40 Batch 316/316 AvgLoss:2.6091 Time:141.3s LR:0.002500\n",
      "Epoch 1 VALID -> Loss: 1.6122 Top1: 61.556 Top5: 81.956\n",
      "Saved new best_m2_retrained.pth (val_top1 improved to 61.556)\n",
      "Epoch 2/40 Batch 20/316 AvgLoss:2.5482 Time:10.1s LR:0.002496\n",
      "Epoch 2/40 Batch 40/316 AvgLoss:2.5239 Time:18.9s LR:0.002496\n",
      "Epoch 2/40 Batch 60/316 AvgLoss:2.4760 Time:27.5s LR:0.002496\n",
      "Epoch 2/40 Batch 80/316 AvgLoss:2.4940 Time:36.3s LR:0.002496\n",
      "Epoch 2/40 Batch 100/316 AvgLoss:2.4875 Time:45.5s LR:0.002496\n",
      "Epoch 2/40 Batch 120/316 AvgLoss:2.4557 Time:54.7s LR:0.002496\n",
      "Epoch 2/40 Batch 140/316 AvgLoss:2.4686 Time:63.6s LR:0.002496\n",
      "Epoch 2/40 Batch 160/316 AvgLoss:2.4821 Time:72.5s LR:0.002496\n",
      "Epoch 2/40 Batch 180/316 AvgLoss:2.4667 Time:81.3s LR:0.002496\n",
      "Epoch 2/40 Batch 200/316 AvgLoss:2.4514 Time:90.1s LR:0.002496\n",
      "Epoch 2/40 Batch 220/316 AvgLoss:2.4410 Time:98.8s LR:0.002496\n",
      "Epoch 2/40 Batch 240/316 AvgLoss:2.4422 Time:107.8s LR:0.002496\n",
      "Epoch 2/40 Batch 260/316 AvgLoss:2.4344 Time:116.8s LR:0.002496\n",
      "Epoch 2/40 Batch 280/316 AvgLoss:2.4166 Time:126.0s LR:0.002496\n",
      "Epoch 2/40 Batch 300/316 AvgLoss:2.4081 Time:134.8s LR:0.002496\n",
      "Epoch 2/40 Batch 316/316 AvgLoss:2.4077 Time:141.9s LR:0.002496\n",
      "Epoch 2 VALID -> Loss: 1.4948 Top1: 64.178 Top5: 83.867\n",
      "Saved new best_m2_retrained.pth (val_top1 improved to 64.178)\n",
      "Epoch 3/40 Batch 20/316 AvgLoss:2.3867 Time:10.1s LR:0.002484\n",
      "Epoch 3/40 Batch 40/316 AvgLoss:2.3670 Time:17.8s LR:0.002484\n",
      "Epoch 3/40 Batch 60/316 AvgLoss:2.4011 Time:26.9s LR:0.002484\n",
      "Epoch 3/40 Batch 80/316 AvgLoss:2.4174 Time:35.8s LR:0.002484\n",
      "Epoch 3/40 Batch 100/316 AvgLoss:2.4207 Time:44.4s LR:0.002484\n",
      "Epoch 3/40 Batch 120/316 AvgLoss:2.4117 Time:53.0s LR:0.002484\n",
      "Epoch 3/40 Batch 140/316 AvgLoss:2.3845 Time:61.7s LR:0.002484\n",
      "Epoch 3/40 Batch 160/316 AvgLoss:2.4060 Time:70.4s LR:0.002484\n",
      "Epoch 3/40 Batch 180/316 AvgLoss:2.3827 Time:79.2s LR:0.002484\n",
      "Epoch 3/40 Batch 200/316 AvgLoss:2.3825 Time:88.0s LR:0.002484\n",
      "Epoch 3/40 Batch 220/316 AvgLoss:2.3731 Time:96.8s LR:0.002484\n",
      "Epoch 3/40 Batch 240/316 AvgLoss:2.3754 Time:105.7s LR:0.002484\n",
      "Epoch 3/40 Batch 260/316 AvgLoss:2.3569 Time:114.5s LR:0.002484\n",
      "Epoch 3/40 Batch 280/316 AvgLoss:2.3605 Time:123.7s LR:0.002484\n",
      "Epoch 3/40 Batch 300/316 AvgLoss:2.3411 Time:132.3s LR:0.002484\n",
      "Epoch 3/40 Batch 316/316 AvgLoss:2.3456 Time:139.0s LR:0.002484\n",
      "Epoch 3 VALID -> Loss: 1.3528 Top1: 67.378 Top5: 86.756\n",
      "Saved new best_m2_retrained.pth (val_top1 improved to 67.378)\n",
      "Epoch 4/40 Batch 20/316 AvgLoss:2.2794 Time:9.8s LR:0.002464\n",
      "Epoch 4/40 Batch 40/316 AvgLoss:2.2715 Time:18.6s LR:0.002464\n",
      "Epoch 4/40 Batch 60/316 AvgLoss:2.2728 Time:27.0s LR:0.002464\n",
      "Epoch 4/40 Batch 80/316 AvgLoss:2.2980 Time:34.6s LR:0.002464\n",
      "Epoch 4/40 Batch 100/316 AvgLoss:2.3342 Time:42.7s LR:0.002464\n",
      "Epoch 4/40 Batch 120/316 AvgLoss:2.3492 Time:51.4s LR:0.002464\n",
      "Epoch 4/40 Batch 140/316 AvgLoss:2.3375 Time:60.7s LR:0.002464\n",
      "Epoch 4/40 Batch 160/316 AvgLoss:2.3066 Time:69.7s LR:0.002464\n",
      "Epoch 4/40 Batch 180/316 AvgLoss:2.2979 Time:78.9s LR:0.002464\n",
      "Epoch 4/40 Batch 200/316 AvgLoss:2.3192 Time:88.5s LR:0.002464\n",
      "Epoch 4/40 Batch 220/316 AvgLoss:2.3094 Time:97.7s LR:0.002464\n",
      "Epoch 4/40 Batch 240/316 AvgLoss:2.3078 Time:107.2s LR:0.002464\n",
      "Epoch 4/40 Batch 260/316 AvgLoss:2.2909 Time:116.3s LR:0.002464\n",
      "Epoch 4/40 Batch 280/316 AvgLoss:2.2821 Time:125.5s LR:0.002464\n",
      "Epoch 4/40 Batch 300/316 AvgLoss:2.2636 Time:134.5s LR:0.002464\n",
      "Epoch 4/40 Batch 316/316 AvgLoss:2.2465 Time:142.2s LR:0.002464\n",
      "Epoch 4 VALID -> Loss: 1.2656 Top1: 68.311 Top5: 87.956\n",
      "Saved new best_m2_retrained.pth (val_top1 improved to 68.311)\n",
      "Epoch 5/40 Batch 20/316 AvgLoss:2.3116 Time:11.2s LR:0.002436\n",
      "Epoch 5/40 Batch 40/316 AvgLoss:2.2732 Time:20.8s LR:0.002436\n",
      "Epoch 5/40 Batch 60/316 AvgLoss:2.2872 Time:30.3s LR:0.002436\n",
      "Epoch 5/40 Batch 80/316 AvgLoss:2.2389 Time:39.5s LR:0.002436\n",
      "Epoch 5/40 Batch 100/316 AvgLoss:2.2412 Time:48.7s LR:0.002436\n",
      "Epoch 5/40 Batch 120/316 AvgLoss:2.2356 Time:56.8s LR:0.002436\n",
      "Epoch 5/40 Batch 140/316 AvgLoss:2.2145 Time:66.3s LR:0.002436\n",
      "Epoch 5/40 Batch 160/316 AvgLoss:2.1552 Time:76.0s LR:0.002436\n",
      "Epoch 5/40 Batch 180/316 AvgLoss:2.1670 Time:85.3s LR:0.002436\n",
      "Epoch 5/40 Batch 200/316 AvgLoss:2.1694 Time:94.4s LR:0.002436\n",
      "Epoch 5/40 Batch 220/316 AvgLoss:2.1770 Time:103.8s LR:0.002436\n",
      "Epoch 5/40 Batch 240/316 AvgLoss:2.1789 Time:112.7s LR:0.002436\n",
      "Epoch 5/40 Batch 260/316 AvgLoss:2.1828 Time:121.9s LR:0.002436\n",
      "Epoch 5/40 Batch 280/316 AvgLoss:2.1970 Time:131.3s LR:0.002436\n",
      "Epoch 5/40 Batch 300/316 AvgLoss:2.2124 Time:140.7s LR:0.002436\n",
      "Epoch 5/40 Batch 316/316 AvgLoss:2.2008 Time:148.1s LR:0.002436\n",
      "Epoch 5 VALID -> Loss: 1.2407 Top1: 69.156 Top5: 87.822\n",
      "Saved new best_m2_retrained.pth (val_top1 improved to 69.156)\n",
      "Epoch 6/40 Batch 20/316 AvgLoss:2.3270 Time:10.7s LR:0.002400\n",
      "Epoch 6/40 Batch 40/316 AvgLoss:2.2391 Time:20.3s LR:0.002400\n",
      "Epoch 6/40 Batch 60/316 AvgLoss:2.2610 Time:29.8s LR:0.002400\n",
      "Epoch 6/40 Batch 80/316 AvgLoss:2.1950 Time:39.3s LR:0.002400\n",
      "Epoch 6/40 Batch 100/316 AvgLoss:2.2132 Time:48.9s LR:0.002400\n",
      "Epoch 6/40 Batch 120/316 AvgLoss:2.2077 Time:58.6s LR:0.002400\n",
      "Epoch 6/40 Batch 140/316 AvgLoss:2.2126 Time:67.9s LR:0.002400\n",
      "Epoch 6/40 Batch 160/316 AvgLoss:2.2188 Time:78.0s LR:0.002400\n",
      "Epoch 6/40 Batch 180/316 AvgLoss:2.1909 Time:88.4s LR:0.002400\n",
      "Epoch 6/40 Batch 200/316 AvgLoss:2.1844 Time:98.3s LR:0.002400\n",
      "Epoch 6/40 Batch 220/316 AvgLoss:2.1736 Time:106.2s LR:0.002400\n",
      "Epoch 6/40 Batch 240/316 AvgLoss:2.1710 Time:115.0s LR:0.002400\n",
      "Epoch 6/40 Batch 260/316 AvgLoss:2.1862 Time:124.1s LR:0.002400\n",
      "Epoch 6/40 Batch 280/316 AvgLoss:2.1695 Time:133.1s LR:0.002400\n",
      "Epoch 6/40 Batch 300/316 AvgLoss:2.1843 Time:142.1s LR:0.002400\n",
      "Epoch 6/40 Batch 316/316 AvgLoss:2.1813 Time:149.0s LR:0.002400\n",
      "Epoch 6 VALID -> Loss: 1.2502 Top1: 70.044 Top5: 87.733\n",
      "Saved new best_m2_retrained.pth (val_top1 improved to 70.044)\n",
      "Epoch 7/40 Batch 20/316 AvgLoss:2.1044 Time:10.4s LR:0.002357\n",
      "Epoch 7/40 Batch 40/316 AvgLoss:2.2548 Time:19.6s LR:0.002357\n",
      "Epoch 7/40 Batch 60/316 AvgLoss:2.2789 Time:28.6s LR:0.002357\n",
      "Epoch 7/40 Batch 80/316 AvgLoss:2.2476 Time:37.3s LR:0.002357\n",
      "Epoch 7/40 Batch 100/316 AvgLoss:2.2831 Time:45.8s LR:0.002357\n",
      "Epoch 7/40 Batch 120/316 AvgLoss:2.2718 Time:54.1s LR:0.002357\n",
      "Epoch 7/40 Batch 140/316 AvgLoss:2.2761 Time:62.8s LR:0.002357\n",
      "Epoch 7/40 Batch 160/316 AvgLoss:2.2596 Time:71.6s LR:0.002357\n",
      "Epoch 7/40 Batch 180/316 AvgLoss:2.2306 Time:80.4s LR:0.002357\n",
      "Epoch 7/40 Batch 200/316 AvgLoss:2.2449 Time:88.9s LR:0.002357\n",
      "Epoch 7/40 Batch 220/316 AvgLoss:2.2398 Time:97.3s LR:0.002357\n",
      "Epoch 7/40 Batch 240/316 AvgLoss:2.2476 Time:106.0s LR:0.002357\n",
      "Epoch 7/40 Batch 260/316 AvgLoss:2.2333 Time:113.4s LR:0.002357\n",
      "Epoch 7/40 Batch 280/316 AvgLoss:2.2379 Time:122.0s LR:0.002357\n",
      "Epoch 7/40 Batch 300/316 AvgLoss:2.2210 Time:130.4s LR:0.002357\n",
      "Epoch 7/40 Batch 316/316 AvgLoss:2.2190 Time:137.2s LR:0.002357\n",
      "Epoch 7 VALID -> Loss: 1.2515 Top1: 69.689 Top5: 87.956\n",
      "Epoch 8/40 Batch 20/316 AvgLoss:2.1655 Time:10.5s LR:0.002307\n",
      "Epoch 8/40 Batch 40/316 AvgLoss:2.0950 Time:19.3s LR:0.002307\n",
      "Epoch 8/40 Batch 60/316 AvgLoss:2.1252 Time:28.0s LR:0.002307\n",
      "Epoch 8/40 Batch 80/316 AvgLoss:2.1359 Time:36.8s LR:0.002307\n",
      "Epoch 8/40 Batch 100/316 AvgLoss:2.0985 Time:45.2s LR:0.002307\n",
      "Epoch 8/40 Batch 120/316 AvgLoss:2.0876 Time:54.0s LR:0.002307\n",
      "Epoch 8/40 Batch 140/316 AvgLoss:2.1190 Time:62.6s LR:0.002307\n",
      "Epoch 8/40 Batch 160/316 AvgLoss:2.1093 Time:71.1s LR:0.002307\n",
      "Epoch 8/40 Batch 180/316 AvgLoss:2.1042 Time:79.7s LR:0.002307\n",
      "Epoch 8/40 Batch 200/316 AvgLoss:2.1208 Time:88.2s LR:0.002307\n",
      "Epoch 8/40 Batch 220/316 AvgLoss:2.1008 Time:96.6s LR:0.002307\n",
      "Epoch 8/40 Batch 240/316 AvgLoss:2.0942 Time:105.4s LR:0.002307\n",
      "Epoch 8/40 Batch 260/316 AvgLoss:2.1068 Time:114.1s LR:0.002307\n",
      "Epoch 8/40 Batch 280/316 AvgLoss:2.1177 Time:122.6s LR:0.002307\n",
      "Epoch 8/40 Batch 300/316 AvgLoss:2.1114 Time:131.4s LR:0.002307\n",
      "Epoch 8/40 Batch 316/316 AvgLoss:2.1147 Time:137.0s LR:0.002307\n",
      "Epoch 8 VALID -> Loss: 1.2482 Top1: 69.689 Top5: 88.356\n",
      "Epoch 9/40 Batch 20/316 AvgLoss:2.1241 Time:10.1s LR:0.002249\n",
      "Epoch 9/40 Batch 40/316 AvgLoss:2.0614 Time:18.8s LR:0.002249\n",
      "Epoch 9/40 Batch 60/316 AvgLoss:2.0152 Time:27.5s LR:0.002249\n",
      "Epoch 9/40 Batch 80/316 AvgLoss:2.0608 Time:35.9s LR:0.002249\n",
      "Epoch 9/40 Batch 100/316 AvgLoss:2.0628 Time:44.5s LR:0.002249\n",
      "Epoch 9/40 Batch 120/316 AvgLoss:2.1011 Time:52.9s LR:0.002249\n",
      "Epoch 9/40 Batch 140/316 AvgLoss:2.1456 Time:61.2s LR:0.002249\n",
      "Epoch 9/40 Batch 160/316 AvgLoss:2.1449 Time:69.9s LR:0.002249\n",
      "Epoch 9/40 Batch 180/316 AvgLoss:2.1502 Time:78.9s LR:0.002249\n",
      "Epoch 9/40 Batch 200/316 AvgLoss:2.1556 Time:87.8s LR:0.002249\n",
      "Epoch 9/40 Batch 220/316 AvgLoss:2.1522 Time:96.5s LR:0.002249\n",
      "Epoch 9/40 Batch 240/316 AvgLoss:2.1492 Time:105.1s LR:0.002249\n",
      "Epoch 9/40 Batch 260/316 AvgLoss:2.1463 Time:113.6s LR:0.002249\n",
      "Epoch 9/40 Batch 280/316 AvgLoss:2.1594 Time:122.3s LR:0.002249\n",
      "Epoch 9/40 Batch 300/316 AvgLoss:2.1536 Time:131.3s LR:0.002249\n",
      "Epoch 9/40 Batch 316/316 AvgLoss:2.1692 Time:138.2s LR:0.002249\n",
      "Epoch 9 VALID -> Loss: 1.2647 Top1: 70.000 Top5: 88.267\n",
      "Epoch 10/40 Batch 20/316 AvgLoss:2.0296 Time:8.8s LR:0.002186\n",
      "Epoch 10/40 Batch 40/316 AvgLoss:2.1470 Time:17.6s LR:0.002186\n",
      "Epoch 10/40 Batch 60/316 AvgLoss:2.0611 Time:26.5s LR:0.002186\n",
      "Epoch 10/40 Batch 80/316 AvgLoss:2.0351 Time:35.0s LR:0.002186\n",
      "Epoch 10/40 Batch 100/316 AvgLoss:2.0367 Time:44.0s LR:0.002186\n",
      "Epoch 10/40 Batch 120/316 AvgLoss:2.0866 Time:53.0s LR:0.002186\n",
      "Epoch 10/40 Batch 140/316 AvgLoss:2.1181 Time:61.8s LR:0.002186\n",
      "Epoch 10/40 Batch 160/316 AvgLoss:2.1105 Time:70.2s LR:0.002186\n",
      "Epoch 10/40 Batch 180/316 AvgLoss:2.0897 Time:79.0s LR:0.002186\n",
      "Epoch 10/40 Batch 200/316 AvgLoss:2.0886 Time:87.6s LR:0.002186\n",
      "Epoch 10/40 Batch 220/316 AvgLoss:2.0787 Time:96.1s LR:0.002186\n",
      "Epoch 10/40 Batch 240/316 AvgLoss:2.0841 Time:104.4s LR:0.002186\n",
      "Epoch 10/40 Batch 260/316 AvgLoss:2.0634 Time:113.3s LR:0.002186\n",
      "Epoch 10/40 Batch 280/316 AvgLoss:2.0604 Time:122.0s LR:0.002186\n",
      "Epoch 10/40 Batch 300/316 AvgLoss:2.0798 Time:130.9s LR:0.002186\n",
      "Epoch 10/40 Batch 316/316 AvgLoss:2.0787 Time:138.4s LR:0.002186\n",
      "Epoch 10 VALID -> Loss: 1.1775 Top1: 71.067 Top5: 89.289\n",
      "Saved new best_m2_retrained.pth (val_top1 improved to 71.067)\n",
      "Epoch 11/40 Batch 20/316 AvgLoss:2.1248 Time:10.2s LR:0.002116\n",
      "Epoch 11/40 Batch 40/316 AvgLoss:1.9513 Time:19.3s LR:0.002116\n",
      "Epoch 11/40 Batch 60/316 AvgLoss:1.9922 Time:27.1s LR:0.002116\n",
      "Epoch 11/40 Batch 80/316 AvgLoss:2.0556 Time:36.1s LR:0.002116\n",
      "Epoch 11/40 Batch 100/316 AvgLoss:2.0563 Time:45.4s LR:0.002116\n",
      "Epoch 11/40 Batch 120/316 AvgLoss:2.0429 Time:54.4s LR:0.002116\n",
      "Epoch 11/40 Batch 140/316 AvgLoss:2.0409 Time:64.0s LR:0.002116\n",
      "Epoch 11/40 Batch 160/316 AvgLoss:2.0419 Time:73.2s LR:0.002116\n",
      "Epoch 11/40 Batch 180/316 AvgLoss:2.0133 Time:82.4s LR:0.002116\n",
      "Epoch 11/40 Batch 200/316 AvgLoss:2.0273 Time:91.3s LR:0.002116\n",
      "Epoch 11/40 Batch 220/316 AvgLoss:2.0208 Time:100.2s LR:0.002116\n",
      "Epoch 11/40 Batch 240/316 AvgLoss:2.0208 Time:109.3s LR:0.002116\n",
      "Epoch 11/40 Batch 260/316 AvgLoss:2.0170 Time:118.3s LR:0.002116\n",
      "Epoch 11/40 Batch 280/316 AvgLoss:2.0376 Time:127.1s LR:0.002116\n",
      "Epoch 11/40 Batch 300/316 AvgLoss:2.0546 Time:136.0s LR:0.002116\n",
      "Epoch 11/40 Batch 316/316 AvgLoss:2.0594 Time:143.5s LR:0.002116\n",
      "Epoch 11 VALID -> Loss: 1.2105 Top1: 70.933 Top5: 88.711\n",
      "Epoch 12/40 Batch 20/316 AvgLoss:1.9552 Time:10.8s LR:0.002041\n",
      "Epoch 12/40 Batch 40/316 AvgLoss:1.9785 Time:20.3s LR:0.002041\n",
      "Epoch 12/40 Batch 60/316 AvgLoss:2.0388 Time:30.0s LR:0.002041\n",
      "Epoch 12/40 Batch 80/316 AvgLoss:2.0521 Time:39.3s LR:0.002041\n",
      "Epoch 12/40 Batch 100/316 AvgLoss:2.0507 Time:48.4s LR:0.002041\n",
      "Epoch 12/40 Batch 120/316 AvgLoss:2.0814 Time:58.8s LR:0.002041\n",
      "Epoch 12/40 Batch 140/316 AvgLoss:2.0699 Time:69.3s LR:0.002041\n",
      "Epoch 12/40 Batch 160/316 AvgLoss:2.0938 Time:80.0s LR:0.002041\n",
      "Epoch 12/40 Batch 180/316 AvgLoss:2.0844 Time:87.9s LR:0.002041\n",
      "Epoch 12/40 Batch 200/316 AvgLoss:2.0490 Time:97.4s LR:0.002041\n",
      "Epoch 12/40 Batch 220/316 AvgLoss:2.0504 Time:106.8s LR:0.002041\n",
      "Epoch 12/40 Batch 240/316 AvgLoss:2.0520 Time:116.2s LR:0.002041\n",
      "Epoch 12/40 Batch 260/316 AvgLoss:2.0455 Time:125.8s LR:0.002041\n",
      "Epoch 12/40 Batch 280/316 AvgLoss:2.0412 Time:135.4s LR:0.002041\n",
      "Epoch 12/40 Batch 300/316 AvgLoss:2.0387 Time:145.1s LR:0.002041\n",
      "Epoch 12/40 Batch 316/316 AvgLoss:2.0427 Time:152.5s LR:0.002041\n",
      "Epoch 12 VALID -> Loss: 1.2237 Top1: 69.867 Top5: 88.844\n",
      "Epoch 13/40 Batch 20/316 AvgLoss:2.1386 Time:10.4s LR:0.001960\n",
      "Epoch 13/40 Batch 40/316 AvgLoss:1.8546 Time:19.1s LR:0.001960\n",
      "Epoch 13/40 Batch 60/316 AvgLoss:1.9820 Time:27.6s LR:0.001960\n",
      "Epoch 13/40 Batch 80/316 AvgLoss:1.9504 Time:36.3s LR:0.001960\n",
      "Epoch 13/40 Batch 100/316 AvgLoss:1.9268 Time:45.0s LR:0.001960\n",
      "Epoch 13/40 Batch 120/316 AvgLoss:1.9664 Time:53.6s LR:0.001960\n",
      "Epoch 13/40 Batch 140/316 AvgLoss:1.9949 Time:62.4s LR:0.001960\n",
      "Epoch 13/40 Batch 160/316 AvgLoss:1.9819 Time:71.3s LR:0.001960\n",
      "Epoch 13/40 Batch 180/316 AvgLoss:1.9671 Time:79.6s LR:0.001960\n",
      "Epoch 13/40 Batch 200/316 AvgLoss:1.9586 Time:88.2s LR:0.001960\n",
      "Epoch 13/40 Batch 220/316 AvgLoss:1.9651 Time:95.7s LR:0.001960\n",
      "Epoch 13/40 Batch 240/316 AvgLoss:1.9752 Time:104.5s LR:0.001960\n",
      "Epoch 13/40 Batch 260/316 AvgLoss:1.9748 Time:113.1s LR:0.001960\n",
      "Epoch 13/40 Batch 280/316 AvgLoss:1.9914 Time:121.8s LR:0.001960\n",
      "Epoch 13/40 Batch 300/316 AvgLoss:1.9994 Time:130.4s LR:0.001960\n",
      "Epoch 13/40 Batch 316/316 AvgLoss:2.0030 Time:137.4s LR:0.001960\n",
      "Epoch 13 VALID -> Loss: 1.2309 Top1: 70.222 Top5: 88.578\n",
      "Epoch 14/40 Batch 20/316 AvgLoss:2.1721 Time:10.0s LR:0.001875\n",
      "Epoch 14/40 Batch 40/316 AvgLoss:1.9799 Time:18.6s LR:0.001875\n",
      "Epoch 14/40 Batch 60/316 AvgLoss:2.0080 Time:27.2s LR:0.001875\n",
      "Epoch 14/40 Batch 80/316 AvgLoss:1.9795 Time:35.8s LR:0.001875\n",
      "Epoch 14/40 Batch 100/316 AvgLoss:1.9849 Time:44.6s LR:0.001875\n",
      "Epoch 14/40 Batch 120/316 AvgLoss:1.9569 Time:53.1s LR:0.001875\n",
      "Epoch 14/40 Batch 140/316 AvgLoss:1.9642 Time:61.7s LR:0.001875\n",
      "Epoch 14/40 Batch 160/316 AvgLoss:1.9791 Time:70.2s LR:0.001875\n",
      "Epoch 14/40 Batch 180/316 AvgLoss:1.9725 Time:78.9s LR:0.001875\n",
      "Epoch 14/40 Batch 200/316 AvgLoss:1.9950 Time:87.8s LR:0.001875\n",
      "Epoch 14/40 Batch 220/316 AvgLoss:1.9906 Time:96.3s LR:0.001875\n",
      "Epoch 14/40 Batch 240/316 AvgLoss:1.9858 Time:104.8s LR:0.001875\n",
      "Epoch 14/40 Batch 260/316 AvgLoss:1.9999 Time:113.5s LR:0.001875\n",
      "Epoch 14/40 Batch 280/316 AvgLoss:2.0028 Time:121.8s LR:0.001875\n",
      "Epoch 14/40 Batch 300/316 AvgLoss:1.9935 Time:130.5s LR:0.001875\n",
      "Epoch 14/40 Batch 316/316 AvgLoss:2.0070 Time:137.4s LR:0.001875\n",
      "Epoch 14 VALID -> Loss: 1.2071 Top1: 71.111 Top5: 90.000\n",
      "Saved new best_m2_retrained.pth (val_top1 improved to 71.111)\n",
      "Epoch 15/40 Batch 20/316 AvgLoss:2.0434 Time:10.7s LR:0.001786\n",
      "Epoch 15/40 Batch 40/316 AvgLoss:2.1145 Time:19.3s LR:0.001786\n",
      "Epoch 15/40 Batch 60/316 AvgLoss:2.1188 Time:28.0s LR:0.001786\n",
      "Epoch 15/40 Batch 80/316 AvgLoss:2.0630 Time:36.4s LR:0.001786\n",
      "Epoch 15/40 Batch 100/316 AvgLoss:2.0571 Time:45.1s LR:0.001786\n",
      "Epoch 15/40 Batch 120/316 AvgLoss:2.0787 Time:53.6s LR:0.001786\n",
      "Epoch 15/40 Batch 140/316 AvgLoss:2.0580 Time:62.0s LR:0.001786\n",
      "Epoch 15/40 Batch 160/316 AvgLoss:2.0625 Time:70.6s LR:0.001786\n",
      "Epoch 15/40 Batch 180/316 AvgLoss:2.0591 Time:79.6s LR:0.001786\n",
      "Epoch 15/40 Batch 200/316 AvgLoss:2.0724 Time:88.4s LR:0.001786\n",
      "Epoch 15/40 Batch 220/316 AvgLoss:2.0623 Time:97.2s LR:0.001786\n",
      "Epoch 15/40 Batch 240/316 AvgLoss:2.0731 Time:105.9s LR:0.001786\n",
      "Epoch 15/40 Batch 260/316 AvgLoss:2.0839 Time:114.6s LR:0.001786\n",
      "Epoch 15/40 Batch 280/316 AvgLoss:2.0741 Time:123.3s LR:0.001786\n",
      "Epoch 15/40 Batch 300/316 AvgLoss:2.0869 Time:132.0s LR:0.001786\n",
      "Epoch 15/40 Batch 316/316 AvgLoss:2.0830 Time:138.0s LR:0.001786\n",
      "Epoch 15 VALID -> Loss: 1.1943 Top1: 72.133 Top5: 88.978\n",
      "Saved new best_m2_retrained.pth (val_top1 improved to 72.133)\n",
      "Epoch 16/40 Batch 20/316 AvgLoss:1.8339 Time:10.1s LR:0.001694\n",
      "Epoch 16/40 Batch 40/316 AvgLoss:1.8315 Time:19.1s LR:0.001694\n",
      "Epoch 16/40 Batch 60/316 AvgLoss:2.0076 Time:27.8s LR:0.001694\n",
      "Epoch 16/40 Batch 80/316 AvgLoss:2.0269 Time:36.5s LR:0.001694\n",
      "Epoch 16/40 Batch 100/316 AvgLoss:1.9865 Time:45.2s LR:0.001694\n",
      "Epoch 16/40 Batch 120/316 AvgLoss:2.0086 Time:54.0s LR:0.001694\n",
      "Epoch 16/40 Batch 140/316 AvgLoss:2.0317 Time:62.7s LR:0.001694\n",
      "Epoch 16/40 Batch 160/316 AvgLoss:2.0195 Time:71.6s LR:0.001694\n",
      "Epoch 16/40 Batch 180/316 AvgLoss:2.0261 Time:80.2s LR:0.001694\n",
      "Epoch 16/40 Batch 200/316 AvgLoss:2.0097 Time:88.9s LR:0.001694\n",
      "Epoch 16/40 Batch 220/316 AvgLoss:2.0040 Time:97.8s LR:0.001694\n",
      "Epoch 16/40 Batch 240/316 AvgLoss:1.9846 Time:106.1s LR:0.001694\n",
      "Epoch 16/40 Batch 260/316 AvgLoss:1.9643 Time:114.7s LR:0.001694\n",
      "Epoch 16/40 Batch 280/316 AvgLoss:1.9554 Time:123.4s LR:0.001694\n",
      "Epoch 16/40 Batch 300/316 AvgLoss:1.9700 Time:131.9s LR:0.001694\n",
      "Epoch 16/40 Batch 316/316 AvgLoss:1.9741 Time:138.5s LR:0.001694\n",
      "Epoch 16 VALID -> Loss: 1.1175 Top1: 74.044 Top5: 91.111\n",
      "Saved new best_m2_retrained.pth (val_top1 improved to 74.044)\n",
      "Epoch 17/40 Batch 20/316 AvgLoss:2.1243 Time:8.7s LR:0.001598\n",
      "Epoch 17/40 Batch 40/316 AvgLoss:1.9335 Time:17.4s LR:0.001598\n",
      "Epoch 17/40 Batch 60/316 AvgLoss:1.9214 Time:26.6s LR:0.001598\n",
      "Epoch 17/40 Batch 80/316 AvgLoss:1.9383 Time:35.3s LR:0.001598\n",
      "Epoch 17/40 Batch 100/316 AvgLoss:2.0255 Time:44.7s LR:0.001598\n",
      "Epoch 17/40 Batch 120/316 AvgLoss:2.0332 Time:53.7s LR:0.001598\n",
      "Epoch 17/40 Batch 140/316 AvgLoss:1.9840 Time:62.8s LR:0.001598\n",
      "Epoch 17/40 Batch 160/316 AvgLoss:2.0137 Time:72.0s LR:0.001598\n",
      "Epoch 17/40 Batch 180/316 AvgLoss:2.0177 Time:80.9s LR:0.001598\n",
      "Epoch 17/40 Batch 200/316 AvgLoss:2.0244 Time:90.2s LR:0.001598\n",
      "Epoch 17/40 Batch 220/316 AvgLoss:2.0119 Time:99.7s LR:0.001598\n",
      "Epoch 17/40 Batch 240/316 AvgLoss:2.0082 Time:109.1s LR:0.001598\n",
      "Epoch 17/40 Batch 260/316 AvgLoss:1.9905 Time:118.3s LR:0.001598\n",
      "Epoch 17/40 Batch 280/316 AvgLoss:1.9801 Time:127.6s LR:0.001598\n",
      "Epoch 17/40 Batch 300/316 AvgLoss:2.0041 Time:136.8s LR:0.001598\n",
      "Epoch 17/40 Batch 316/316 AvgLoss:2.0001 Time:144.1s LR:0.001598\n",
      "Epoch 17 VALID -> Loss: 1.1508 Top1: 72.578 Top5: 89.689\n",
      "Epoch 18/40 Batch 20/316 AvgLoss:2.1023 Time:10.9s LR:0.001500\n",
      "Epoch 18/40 Batch 40/316 AvgLoss:2.0413 Time:20.0s LR:0.001500\n",
      "Epoch 18/40 Batch 60/316 AvgLoss:1.9803 Time:29.3s LR:0.001500\n",
      "Epoch 18/40 Batch 80/316 AvgLoss:1.9434 Time:39.6s LR:0.001500\n",
      "Epoch 18/40 Batch 100/316 AvgLoss:1.9528 Time:50.1s LR:0.001500\n",
      "Epoch 18/40 Batch 120/316 AvgLoss:1.9756 Time:60.4s LR:0.001500\n",
      "Epoch 18/40 Batch 140/316 AvgLoss:1.9713 Time:67.8s LR:0.001500\n",
      "Epoch 18/40 Batch 160/316 AvgLoss:1.9557 Time:77.0s LR:0.001500\n",
      "Epoch 18/40 Batch 180/316 AvgLoss:1.9336 Time:86.4s LR:0.001500\n",
      "Epoch 18/40 Batch 200/316 AvgLoss:1.9293 Time:95.3s LR:0.001500\n",
      "Epoch 18/40 Batch 220/316 AvgLoss:1.9412 Time:104.1s LR:0.001500\n",
      "Epoch 18/40 Batch 240/316 AvgLoss:1.9384 Time:113.3s LR:0.001500\n",
      "Epoch 18/40 Batch 260/316 AvgLoss:1.9291 Time:122.4s LR:0.001500\n",
      "Epoch 18/40 Batch 280/316 AvgLoss:1.9491 Time:131.7s LR:0.001500\n",
      "Epoch 18/40 Batch 300/316 AvgLoss:1.9302 Time:140.9s LR:0.001500\n",
      "Epoch 18/40 Batch 316/316 AvgLoss:1.9316 Time:148.3s LR:0.001500\n",
      "Epoch 18 VALID -> Loss: 1.1910 Top1: 71.156 Top5: 89.689\n",
      "Epoch 19/40 Batch 20/316 AvgLoss:1.7468 Time:10.3s LR:0.001401\n",
      "Epoch 19/40 Batch 40/316 AvgLoss:2.0064 Time:19.3s LR:0.001401\n",
      "Epoch 19/40 Batch 60/316 AvgLoss:2.0740 Time:28.6s LR:0.001401\n",
      "Epoch 19/40 Batch 80/316 AvgLoss:2.0430 Time:37.6s LR:0.001401\n",
      "Epoch 19/40 Batch 100/316 AvgLoss:2.0291 Time:46.3s LR:0.001401\n",
      "Epoch 19/40 Batch 120/316 AvgLoss:2.0152 Time:55.1s LR:0.001401\n",
      "Epoch 19/40 Batch 140/316 AvgLoss:1.9662 Time:63.8s LR:0.001401\n",
      "Epoch 19/40 Batch 160/316 AvgLoss:1.9871 Time:72.8s LR:0.001401\n",
      "Epoch 19/40 Batch 180/316 AvgLoss:2.0153 Time:80.1s LR:0.001401\n",
      "Epoch 19/40 Batch 200/316 AvgLoss:2.0057 Time:88.8s LR:0.001401\n",
      "Epoch 19/40 Batch 220/316 AvgLoss:1.9839 Time:97.4s LR:0.001401\n",
      "Epoch 19/40 Batch 240/316 AvgLoss:1.9902 Time:106.3s LR:0.001401\n",
      "Epoch 19/40 Batch 260/316 AvgLoss:1.9973 Time:115.0s LR:0.001401\n",
      "Epoch 19/40 Batch 280/316 AvgLoss:1.9917 Time:123.7s LR:0.001401\n",
      "Epoch 19/40 Batch 300/316 AvgLoss:1.9766 Time:132.3s LR:0.001401\n",
      "Epoch 19/40 Batch 316/316 AvgLoss:1.9729 Time:139.3s LR:0.001401\n",
      "Epoch 19 VALID -> Loss: 1.1355 Top1: 73.289 Top5: 89.600\n",
      "Epoch 20/40 Batch 20/316 AvgLoss:2.0130 Time:9.8s LR:0.001301\n",
      "Epoch 20/40 Batch 40/316 AvgLoss:1.9309 Time:18.5s LR:0.001301\n",
      "Epoch 20/40 Batch 60/316 AvgLoss:1.9998 Time:27.3s LR:0.001301\n",
      "Epoch 20/40 Batch 80/316 AvgLoss:2.0057 Time:35.8s LR:0.001301\n",
      "Epoch 20/40 Batch 100/316 AvgLoss:1.9173 Time:44.4s LR:0.001301\n",
      "Epoch 20/40 Batch 120/316 AvgLoss:1.9432 Time:53.2s LR:0.001301\n",
      "Epoch 20/40 Batch 140/316 AvgLoss:1.9628 Time:62.1s LR:0.001301\n",
      "Epoch 20/40 Batch 160/316 AvgLoss:1.9606 Time:70.8s LR:0.001301\n",
      "Epoch 20/40 Batch 180/316 AvgLoss:1.9600 Time:79.3s LR:0.001301\n",
      "Epoch 20/40 Batch 200/316 AvgLoss:1.9404 Time:88.2s LR:0.001301\n",
      "Epoch 20/40 Batch 220/316 AvgLoss:1.9624 Time:96.7s LR:0.001301\n",
      "Epoch 20/40 Batch 240/316 AvgLoss:1.9600 Time:104.3s LR:0.001301\n",
      "Epoch 20/40 Batch 260/316 AvgLoss:1.9581 Time:113.1s LR:0.001301\n",
      "Epoch 20/40 Batch 280/316 AvgLoss:1.9474 Time:121.7s LR:0.001301\n",
      "Epoch 20/40 Batch 300/316 AvgLoss:1.9634 Time:130.4s LR:0.001301\n",
      "Epoch 20/40 Batch 316/316 AvgLoss:1.9672 Time:137.3s LR:0.001301\n",
      "Epoch 20 VALID -> Loss: 1.1315 Top1: 73.644 Top5: 90.000\n",
      "Epoch 21/40 Batch 20/316 AvgLoss:1.8869 Time:10.0s LR:0.001200\n",
      "Epoch 21/40 Batch 40/316 AvgLoss:1.8173 Time:18.6s LR:0.001200\n",
      "Epoch 21/40 Batch 60/316 AvgLoss:1.9732 Time:27.6s LR:0.001200\n",
      "Epoch 21/40 Batch 80/316 AvgLoss:1.9696 Time:36.5s LR:0.001200\n",
      "Epoch 21/40 Batch 100/316 AvgLoss:2.0022 Time:45.0s LR:0.001200\n",
      "Epoch 21/40 Batch 120/316 AvgLoss:2.0301 Time:53.5s LR:0.001200\n",
      "Epoch 21/40 Batch 140/316 AvgLoss:2.0336 Time:62.1s LR:0.001200\n",
      "Epoch 21/40 Batch 160/316 AvgLoss:2.0135 Time:70.7s LR:0.001200\n",
      "Epoch 21/40 Batch 180/316 AvgLoss:1.9956 Time:79.3s LR:0.001200\n",
      "Epoch 21/40 Batch 200/316 AvgLoss:1.9749 Time:88.0s LR:0.001200\n",
      "Epoch 21/40 Batch 220/316 AvgLoss:1.9970 Time:97.0s LR:0.001200\n",
      "Epoch 21/40 Batch 240/316 AvgLoss:1.9817 Time:105.7s LR:0.001200\n",
      "Epoch 21/40 Batch 260/316 AvgLoss:1.9776 Time:114.2s LR:0.001200\n",
      "Epoch 21/40 Batch 280/316 AvgLoss:1.9815 Time:123.1s LR:0.001200\n",
      "Epoch 21/40 Batch 300/316 AvgLoss:1.9739 Time:130.4s LR:0.001200\n",
      "Epoch 21/40 Batch 316/316 AvgLoss:1.9806 Time:137.2s LR:0.001200\n",
      "Epoch 21 VALID -> Loss: 1.1780 Top1: 71.511 Top5: 89.333\n",
      "Epoch 22/40 Batch 20/316 AvgLoss:1.8200 Time:10.7s LR:0.001100\n",
      "Epoch 22/40 Batch 40/316 AvgLoss:1.9402 Time:19.3s LR:0.001100\n",
      "Epoch 22/40 Batch 60/316 AvgLoss:1.9879 Time:27.9s LR:0.001100\n",
      "Epoch 22/40 Batch 80/316 AvgLoss:1.9023 Time:36.7s LR:0.001100\n",
      "Epoch 22/40 Batch 100/316 AvgLoss:1.9281 Time:45.2s LR:0.001100\n",
      "Epoch 22/40 Batch 120/316 AvgLoss:1.9292 Time:54.0s LR:0.001100\n",
      "Epoch 22/40 Batch 140/316 AvgLoss:1.9229 Time:62.7s LR:0.001100\n",
      "Epoch 22/40 Batch 160/316 AvgLoss:1.9401 Time:71.3s LR:0.001100\n",
      "Epoch 22/40 Batch 180/316 AvgLoss:1.9570 Time:79.8s LR:0.001100\n",
      "Epoch 22/40 Batch 200/316 AvgLoss:1.9308 Time:88.5s LR:0.001100\n",
      "Epoch 22/40 Batch 220/316 AvgLoss:1.9520 Time:97.1s LR:0.001100\n",
      "Epoch 22/40 Batch 240/316 AvgLoss:1.9885 Time:105.7s LR:0.001100\n",
      "Epoch 22/40 Batch 260/316 AvgLoss:1.9778 Time:114.4s LR:0.001100\n",
      "Epoch 22/40 Batch 280/316 AvgLoss:1.9775 Time:122.7s LR:0.001100\n",
      "Epoch 22/40 Batch 300/316 AvgLoss:1.9684 Time:131.4s LR:0.001100\n",
      "Epoch 22/40 Batch 316/316 AvgLoss:1.9636 Time:138.6s LR:0.001100\n",
      "Epoch 22 VALID -> Loss: 1.1464 Top1: 72.489 Top5: 89.378\n",
      "Epoch 23/40 Batch 20/316 AvgLoss:1.8898 Time:9.9s LR:0.001001\n",
      "Epoch 23/40 Batch 40/316 AvgLoss:1.8994 Time:18.6s LR:0.001001\n",
      "Epoch 23/40 Batch 60/316 AvgLoss:1.9141 Time:27.2s LR:0.001001\n",
      "Epoch 23/40 Batch 80/316 AvgLoss:1.9303 Time:36.0s LR:0.001001\n",
      "Epoch 23/40 Batch 100/316 AvgLoss:1.8889 Time:44.7s LR:0.001001\n",
      "Epoch 23/40 Batch 120/316 AvgLoss:1.9296 Time:53.4s LR:0.001001\n",
      "Epoch 23/40 Batch 140/316 AvgLoss:1.9380 Time:62.0s LR:0.001001\n",
      "Epoch 23/40 Batch 160/316 AvgLoss:1.9485 Time:70.5s LR:0.001001\n",
      "Epoch 23/40 Batch 180/316 AvgLoss:1.9458 Time:79.1s LR:0.001001\n",
      "Epoch 23/40 Batch 200/316 AvgLoss:1.9518 Time:87.1s LR:0.001001\n",
      "Epoch 23/40 Batch 220/316 AvgLoss:1.9424 Time:95.6s LR:0.001001\n",
      "Epoch 23/40 Batch 240/316 AvgLoss:1.9408 Time:104.4s LR:0.001001\n",
      "Epoch 23/40 Batch 260/316 AvgLoss:1.9277 Time:113.5s LR:0.001001\n",
      "Epoch 23/40 Batch 280/316 AvgLoss:1.8985 Time:122.2s LR:0.001001\n",
      "Epoch 23/40 Batch 300/316 AvgLoss:1.9216 Time:131.3s LR:0.001001\n",
      "Epoch 23/40 Batch 316/316 AvgLoss:1.9325 Time:138.6s LR:0.001001\n",
      "Epoch 23 VALID -> Loss: 1.2054 Top1: 71.556 Top5: 89.067\n",
      "Epoch 24/40 Batch 20/316 AvgLoss:1.9159 Time:10.7s LR:0.000903\n",
      "Epoch 24/40 Batch 40/316 AvgLoss:1.9769 Time:19.4s LR:0.000903\n",
      "Epoch 24/40 Batch 60/316 AvgLoss:2.0165 Time:29.8s LR:0.000903\n",
      "Epoch 24/40 Batch 80/316 AvgLoss:2.0936 Time:40.3s LR:0.000903\n",
      "Epoch 24/40 Batch 100/316 AvgLoss:2.1073 Time:51.1s LR:0.000903\n",
      "Epoch 24/40 Batch 120/316 AvgLoss:2.0793 Time:59.6s LR:0.000903\n",
      "Epoch 24/40 Batch 140/316 AvgLoss:2.0623 Time:68.4s LR:0.000903\n",
      "Epoch 24/40 Batch 160/316 AvgLoss:2.0594 Time:76.9s LR:0.000903\n",
      "Epoch 24/40 Batch 180/316 AvgLoss:2.0635 Time:85.6s LR:0.000903\n",
      "Epoch 24/40 Batch 200/316 AvgLoss:2.0613 Time:95.0s LR:0.000903\n",
      "Epoch 24/40 Batch 220/316 AvgLoss:2.0502 Time:103.9s LR:0.000903\n",
      "Epoch 24/40 Batch 240/316 AvgLoss:2.0508 Time:112.9s LR:0.000903\n",
      "Epoch 24/40 Batch 260/316 AvgLoss:2.0533 Time:121.9s LR:0.000903\n",
      "Epoch 24/40 Batch 280/316 AvgLoss:2.0525 Time:131.2s LR:0.000903\n",
      "Epoch 24/40 Batch 300/316 AvgLoss:2.0349 Time:140.5s LR:0.000903\n",
      "Epoch 24/40 Batch 316/316 AvgLoss:2.0208 Time:147.8s LR:0.000903\n",
      "Epoch 24 VALID -> Loss: 1.1388 Top1: 72.889 Top5: 89.778\n",
      "Epoch 25/40 Batch 20/316 AvgLoss:2.0359 Time:11.0s LR:0.000807\n",
      "Epoch 25/40 Batch 40/316 AvgLoss:2.0241 Time:20.0s LR:0.000807\n",
      "Epoch 25/40 Batch 60/316 AvgLoss:1.9095 Time:29.2s LR:0.000807\n",
      "Epoch 25/40 Batch 80/316 AvgLoss:1.8993 Time:38.4s LR:0.000807\n",
      "Epoch 25/40 Batch 100/316 AvgLoss:1.9039 Time:47.8s LR:0.000807\n",
      "Epoch 25/40 Batch 120/316 AvgLoss:1.8971 Time:56.9s LR:0.000807\n",
      "Epoch 25/40 Batch 140/316 AvgLoss:1.8344 Time:66.0s LR:0.000807\n",
      "Epoch 25/40 Batch 160/316 AvgLoss:1.8215 Time:74.1s LR:0.000807\n",
      "Epoch 25/40 Batch 180/316 AvgLoss:1.8319 Time:82.4s LR:0.000807\n",
      "Epoch 25/40 Batch 200/316 AvgLoss:1.8518 Time:91.5s LR:0.000807\n",
      "Epoch 25/40 Batch 220/316 AvgLoss:1.8331 Time:100.4s LR:0.000807\n",
      "Epoch 25/40 Batch 240/316 AvgLoss:1.8194 Time:109.4s LR:0.000807\n",
      "Epoch 25/40 Batch 260/316 AvgLoss:1.8208 Time:118.1s LR:0.000807\n",
      "Epoch 25/40 Batch 280/316 AvgLoss:1.8275 Time:126.8s LR:0.000807\n",
      "Epoch 25/40 Batch 300/316 AvgLoss:1.8408 Time:135.6s LR:0.000807\n",
      "Epoch 25/40 Batch 316/316 AvgLoss:1.8433 Time:142.5s LR:0.000807\n",
      "Epoch 25 VALID -> Loss: 1.2105 Top1: 71.067 Top5: 88.267\n",
      "Epoch 26/40 Batch 20/316 AvgLoss:1.8291 Time:10.0s LR:0.000715\n",
      "Epoch 26/40 Batch 40/316 AvgLoss:1.8802 Time:18.6s LR:0.000715\n",
      "Epoch 26/40 Batch 60/316 AvgLoss:1.7919 Time:27.4s LR:0.000715\n",
      "Epoch 26/40 Batch 80/316 AvgLoss:1.8618 Time:36.0s LR:0.000715\n",
      "Epoch 26/40 Batch 100/316 AvgLoss:1.9084 Time:44.5s LR:0.000715\n",
      "Epoch 26/40 Batch 120/316 AvgLoss:1.9311 Time:53.0s LR:0.000715\n",
      "Epoch 26/40 Batch 140/316 AvgLoss:1.8855 Time:61.7s LR:0.000715\n",
      "Epoch 26/40 Batch 160/316 AvgLoss:1.8805 Time:70.5s LR:0.000715\n",
      "Epoch 26/40 Batch 180/316 AvgLoss:1.8670 Time:79.2s LR:0.000715\n",
      "Epoch 26/40 Batch 200/316 AvgLoss:1.8804 Time:87.6s LR:0.000715\n",
      "Epoch 26/40 Batch 220/316 AvgLoss:1.8971 Time:95.1s LR:0.000715\n",
      "Epoch 26/40 Batch 240/316 AvgLoss:1.8981 Time:103.7s LR:0.000715\n",
      "Epoch 26/40 Batch 260/316 AvgLoss:1.8710 Time:112.1s LR:0.000715\n",
      "Epoch 26/40 Batch 280/316 AvgLoss:1.8936 Time:120.7s LR:0.000715\n",
      "Epoch 26/40 Batch 300/316 AvgLoss:1.9222 Time:129.2s LR:0.000715\n",
      "Epoch 26/40 Batch 316/316 AvgLoss:1.9200 Time:136.3s LR:0.000715\n",
      "Epoch 26 VALID -> Loss: 1.0968 Top1: 73.244 Top5: 90.889\n",
      "Epoch 27/40 Batch 20/316 AvgLoss:2.0290 Time:10.2s LR:0.000626\n",
      "Epoch 27/40 Batch 40/316 AvgLoss:1.9736 Time:19.0s LR:0.000626\n",
      "Epoch 27/40 Batch 60/316 AvgLoss:1.9563 Time:27.6s LR:0.000626\n",
      "Epoch 27/40 Batch 80/316 AvgLoss:1.9501 Time:36.1s LR:0.000626\n",
      "Epoch 27/40 Batch 100/316 AvgLoss:1.9666 Time:44.6s LR:0.000626\n",
      "Epoch 27/40 Batch 120/316 AvgLoss:1.9635 Time:53.0s LR:0.000626\n",
      "Epoch 27/40 Batch 140/316 AvgLoss:1.9567 Time:61.4s LR:0.000626\n",
      "Epoch 27/40 Batch 160/316 AvgLoss:1.8863 Time:70.1s LR:0.000626\n",
      "Epoch 27/40 Batch 180/316 AvgLoss:1.9141 Time:78.6s LR:0.000626\n",
      "Epoch 27/40 Batch 200/316 AvgLoss:1.9283 Time:87.0s LR:0.000626\n",
      "Epoch 27/40 Batch 220/316 AvgLoss:1.9344 Time:95.6s LR:0.000626\n",
      "Epoch 27/40 Batch 240/316 AvgLoss:1.9465 Time:104.2s LR:0.000626\n",
      "Epoch 27/40 Batch 260/316 AvgLoss:1.9324 Time:112.3s LR:0.000626\n",
      "Epoch 27/40 Batch 280/316 AvgLoss:1.9296 Time:121.1s LR:0.000626\n",
      "Epoch 27/40 Batch 300/316 AvgLoss:1.9373 Time:128.6s LR:0.000626\n",
      "Epoch 27/40 Batch 316/316 AvgLoss:1.9510 Time:135.5s LR:0.000626\n",
      "Epoch 27 VALID -> Loss: 1.1847 Top1: 72.356 Top5: 89.422\n",
      "Epoch 28/40 Batch 20/316 AvgLoss:1.8684 Time:10.5s LR:0.000541\n",
      "Epoch 28/40 Batch 40/316 AvgLoss:1.9289 Time:19.4s LR:0.000541\n",
      "Epoch 28/40 Batch 60/316 AvgLoss:1.9146 Time:27.9s LR:0.000541\n",
      "Epoch 28/40 Batch 80/316 AvgLoss:1.9591 Time:36.7s LR:0.000541\n",
      "Epoch 28/40 Batch 100/316 AvgLoss:1.9475 Time:45.6s LR:0.000541\n",
      "Epoch 28/40 Batch 120/316 AvgLoss:1.9680 Time:54.1s LR:0.000541\n",
      "Epoch 28/40 Batch 140/316 AvgLoss:1.9591 Time:62.8s LR:0.000541\n",
      "Epoch 28/40 Batch 160/316 AvgLoss:1.9421 Time:71.5s LR:0.000541\n",
      "Epoch 28/40 Batch 180/316 AvgLoss:1.9407 Time:80.2s LR:0.000541\n",
      "Epoch 28/40 Batch 200/316 AvgLoss:1.9418 Time:89.1s LR:0.000541\n",
      "Epoch 28/40 Batch 220/316 AvgLoss:1.9314 Time:97.7s LR:0.000541\n",
      "Epoch 28/40 Batch 240/316 AvgLoss:1.9403 Time:106.3s LR:0.000541\n",
      "Epoch 28/40 Batch 260/316 AvgLoss:1.9465 Time:114.8s LR:0.000541\n",
      "Epoch 28/40 Batch 280/316 AvgLoss:1.9293 Time:123.7s LR:0.000541\n",
      "Epoch 28/40 Batch 300/316 AvgLoss:1.9167 Time:132.5s LR:0.000541\n",
      "Epoch 28/40 Batch 316/316 AvgLoss:1.9170 Time:139.8s LR:0.000541\n",
      "Epoch 28 VALID -> Loss: 1.1566 Top1: 72.444 Top5: 89.733\n",
      "Epoch 29/40 Batch 20/316 AvgLoss:1.9234 Time:10.9s LR:0.000460\n",
      "Epoch 29/40 Batch 40/316 AvgLoss:1.9440 Time:19.7s LR:0.000460\n",
      "Epoch 29/40 Batch 60/316 AvgLoss:1.9576 Time:28.3s LR:0.000460\n",
      "Epoch 29/40 Batch 80/316 AvgLoss:1.9718 Time:36.9s LR:0.000460\n",
      "Epoch 29/40 Batch 100/316 AvgLoss:1.9190 Time:45.5s LR:0.000460\n",
      "Epoch 29/40 Batch 120/316 AvgLoss:1.9557 Time:54.3s LR:0.000460\n",
      "Epoch 29/40 Batch 140/316 AvgLoss:1.9373 Time:63.0s LR:0.000460\n",
      "Epoch 29/40 Batch 160/316 AvgLoss:1.9354 Time:71.6s LR:0.000460\n",
      "Epoch 29/40 Batch 180/316 AvgLoss:1.9325 Time:79.9s LR:0.000460\n",
      "Epoch 29/40 Batch 200/316 AvgLoss:1.9103 Time:89.0s LR:0.000460\n",
      "Epoch 29/40 Batch 220/316 AvgLoss:1.8857 Time:97.5s LR:0.000460\n",
      "Epoch 29/40 Batch 240/316 AvgLoss:1.9026 Time:105.9s LR:0.000460\n",
      "Epoch 29/40 Batch 260/316 AvgLoss:1.9054 Time:114.7s LR:0.000460\n",
      "Epoch 29/40 Batch 280/316 AvgLoss:1.8881 Time:123.3s LR:0.000460\n",
      "Epoch 29/40 Batch 300/316 AvgLoss:1.9001 Time:131.8s LR:0.000460\n",
      "Epoch 29/40 Batch 316/316 AvgLoss:1.8995 Time:138.9s LR:0.000460\n",
      "Epoch 29 VALID -> Loss: 1.1754 Top1: 71.822 Top5: 89.511\n",
      "Epoch 30/40 Batch 20/316 AvgLoss:2.0640 Time:9.7s LR:0.000385\n",
      "Epoch 30/40 Batch 40/316 AvgLoss:1.9443 Time:18.3s LR:0.000385\n",
      "Epoch 30/40 Batch 60/316 AvgLoss:1.9245 Time:28.1s LR:0.000385\n",
      "Epoch 30/40 Batch 80/316 AvgLoss:1.9177 Time:38.6s LR:0.000385\n",
      "Epoch 30/40 Batch 100/316 AvgLoss:1.8750 Time:49.1s LR:0.000385\n",
      "Epoch 30/40 Batch 120/316 AvgLoss:1.9085 Time:58.0s LR:0.000385\n",
      "Epoch 30/40 Batch 140/316 AvgLoss:1.9232 Time:66.7s LR:0.000385\n",
      "Epoch 30/40 Batch 160/316 AvgLoss:1.8998 Time:75.8s LR:0.000385\n",
      "Epoch 30/40 Batch 180/316 AvgLoss:1.9032 Time:84.7s LR:0.000385\n",
      "Epoch 30/40 Batch 200/316 AvgLoss:1.8884 Time:93.8s LR:0.000385\n",
      "Epoch 30/40 Batch 220/316 AvgLoss:1.8942 Time:102.7s LR:0.000385\n",
      "Epoch 30/40 Batch 240/316 AvgLoss:1.9064 Time:111.6s LR:0.000385\n",
      "Epoch 30/40 Batch 260/316 AvgLoss:1.9013 Time:120.3s LR:0.000385\n",
      "Epoch 30/40 Batch 280/316 AvgLoss:1.9010 Time:129.2s LR:0.000385\n",
      "Epoch 30/40 Batch 300/316 AvgLoss:1.9036 Time:138.3s LR:0.000385\n",
      "Epoch 30/40 Batch 316/316 AvgLoss:1.9050 Time:145.6s LR:0.000385\n",
      "Epoch 30 VALID -> Loss: 1.1309 Top1: 73.378 Top5: 89.644\n",
      "Epoch 31/40 Batch 20/316 AvgLoss:1.9788 Time:10.4s LR:0.000315\n",
      "Epoch 31/40 Batch 40/316 AvgLoss:1.8126 Time:19.3s LR:0.000315\n",
      "Epoch 31/40 Batch 60/316 AvgLoss:1.7933 Time:28.3s LR:0.000315\n",
      "Epoch 31/40 Batch 80/316 AvgLoss:1.8481 Time:36.8s LR:0.000315\n",
      "Epoch 31/40 Batch 100/316 AvgLoss:1.8617 Time:45.9s LR:0.000315\n",
      "Epoch 31/40 Batch 120/316 AvgLoss:1.8663 Time:54.9s LR:0.000315\n",
      "Epoch 31/40 Batch 140/316 AvgLoss:1.8804 Time:63.7s LR:0.000315\n",
      "Epoch 31/40 Batch 160/316 AvgLoss:1.9023 Time:73.0s LR:0.000315\n",
      "Epoch 31/40 Batch 180/316 AvgLoss:1.8772 Time:80.7s LR:0.000315\n",
      "Epoch 31/40 Batch 200/316 AvgLoss:1.8736 Time:89.8s LR:0.000315\n",
      "Epoch 31/40 Batch 220/316 AvgLoss:1.8806 Time:98.7s LR:0.000315\n",
      "Epoch 31/40 Batch 240/316 AvgLoss:1.8619 Time:107.8s LR:0.000315\n",
      "Epoch 31/40 Batch 260/316 AvgLoss:1.8696 Time:116.8s LR:0.000315\n",
      "Epoch 31/40 Batch 280/316 AvgLoss:1.8713 Time:125.8s LR:0.000315\n",
      "Epoch 31/40 Batch 300/316 AvgLoss:1.8755 Time:135.0s LR:0.000315\n",
      "Epoch 31/40 Batch 316/316 AvgLoss:1.8838 Time:142.2s LR:0.000315\n",
      "Epoch 31 VALID -> Loss: 1.2114 Top1: 71.333 Top5: 89.022\n",
      "Epoch 32/40 Batch 20/316 AvgLoss:1.9195 Time:10.6s LR:0.000252\n",
      "Epoch 32/40 Batch 40/316 AvgLoss:1.8832 Time:19.4s LR:0.000252\n",
      "Epoch 32/40 Batch 60/316 AvgLoss:1.8513 Time:28.3s LR:0.000252\n",
      "Epoch 32/40 Batch 80/316 AvgLoss:1.8659 Time:36.8s LR:0.000252\n",
      "Epoch 32/40 Batch 100/316 AvgLoss:1.8434 Time:45.6s LR:0.000252\n",
      "Epoch 32/40 Batch 120/316 AvgLoss:1.7936 Time:54.2s LR:0.000252\n",
      "Epoch 32/40 Batch 140/316 AvgLoss:1.7927 Time:63.0s LR:0.000252\n",
      "Epoch 32/40 Batch 160/316 AvgLoss:1.8010 Time:71.5s LR:0.000252\n",
      "Epoch 32/40 Batch 180/316 AvgLoss:1.7909 Time:80.2s LR:0.000252\n",
      "Epoch 32/40 Batch 200/316 AvgLoss:1.8023 Time:88.7s LR:0.000252\n",
      "Epoch 32/40 Batch 220/316 AvgLoss:1.8240 Time:96.5s LR:0.000252\n",
      "Epoch 32/40 Batch 240/316 AvgLoss:1.8099 Time:105.1s LR:0.000252\n",
      "Epoch 32/40 Batch 260/316 AvgLoss:1.8152 Time:114.0s LR:0.000252\n",
      "Epoch 32/40 Batch 280/316 AvgLoss:1.8305 Time:122.6s LR:0.000252\n",
      "Epoch 32/40 Batch 300/316 AvgLoss:1.8358 Time:131.4s LR:0.000252\n",
      "Epoch 32/40 Batch 316/316 AvgLoss:1.8541 Time:138.5s LR:0.000252\n",
      "Epoch 32 VALID -> Loss: 1.1472 Top1: 72.711 Top5: 89.867\n",
      "Epoch 33/40 Batch 20/316 AvgLoss:1.9074 Time:9.8s LR:0.000194\n",
      "Epoch 33/40 Batch 40/316 AvgLoss:1.8626 Time:18.4s LR:0.000194\n",
      "Epoch 33/40 Batch 60/316 AvgLoss:1.8078 Time:27.2s LR:0.000194\n",
      "Epoch 33/40 Batch 80/316 AvgLoss:1.8517 Time:35.6s LR:0.000194\n",
      "Epoch 33/40 Batch 100/316 AvgLoss:1.8685 Time:44.2s LR:0.000194\n",
      "Epoch 33/40 Batch 120/316 AvgLoss:1.8769 Time:53.0s LR:0.000194\n",
      "Epoch 33/40 Batch 140/316 AvgLoss:1.9074 Time:61.4s LR:0.000194\n",
      "Epoch 33/40 Batch 160/316 AvgLoss:1.8931 Time:70.0s LR:0.000194\n",
      "Epoch 33/40 Batch 180/316 AvgLoss:1.8854 Time:78.8s LR:0.000194\n",
      "Epoch 33/40 Batch 200/316 AvgLoss:1.8808 Time:87.1s LR:0.000194\n",
      "Epoch 33/40 Batch 220/316 AvgLoss:1.8843 Time:95.8s LR:0.000194\n",
      "Epoch 33/40 Batch 240/316 AvgLoss:1.8959 Time:104.5s LR:0.000194\n",
      "Epoch 33/40 Batch 260/316 AvgLoss:1.8944 Time:113.0s LR:0.000194\n",
      "Epoch 33/40 Batch 280/316 AvgLoss:1.8888 Time:120.7s LR:0.000194\n",
      "Epoch 33/40 Batch 300/316 AvgLoss:1.8958 Time:129.5s LR:0.000194\n",
      "Epoch 33/40 Batch 316/316 AvgLoss:1.8999 Time:136.5s LR:0.000194\n",
      "Epoch 33 VALID -> Loss: 1.1411 Top1: 72.667 Top5: 89.422\n",
      "Epoch 34/40 Batch 20/316 AvgLoss:2.0242 Time:10.4s LR:0.000144\n",
      "Epoch 34/40 Batch 40/316 AvgLoss:1.9446 Time:19.1s LR:0.000144\n",
      "Epoch 34/40 Batch 60/316 AvgLoss:1.9250 Time:27.9s LR:0.000144\n",
      "Epoch 34/40 Batch 80/316 AvgLoss:1.9461 Time:36.7s LR:0.000144\n",
      "Epoch 34/40 Batch 100/316 AvgLoss:1.9190 Time:45.2s LR:0.000144\n",
      "Epoch 34/40 Batch 120/316 AvgLoss:1.9308 Time:53.8s LR:0.000144\n",
      "Epoch 34/40 Batch 140/316 AvgLoss:1.9121 Time:62.6s LR:0.000144\n",
      "Epoch 34/40 Batch 160/316 AvgLoss:1.9353 Time:71.1s LR:0.000144\n",
      "Epoch 34/40 Batch 180/316 AvgLoss:1.9222 Time:80.0s LR:0.000144\n",
      "Epoch 34/40 Batch 200/316 AvgLoss:1.9167 Time:88.4s LR:0.000144\n",
      "Epoch 34/40 Batch 220/316 AvgLoss:1.9073 Time:97.0s LR:0.000144\n",
      "Epoch 34/40 Batch 240/316 AvgLoss:1.9118 Time:105.4s LR:0.000144\n",
      "Epoch 34/40 Batch 260/316 AvgLoss:1.9220 Time:113.7s LR:0.000144\n",
      "Epoch 34/40 Batch 280/316 AvgLoss:1.9306 Time:122.5s LR:0.000144\n",
      "Epoch 34/40 Batch 300/316 AvgLoss:1.9324 Time:131.2s LR:0.000144\n",
      "Epoch 34/40 Batch 316/316 AvgLoss:1.9110 Time:138.0s LR:0.000144\n",
      "Epoch 34 VALID -> Loss: 1.0981 Top1: 73.689 Top5: 89.778\n",
      "Epoch 35/40 Batch 20/316 AvgLoss:1.7916 Time:10.0s LR:0.000101\n",
      "Epoch 35/40 Batch 40/316 AvgLoss:1.6202 Time:19.0s LR:0.000101\n",
      "Epoch 35/40 Batch 60/316 AvgLoss:1.7031 Time:27.3s LR:0.000101\n",
      "Epoch 35/40 Batch 80/316 AvgLoss:1.7606 Time:35.7s LR:0.000101\n",
      "Epoch 35/40 Batch 100/316 AvgLoss:1.7472 Time:44.3s LR:0.000101\n",
      "Epoch 35/40 Batch 120/316 AvgLoss:1.7607 Time:52.7s LR:0.000101\n",
      "Epoch 35/40 Batch 140/316 AvgLoss:1.7608 Time:61.5s LR:0.000101\n",
      "Epoch 35/40 Batch 160/316 AvgLoss:1.7824 Time:70.2s LR:0.000101\n",
      "Epoch 35/40 Batch 180/316 AvgLoss:1.7976 Time:78.7s LR:0.000101\n",
      "Epoch 35/40 Batch 200/316 AvgLoss:1.7967 Time:87.2s LR:0.000101\n",
      "Epoch 35/40 Batch 220/316 AvgLoss:1.8289 Time:95.8s LR:0.000101\n",
      "Epoch 35/40 Batch 240/316 AvgLoss:1.8352 Time:104.2s LR:0.000101\n",
      "Epoch 35/40 Batch 260/316 AvgLoss:1.8321 Time:112.7s LR:0.000101\n",
      "Epoch 35/40 Batch 280/316 AvgLoss:1.8514 Time:121.5s LR:0.000101\n",
      "Epoch 35/40 Batch 300/316 AvgLoss:1.8651 Time:130.2s LR:0.000101\n",
      "Epoch 35/40 Batch 316/316 AvgLoss:1.8674 Time:137.2s LR:0.000101\n",
      "Epoch 35 VALID -> Loss: 1.1191 Top1: 72.622 Top5: 90.578\n",
      "Epoch 36/40 Batch 20/316 AvgLoss:2.2126 Time:9.7s LR:0.000065\n",
      "Epoch 36/40 Batch 40/316 AvgLoss:2.0919 Time:17.9s LR:0.000065\n",
      "Epoch 36/40 Batch 60/316 AvgLoss:2.0272 Time:28.3s LR:0.000065\n",
      "Epoch 36/40 Batch 80/316 AvgLoss:2.0333 Time:38.5s LR:0.000065\n",
      "Epoch 36/40 Batch 100/316 AvgLoss:2.0110 Time:48.7s LR:0.000065\n",
      "Epoch 36/40 Batch 120/316 AvgLoss:1.9920 Time:56.8s LR:0.000065\n",
      "Epoch 36/40 Batch 140/316 AvgLoss:1.9576 Time:65.1s LR:0.000065\n",
      "Epoch 36/40 Batch 160/316 AvgLoss:1.9661 Time:73.6s LR:0.000065\n",
      "Epoch 36/40 Batch 180/316 AvgLoss:1.9677 Time:81.8s LR:0.000065\n",
      "Epoch 36/40 Batch 200/316 AvgLoss:1.9776 Time:90.7s LR:0.000065\n",
      "Epoch 36/40 Batch 220/316 AvgLoss:1.9774 Time:100.0s LR:0.000065\n",
      "Epoch 36/40 Batch 240/316 AvgLoss:1.9653 Time:108.7s LR:0.000065\n",
      "Epoch 36/40 Batch 260/316 AvgLoss:1.9345 Time:117.5s LR:0.000065\n",
      "Epoch 36/40 Batch 280/316 AvgLoss:1.9300 Time:126.4s LR:0.000065\n",
      "Epoch 36/40 Batch 300/316 AvgLoss:1.9266 Time:135.5s LR:0.000065\n",
      "Epoch 36/40 Batch 316/316 AvgLoss:1.9226 Time:142.6s LR:0.000065\n",
      "Epoch 36 VALID -> Loss: 1.1215 Top1: 72.844 Top5: 90.178\n",
      "Epoch 37/40 Batch 20/316 AvgLoss:2.1010 Time:10.4s LR:0.000037\n",
      "Epoch 37/40 Batch 40/316 AvgLoss:1.9151 Time:19.6s LR:0.000037\n",
      "Epoch 37/40 Batch 60/316 AvgLoss:1.8905 Time:28.5s LR:0.000037\n",
      "Epoch 37/40 Batch 80/316 AvgLoss:1.8965 Time:37.8s LR:0.000037\n",
      "Epoch 37/40 Batch 100/316 AvgLoss:1.8839 Time:46.9s LR:0.000037\n",
      "Epoch 37/40 Batch 120/316 AvgLoss:1.8629 Time:56.5s LR:0.000037\n",
      "Epoch 37/40 Batch 140/316 AvgLoss:1.8825 Time:65.8s LR:0.000037\n",
      "Epoch 37/40 Batch 160/316 AvgLoss:1.8970 Time:73.8s LR:0.000037\n",
      "Epoch 37/40 Batch 180/316 AvgLoss:1.8919 Time:82.9s LR:0.000037\n",
      "Epoch 37/40 Batch 200/316 AvgLoss:1.8719 Time:91.9s LR:0.000037\n",
      "Epoch 37/40 Batch 220/316 AvgLoss:1.8796 Time:100.5s LR:0.000037\n",
      "Epoch 37/40 Batch 240/316 AvgLoss:1.8798 Time:109.1s LR:0.000037\n",
      "Epoch 37/40 Batch 260/316 AvgLoss:1.8812 Time:118.1s LR:0.000037\n",
      "Epoch 37/40 Batch 280/316 AvgLoss:1.8655 Time:127.2s LR:0.000037\n",
      "Epoch 37/40 Batch 300/316 AvgLoss:1.8563 Time:136.2s LR:0.000037\n",
      "Epoch 37/40 Batch 316/316 AvgLoss:1.8478 Time:143.5s LR:0.000037\n",
      "Epoch 37 VALID -> Loss: 1.1036 Top1: 73.022 Top5: 90.222\n",
      "Epoch 38/40 Batch 20/316 AvgLoss:1.6487 Time:11.1s LR:0.000017\n",
      "Epoch 38/40 Batch 40/316 AvgLoss:1.7583 Time:20.2s LR:0.000017\n",
      "Epoch 38/40 Batch 60/316 AvgLoss:1.7649 Time:29.3s LR:0.000017\n",
      "Epoch 38/40 Batch 80/316 AvgLoss:1.7817 Time:38.5s LR:0.000017\n",
      "Epoch 38/40 Batch 100/316 AvgLoss:1.8230 Time:47.8s LR:0.000017\n",
      "Epoch 38/40 Batch 120/316 AvgLoss:1.7917 Time:56.8s LR:0.000017\n",
      "Epoch 38/40 Batch 140/316 AvgLoss:1.8214 Time:65.8s LR:0.000017\n",
      "Epoch 38/40 Batch 160/316 AvgLoss:1.8012 Time:74.9s LR:0.000017\n",
      "Epoch 38/40 Batch 180/316 AvgLoss:1.8318 Time:84.2s LR:0.000017\n",
      "Epoch 38/40 Batch 200/316 AvgLoss:1.8280 Time:92.0s LR:0.000017\n",
      "Epoch 38/40 Batch 220/316 AvgLoss:1.8304 Time:101.2s LR:0.000017\n",
      "Epoch 38/40 Batch 240/316 AvgLoss:1.8270 Time:109.7s LR:0.000017\n",
      "Epoch 38/40 Batch 260/316 AvgLoss:1.8452 Time:118.3s LR:0.000017\n",
      "Epoch 38/40 Batch 280/316 AvgLoss:1.8500 Time:127.2s LR:0.000017\n",
      "Epoch 38/40 Batch 300/316 AvgLoss:1.8521 Time:136.0s LR:0.000017\n",
      "Epoch 38/40 Batch 316/316 AvgLoss:1.8509 Time:143.0s LR:0.000017\n",
      "Epoch 38 VALID -> Loss: 1.1131 Top1: 73.111 Top5: 90.356\n",
      "Epoch 39/40 Batch 20/316 AvgLoss:1.7087 Time:10.0s LR:0.000005\n",
      "Epoch 39/40 Batch 40/316 AvgLoss:1.7920 Time:18.4s LR:0.000005\n",
      "Epoch 39/40 Batch 60/316 AvgLoss:1.8475 Time:27.1s LR:0.000005\n",
      "Epoch 39/40 Batch 80/316 AvgLoss:1.9319 Time:35.7s LR:0.000005\n",
      "Epoch 39/40 Batch 100/316 AvgLoss:1.8536 Time:43.7s LR:0.000005\n",
      "Epoch 39/40 Batch 120/316 AvgLoss:1.8996 Time:52.4s LR:0.000005\n",
      "Epoch 39/40 Batch 140/316 AvgLoss:1.8662 Time:60.9s LR:0.000005\n",
      "Epoch 39/40 Batch 160/316 AvgLoss:1.8703 Time:69.8s LR:0.000005\n",
      "Epoch 39/40 Batch 180/316 AvgLoss:1.8613 Time:78.5s LR:0.000005\n",
      "Epoch 39/40 Batch 200/316 AvgLoss:1.8475 Time:87.1s LR:0.000005\n",
      "Epoch 39/40 Batch 220/316 AvgLoss:1.8496 Time:95.6s LR:0.000005\n",
      "Epoch 39/40 Batch 240/316 AvgLoss:1.8492 Time:104.0s LR:0.000005\n",
      "Epoch 39/40 Batch 260/316 AvgLoss:1.8492 Time:111.4s LR:0.000005\n",
      "Epoch 39/40 Batch 280/316 AvgLoss:1.8358 Time:119.9s LR:0.000005\n",
      "Epoch 39/40 Batch 300/316 AvgLoss:1.8130 Time:128.2s LR:0.000005\n",
      "Epoch 39/40 Batch 316/316 AvgLoss:1.8048 Time:134.9s LR:0.000005\n",
      "Epoch 39 VALID -> Loss: 1.0836 Top1: 73.200 Top5: 90.133\n",
      "Epoch 40/40 Batch 20/316 AvgLoss:1.6939 Time:9.7s LR:0.000001\n",
      "Epoch 40/40 Batch 40/316 AvgLoss:1.8071 Time:18.5s LR:0.000001\n",
      "Epoch 40/40 Batch 60/316 AvgLoss:1.8732 Time:27.0s LR:0.000001\n",
      "Epoch 40/40 Batch 80/316 AvgLoss:1.8490 Time:35.8s LR:0.000001\n",
      "Epoch 40/40 Batch 100/316 AvgLoss:1.8994 Time:44.4s LR:0.000001\n",
      "Epoch 40/40 Batch 120/316 AvgLoss:1.8999 Time:52.9s LR:0.000001\n",
      "Epoch 40/40 Batch 140/316 AvgLoss:1.8813 Time:61.8s LR:0.000001\n",
      "Epoch 40/40 Batch 160/316 AvgLoss:1.8897 Time:70.6s LR:0.000001\n",
      "Epoch 40/40 Batch 180/316 AvgLoss:1.8821 Time:79.2s LR:0.000001\n",
      "Epoch 40/40 Batch 200/316 AvgLoss:1.8986 Time:87.8s LR:0.000001\n",
      "Epoch 40/40 Batch 220/316 AvgLoss:1.8947 Time:96.2s LR:0.000001\n",
      "Epoch 40/40 Batch 240/316 AvgLoss:1.8994 Time:104.6s LR:0.000001\n",
      "Epoch 40/40 Batch 260/316 AvgLoss:1.9016 Time:113.3s LR:0.000001\n",
      "Epoch 40/40 Batch 280/316 AvgLoss:1.8922 Time:121.9s LR:0.000001\n",
      "Epoch 40/40 Batch 300/316 AvgLoss:1.8938 Time:130.4s LR:0.000001\n",
      "Epoch 40/40 Batch 316/316 AvgLoss:1.8929 Time:137.2s LR:0.000001\n",
      "Epoch 40 VALID -> Loss: 1.1734 Top1: 72.267 Top5: 89.378\n",
      "Retraining finished. Best val top1: 74.04444444444445\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=BASE_LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "best_val_top1=0.0\n",
    "start_epoch=0\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, M2_EPOCHS):\n",
    "    t = epoch / float(max(1, M2_EPOCHS - 1))\n",
    "    cur_lr = MIN_LR + 0.5 * (BASE_LR - MIN_LR) * (1.0 + math.cos(math.pi * t))\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = cur_lr\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    seen = 0\n",
    "    t0 = time.time()\n",
    "    for i, (imgs, targets) in enumerate(train_loader_m2):\n",
    "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "        targets = targets.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        if USE_MIXUP_M2:\n",
    "            lam = np.random.beta(MIXUP_ALPHA_M2, MIXUP_ALPHA_M2)\n",
    "            idx = torch.randperm(imgs.size(0)).to(DEVICE)\n",
    "            imgs_m = lam * imgs + (1.0 - lam) * imgs[idx]\n",
    "            y_a = torch.zeros((imgs.size(0), 100), device=DEVICE).scatter_(1, targets.unsqueeze(1), 1.0)\n",
    "            y_b = torch.zeros_like(y_a).scatter_(1, targets[idx].unsqueeze(1), 1.0)\n",
    "            soft_targets = lam * y_a + (1.0 - lam) * y_b\n",
    "            loss_fn = lambda logits, soft: -(F.log_softmax(logits, dim=1) * soft).sum(dim=1).mean()\n",
    "            inputs = imgs_m\n",
    "        else:\n",
    "            inputs = imgs\n",
    "            loss_fn = lambda logits, targs: F.cross_entropy(logits, targs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
    "            logits = model(inputs)\n",
    "            if USE_MIXUP_M2:\n",
    "                loss = loss_fn(logits, soft_targets)\n",
    "            else:\n",
    "                loss = loss_fn(logits, targets)\n",
    "\n",
    "        if DEVICE == \"cuda\":\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += float(loss.item()) * imgs.size(0)\n",
    "        seen += imgs.size(0)\n",
    "        if (i + 1) % 20 == 0 or (i+1) == len(train_loader_m2):\n",
    "            print(f\"Epoch {epoch+1}/{M2_EPOCHS} Batch {i+1}/{len(train_loader_m2)} AvgLoss:{running_loss/seen:.4f} Time:{time.time()-t0:.1f}s LR:{cur_lr:.6f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_total = 0\n",
    "    val_top1_count = 0\n",
    "    val_top5_count = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in val_loader_m2:\n",
    "            imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "            targets = targets.to(DEVICE, non_blocking=True)\n",
    "            logits = model(imgs)\n",
    "            loss_v = F.cross_entropy(logits, targets)\n",
    "            bs = imgs.size(0)\n",
    "            val_running_loss += float(loss_v.item()) * bs\n",
    "            val_total += bs\n",
    "            _, pred = logits.topk(5, dim=1, largest=True, sorted=True)\n",
    "            correct = pred.eq(targets.view(-1,1).expand_as(pred))\n",
    "            val_top1_count += correct[:, :1].reshape(-1).float().sum().item()\n",
    "            val_top5_count += correct[:, :5].reshape(-1).float().sum().item()\n",
    "\n",
    "    val_loss = val_running_loss / val_total\n",
    "    val_top1 = 100.0 * val_top1_count / val_total\n",
    "    val_top5 = 100.0 * val_top5_count / val_total\n",
    "    print(f\"Epoch {epoch+1} VALID -> Loss: {val_loss:.4f} Top1: {val_top1:.3f} Top5: {val_top5:.3f}\")\n",
    "\n",
    "    ckpt = {\n",
    "        \"epoch\": epoch+1,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"val_top1\": val_top1,\n",
    "        \"cfg\": {\"BASE_LR\": BASE_LR, \"M2_EPOCHS\": M2_EPOCHS}\n",
    "    }\n",
    "    if epoch%5==0:\n",
    "        torch.save(ckpt, os.path.join(M2_SAVE_DIR, f\"retrain_epoch_{epoch+1}.pth\"))\n",
    "    if val_top1 > best_val_top1:\n",
    "        best_val_top1 = val_top1\n",
    "        torch.save(ckpt, os.path.join(M2_SAVE_DIR, \"best_m2_retrained.pth\"))\n",
    "        print(\"Saved new best_m2_retrained.pth (val_top1 improved to {:.3f})\".format(val_top1))\n",
    "\n",
    "print(\"Retraining finished. Best val top1:\", best_val_top1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
