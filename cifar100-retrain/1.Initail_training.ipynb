{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40ba2663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/23ucc611/miniconda3/envs/ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\" \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "import timm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a7ef8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")  \n",
    "\n",
    "torch.cuda.memory_allocated(device)   # bytes allocated by tensors\n",
    "torch.cuda.memory_reserved(device)    # bytes reserved by allocator\n",
    "torch.cuda.empty_cache()              # frees cached memory (non-deterministic improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77bb7e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]\n",
      "torch: 2.6.0+cu124\n",
      "torchvision: 0.21.0+cu124\n"
     ]
    }
   ],
   "source": [
    "print(\"python:\", os.sys.version.splitlines()[0])\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"torchvision:\", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edbd1742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available. 1 device(s):\n",
      "  [0] Tesla V100-SXM2-32GB  (cap=(7, 0), mem=31.7 GB)\n"
     ]
    }
   ],
   "source": [
    "def print_gpu_info():\n",
    "    if torch.cuda.is_available():\n",
    "        dev = torch.device(\"cuda\")\n",
    "        n = torch.cuda.device_count()\n",
    "        print(f\"CUDA available. {n} device(s):\")\n",
    "        for i in range(n):\n",
    "            name = torch.cuda.get_device_name(i)\n",
    "            cap = torch.cuda.get_device_capability(i)\n",
    "            total = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "            print(f\"  [{i}] {name}  (cap={cap}, mem={total:.1f} GB)\")\n",
    "    else:\n",
    "        print(\"CUDA not available, using CPU.\")\n",
    "print_gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f03d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42, deterministic=False):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    if deterministic:\n",
    "        # may slow training, but makes some operations deterministic\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    else:\n",
    "        torch.backends.cudnn.benchmark = True  # good for speed on fixed-size inputs\n",
    "set_seed(42, deterministic=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06ccb9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]\n",
      "PyTorch: 2.6.0+cu124\n",
      "Timm: 1.0.21\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = \"./data\"\n",
    "MODEL_NAME = \"resnext101_32x8d\"\n",
    "PRETRAINED = True\n",
    "NUM_CLASSES = 100\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64        # reduce if OOM: try 128 or 64\n",
    "EPOCHS = 120\n",
    "BASE_LR = 0.1           # intended for BATCH_SIZE=256 baseline\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "WARMUP_EPOCHS = 5\n",
    "NUM_WORKERS = 8\n",
    "PIN_MEMORY = True\n",
    "MIXUP_ALPHA = 0.8       # set 0.0 to disable mixup\n",
    "LABEL_SMOOTHING = 0.1   # used only when mixup disabled\n",
    "EMA_DECAY = 0.9999      # <=0 to disable EMA\n",
    "GRAD_CLIP = None        # set number like 1.0 to enable\n",
    "SEED = 42\n",
    "USE_AMP = True          # mixed precision\n",
    "PRINT_FREQ = 50\n",
    "SAVE_DIR = \"./checkpoints_linear\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Python:\", os.sys.version.splitlines()[0])\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Timm:\", timm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4b68874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test batches :  703 79 157\n"
     ]
    }
   ],
   "source": [
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.RandomResizedCrop(IMG_SIZE, scale=(0.08, 1.0), ratio=(0.75, 1.3333)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "\n",
    "train_full = datasets.CIFAR100(root=DATA_ROOT, train=True, download=True, transform=train_transform)\n",
    "test_set    = datasets.CIFAR100(root=DATA_ROOT, train=False, download=True, transform=test_transform)\n",
    "\n",
    "val_size=5000\n",
    "train_size=len(train_full)-val_size\n",
    "torch.manual_seed(SEED)\n",
    "train_set,val_set=torch.utils.data.random_split(train_full,[train_size,val_size],generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "val_set.dataset.transform=test_transform\n",
    "\n",
    "train_loader=DataLoader(train_set,batch_size=BATCH_SIZE,shuffle=True,pin_memory=PIN_MEMORY,num_workers=NUM_WORKERS,drop_last=True)\n",
    "val_loader=DataLoader(val_set,batch_size=BATCH_SIZE,shuffle=False,pin_memory=PIN_MEMORY,num_workers=NUM_WORKERS)\n",
    "test_loader=DataLoader(test_set,batch_size=BATCH_SIZE,num_workers=NUM_WORKERS,pin_memory=PIN_MEMORY,shuffle=False)\n",
    "\n",
    "print(\"Train/Val/Test batches : \", len(train_loader), len(val_loader), len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa0419bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters : 86947236, trainable parameters : 86947236\n"
     ]
    }
   ],
   "source": [
    "model=timm.create_model(MODEL_NAME,pretrained=PRETRAINED,num_classes=NUM_CLASSES)\n",
    "model=model.to(device)\n",
    "\n",
    "total_params=sum(p.numel() for p in model.parameters())\n",
    "trainable_parameters=sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters : {total_params}, trainable parameters : {trainable_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67f12f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward OK. Output shape: (1, 100)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    dummy = torch.zeros((1, 3, IMG_SIZE, IMG_SIZE), device=device)\n",
    "    out = model(dummy)\n",
    "print(\"Forward OK. Output shape:\", tuple(out.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a242310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79090/3045800104.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler=torch.cuda.amp.GradScaler(enabled=(USE_AMP and device==\"cuda\"))\n"
     ]
    }
   ],
   "source": [
    "lr=BASE_LR *(BATCH_SIZE/256.0)\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=lr,momentum=MOMENTUM,weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "scaler=torch.cuda.amp.GradScaler(enabled=(USE_AMP and device==\"cuda\"))\n",
    "\n",
    "if EMA_DECAY > 0:\n",
    "    ema_model = timm.create_model(MODEL_NAME, pretrained=False, num_classes=NUM_CLASSES)\n",
    "    ema_model.load_state_dict(model.state_dict())   # copy initial weights\n",
    "    ema_model.to(device)\n",
    "    for p in ema_model.parameters():\n",
    "        p.requires_grad_(False)\n",
    "else:\n",
    "    ema_model = None\n",
    "\n",
    "\n",
    "def one_hot_smooth(labels, num_classes, smoothing, device):\n",
    "    if smoothing > 0:\n",
    "        off_value = smoothing / (num_classes - 1)\n",
    "        on_value = 1.0 - smoothing\n",
    "    else:\n",
    "        off_value = 0.0\n",
    "        on_value = 1.0\n",
    "    y = torch.full((labels.size(0), num_classes), off_value, device=device)\n",
    "    y.scatter_(1, labels.unsqueeze(1), on_value)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2db7880f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/120 - lr: 0.005000 - starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79090/4174858174.py:42: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(USE_AMP and device==\"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 50/703  AvgLoss: 4.2151  Time: 25.1s\n",
      "Epoch 1 Batch 100/703  AvgLoss: 3.5584  Time: 49.1s\n",
      "Epoch 1 Batch 150/703  AvgLoss: 3.1548  Time: 72.9s\n",
      "Epoch 1 Batch 200/703  AvgLoss: 3.0058  Time: 96.6s\n",
      "Epoch 1 Batch 250/703  AvgLoss: 2.8722  Time: 120.4s\n",
      "Epoch 1 Batch 300/703  AvgLoss: 2.7712  Time: 144.0s\n",
      "Epoch 1 Batch 350/703  AvgLoss: 2.6912  Time: 166.0s\n",
      "Epoch 1 Batch 400/703  AvgLoss: 2.6200  Time: 190.1s\n",
      "Epoch 1 Batch 450/703  AvgLoss: 2.5913  Time: 214.2s\n",
      "Epoch 1 Batch 500/703  AvgLoss: 2.5573  Time: 238.0s\n",
      "Epoch 1 Batch 550/703  AvgLoss: 2.5109  Time: 262.0s\n",
      "Epoch 1 Batch 600/703  AvgLoss: 2.4774  Time: 285.9s\n",
      "Epoch 1 Batch 650/703  AvgLoss: 2.4508  Time: 310.2s\n",
      "Epoch 1 Batch 700/703  AvgLoss: 2.4189  Time: 334.2s\n",
      "Epoch 1 Batch 703/703  AvgLoss: 2.4154  Time: 335.1s\n",
      "Epoch 1 VALID -> Loss: 4.4649  Top1: 3.540  Top5: 13.260\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 2/120 - lr: 0.010000 - starting training...\n",
      "Epoch 2 Batch 50/703  AvgLoss: 2.1309  Time: 25.2s\n",
      "Epoch 2 Batch 100/703  AvgLoss: 2.1326  Time: 48.5s\n",
      "Epoch 2 Batch 150/703  AvgLoss: 2.1372  Time: 72.6s\n",
      "Epoch 2 Batch 200/703  AvgLoss: 2.1324  Time: 97.0s\n",
      "Epoch 2 Batch 250/703  AvgLoss: 2.1422  Time: 120.8s\n",
      "Epoch 2 Batch 300/703  AvgLoss: 2.1691  Time: 144.5s\n",
      "Epoch 2 Batch 350/703  AvgLoss: 2.1646  Time: 169.1s\n",
      "Epoch 2 Batch 400/703  AvgLoss: 2.1895  Time: 193.2s\n",
      "Epoch 2 Batch 450/703  AvgLoss: 2.1951  Time: 216.6s\n",
      "Epoch 2 Batch 500/703  AvgLoss: 2.1925  Time: 240.7s\n",
      "Epoch 2 Batch 550/703  AvgLoss: 2.1895  Time: 265.4s\n",
      "Epoch 2 Batch 600/703  AvgLoss: 2.1974  Time: 289.9s\n",
      "Epoch 2 Batch 650/703  AvgLoss: 2.1947  Time: 314.6s\n",
      "Epoch 2 Batch 700/703  AvgLoss: 2.2016  Time: 338.8s\n",
      "Epoch 2 Batch 703/703  AvgLoss: 2.2033  Time: 340.3s\n",
      "Epoch 2 VALID -> Loss: 4.1227  Top1: 21.320  Top5: 45.320\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 3/120 - lr: 0.015000 - starting training...\n",
      "Epoch 3 Batch 50/703  AvgLoss: 2.0839  Time: 24.4s\n",
      "Epoch 3 Batch 100/703  AvgLoss: 2.1224  Time: 48.7s\n",
      "Epoch 3 Batch 150/703  AvgLoss: 2.1749  Time: 73.6s\n",
      "Epoch 3 Batch 200/703  AvgLoss: 2.1857  Time: 98.1s\n",
      "Epoch 3 Batch 250/703  AvgLoss: 2.2139  Time: 122.6s\n",
      "Epoch 3 Batch 300/703  AvgLoss: 2.2132  Time: 146.9s\n",
      "Epoch 3 Batch 350/703  AvgLoss: 2.2102  Time: 171.6s\n",
      "Epoch 3 Batch 400/703  AvgLoss: 2.2023  Time: 195.2s\n",
      "Epoch 3 Batch 450/703  AvgLoss: 2.1833  Time: 217.3s\n",
      "Epoch 3 Batch 500/703  AvgLoss: 2.1773  Time: 241.0s\n",
      "Epoch 3 Batch 550/703  AvgLoss: 2.1747  Time: 264.2s\n",
      "Epoch 3 Batch 600/703  AvgLoss: 2.1499  Time: 287.7s\n",
      "Epoch 3 Batch 650/703  AvgLoss: 2.1514  Time: 311.2s\n",
      "Epoch 3 Batch 700/703  AvgLoss: 2.1513  Time: 334.7s\n",
      "Epoch 3 Batch 703/703  AvgLoss: 2.1507  Time: 336.2s\n",
      "Epoch 3 VALID -> Loss: 3.7131  Top1: 52.540  Top5: 79.020\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 4/120 - lr: 0.020000 - starting training...\n",
      "Epoch 4 Batch 50/703  AvgLoss: 2.0128  Time: 25.7s\n",
      "Epoch 4 Batch 100/703  AvgLoss: 2.0145  Time: 47.9s\n",
      "Epoch 4 Batch 150/703  AvgLoss: 2.0249  Time: 71.6s\n",
      "Epoch 4 Batch 200/703  AvgLoss: 2.1068  Time: 95.1s\n",
      "Epoch 4 Batch 250/703  AvgLoss: 2.1131  Time: 118.6s\n",
      "Epoch 4 Batch 300/703  AvgLoss: 2.0851  Time: 142.6s\n",
      "Epoch 4 Batch 350/703  AvgLoss: 2.0618  Time: 166.2s\n",
      "Epoch 4 Batch 400/703  AvgLoss: 2.0702  Time: 190.2s\n",
      "Epoch 4 Batch 450/703  AvgLoss: 2.0562  Time: 213.7s\n",
      "Epoch 4 Batch 500/703  AvgLoss: 2.0602  Time: 235.6s\n",
      "Epoch 4 Batch 550/703  AvgLoss: 2.0548  Time: 259.2s\n",
      "Epoch 4 Batch 600/703  AvgLoss: 2.0659  Time: 282.8s\n",
      "Epoch 4 Batch 650/703  AvgLoss: 2.0551  Time: 306.5s\n",
      "Epoch 4 Batch 700/703  AvgLoss: 2.0620  Time: 329.9s\n",
      "Epoch 4 Batch 703/703  AvgLoss: 2.0615  Time: 331.4s\n",
      "Epoch 4 VALID -> Loss: 3.3196  Top1: 69.700  Top5: 92.480\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 5/120 - lr: 0.025000 - starting training...\n",
      "Epoch 5 Batch 50/703  AvgLoss: 2.0569  Time: 25.3s\n",
      "Epoch 5 Batch 100/703  AvgLoss: 2.0206  Time: 48.5s\n",
      "Epoch 5 Batch 150/703  AvgLoss: 2.0454  Time: 74.7s\n",
      "Epoch 5 Batch 200/703  AvgLoss: 2.0605  Time: 98.0s\n",
      "Epoch 5 Batch 250/703  AvgLoss: 2.0334  Time: 121.9s\n",
      "Epoch 5 Batch 300/703  AvgLoss: 1.9955  Time: 146.2s\n",
      "Epoch 5 Batch 350/703  AvgLoss: 2.0179  Time: 170.3s\n",
      "Epoch 5 Batch 400/703  AvgLoss: 2.0122  Time: 194.8s\n",
      "Epoch 5 Batch 450/703  AvgLoss: 2.0304  Time: 219.3s\n",
      "Epoch 5 Batch 500/703  AvgLoss: 2.0377  Time: 243.9s\n",
      "Epoch 5 Batch 550/703  AvgLoss: 2.0284  Time: 267.8s\n",
      "Epoch 5 Batch 600/703  AvgLoss: 2.0244  Time: 290.2s\n",
      "Epoch 5 Batch 650/703  AvgLoss: 2.0221  Time: 314.5s\n",
      "Epoch 5 Batch 700/703  AvgLoss: 2.0151  Time: 338.9s\n",
      "Epoch 5 Batch 703/703  AvgLoss: 2.0147  Time: 340.3s\n",
      "Epoch 5 VALID -> Loss: 2.9624  Top1: 75.400  Top5: 95.200\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 6/120 - lr: 0.025000 - starting training...\n",
      "Epoch 6 Batch 50/703  AvgLoss: 1.8192  Time: 26.2s\n",
      "Epoch 6 Batch 100/703  AvgLoss: 1.9109  Time: 50.7s\n",
      "Epoch 6 Batch 150/703  AvgLoss: 1.8756  Time: 75.4s\n",
      "Epoch 6 Batch 200/703  AvgLoss: 1.8808  Time: 99.5s\n",
      "Epoch 6 Batch 250/703  AvgLoss: 1.9125  Time: 121.4s\n",
      "Epoch 6 Batch 300/703  AvgLoss: 1.9214  Time: 145.5s\n",
      "Epoch 6 Batch 350/703  AvgLoss: 1.9162  Time: 169.3s\n",
      "Epoch 6 Batch 400/703  AvgLoss: 1.9299  Time: 192.9s\n",
      "Epoch 6 Batch 450/703  AvgLoss: 1.9300  Time: 217.0s\n",
      "Epoch 6 Batch 500/703  AvgLoss: 1.9076  Time: 240.7s\n",
      "Epoch 6 Batch 550/703  AvgLoss: 1.9058  Time: 264.2s\n",
      "Epoch 6 Batch 600/703  AvgLoss: 1.9108  Time: 287.8s\n",
      "Epoch 6 Batch 650/703  AvgLoss: 1.9156  Time: 310.0s\n",
      "Epoch 6 Batch 700/703  AvgLoss: 1.9125  Time: 333.5s\n",
      "Epoch 6 Batch 703/703  AvgLoss: 1.9118  Time: 335.0s\n",
      "Epoch 6 VALID -> Loss: 2.6248  Top1: 76.840  Top5: 96.120\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 7/120 - lr: 0.024995 - starting training...\n",
      "Epoch 7 Batch 50/703  AvgLoss: 1.7664  Time: 25.7s\n",
      "Epoch 7 Batch 100/703  AvgLoss: 1.7898  Time: 49.9s\n",
      "Epoch 7 Batch 150/703  AvgLoss: 1.7697  Time: 74.0s\n",
      "Epoch 7 Batch 200/703  AvgLoss: 1.7459  Time: 97.9s\n",
      "Epoch 7 Batch 250/703  AvgLoss: 1.7875  Time: 120.3s\n",
      "Epoch 7 Batch 300/703  AvgLoss: 1.7927  Time: 144.3s\n",
      "Epoch 7 Batch 350/703  AvgLoss: 1.8055  Time: 168.4s\n",
      "Epoch 7 Batch 400/703  AvgLoss: 1.7807  Time: 191.8s\n",
      "Epoch 7 Batch 450/703  AvgLoss: 1.8052  Time: 215.5s\n",
      "Epoch 7 Batch 500/703  AvgLoss: 1.7778  Time: 239.3s\n",
      "Epoch 7 Batch 550/703  AvgLoss: 1.7794  Time: 262.8s\n",
      "Epoch 7 Batch 600/703  AvgLoss: 1.7735  Time: 287.1s\n",
      "Epoch 7 Batch 650/703  AvgLoss: 1.7730  Time: 310.6s\n",
      "Epoch 7 Batch 700/703  AvgLoss: 1.7761  Time: 337.0s\n",
      "Epoch 7 Batch 703/703  AvgLoss: 1.7764  Time: 338.5s\n",
      "Epoch 7 VALID -> Loss: 2.3257  Top1: 76.960  Top5: 95.780\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 8/120 - lr: 0.024981 - starting training...\n",
      "Epoch 8 Batch 50/703  AvgLoss: 1.7940  Time: 25.3s\n",
      "Epoch 8 Batch 100/703  AvgLoss: 1.7297  Time: 49.1s\n",
      "Epoch 8 Batch 150/703  AvgLoss: 1.7446  Time: 73.5s\n",
      "Epoch 8 Batch 200/703  AvgLoss: 1.7625  Time: 98.2s\n",
      "Epoch 8 Batch 250/703  AvgLoss: 1.7297  Time: 122.3s\n",
      "Epoch 8 Batch 300/703  AvgLoss: 1.7199  Time: 147.0s\n",
      "Epoch 8 Batch 350/703  AvgLoss: 1.7356  Time: 169.5s\n",
      "Epoch 8 Batch 400/703  AvgLoss: 1.7232  Time: 194.1s\n",
      "Epoch 8 Batch 450/703  AvgLoss: 1.7279  Time: 218.6s\n",
      "Epoch 8 Batch 500/703  AvgLoss: 1.7236  Time: 243.1s\n",
      "Epoch 8 Batch 550/703  AvgLoss: 1.7338  Time: 267.4s\n",
      "Epoch 8 Batch 600/703  AvgLoss: 1.7230  Time: 291.8s\n",
      "Epoch 8 Batch 650/703  AvgLoss: 1.7367  Time: 316.4s\n",
      "Epoch 8 Batch 700/703  AvgLoss: 1.7300  Time: 340.8s\n",
      "Epoch 8 Batch 703/703  AvgLoss: 1.7305  Time: 342.2s\n",
      "Epoch 8 VALID -> Loss: 2.0875  Top1: 75.620  Top5: 94.960\n",
      "\n",
      "Epoch 9/120 - lr: 0.024958 - starting training...\n",
      "Epoch 9 Batch 50/703  AvgLoss: 1.7937  Time: 25.9s\n",
      "Epoch 9 Batch 100/703  AvgLoss: 1.7334  Time: 49.9s\n",
      "Epoch 9 Batch 150/703  AvgLoss: 1.6609  Time: 73.7s\n",
      "Epoch 9 Batch 200/703  AvgLoss: 1.6544  Time: 97.3s\n",
      "Epoch 9 Batch 250/703  AvgLoss: 1.6137  Time: 120.6s\n",
      "Epoch 9 Batch 300/703  AvgLoss: 1.6502  Time: 144.5s\n",
      "Epoch 9 Batch 350/703  AvgLoss: 1.6625  Time: 168.4s\n",
      "Epoch 9 Batch 400/703  AvgLoss: 1.6775  Time: 190.5s\n",
      "Epoch 9 Batch 450/703  AvgLoss: 1.6834  Time: 214.4s\n",
      "Epoch 9 Batch 500/703  AvgLoss: 1.6962  Time: 238.5s\n",
      "Epoch 9 Batch 550/703  AvgLoss: 1.6911  Time: 262.4s\n",
      "Epoch 9 Batch 600/703  AvgLoss: 1.6940  Time: 286.3s\n",
      "Epoch 9 Batch 650/703  AvgLoss: 1.6973  Time: 310.6s\n",
      "Epoch 9 Batch 700/703  AvgLoss: 1.6937  Time: 334.3s\n",
      "Epoch 9 Batch 703/703  AvgLoss: 1.6925  Time: 335.8s\n",
      "Epoch 9 VALID -> Loss: 1.8930  Top1: 74.240  Top5: 94.040\n",
      "\n",
      "Epoch 10/120 - lr: 0.024925 - starting training...\n",
      "Epoch 10 Batch 50/703  AvgLoss: 1.6243  Time: 25.1s\n",
      "Epoch 10 Batch 100/703  AvgLoss: 1.5363  Time: 48.7s\n",
      "Epoch 10 Batch 150/703  AvgLoss: 1.5673  Time: 72.4s\n",
      "Epoch 10 Batch 200/703  AvgLoss: 1.5452  Time: 96.3s\n",
      "Epoch 10 Batch 250/703  AvgLoss: 1.5114  Time: 121.4s\n",
      "Epoch 10 Batch 300/703  AvgLoss: 1.5291  Time: 145.7s\n",
      "Epoch 10 Batch 350/703  AvgLoss: 1.5419  Time: 169.6s\n",
      "Epoch 10 Batch 400/703  AvgLoss: 1.5545  Time: 193.6s\n",
      "Epoch 10 Batch 450/703  AvgLoss: 1.5552  Time: 220.0s\n",
      "Epoch 10 Batch 500/703  AvgLoss: 1.5651  Time: 242.8s\n",
      "Epoch 10 Batch 550/703  AvgLoss: 1.5799  Time: 266.7s\n",
      "Epoch 10 Batch 600/703  AvgLoss: 1.5831  Time: 290.9s\n",
      "Epoch 10 Batch 650/703  AvgLoss: 1.5855  Time: 314.9s\n",
      "Epoch 10 Batch 700/703  AvgLoss: 1.6003  Time: 338.4s\n",
      "Epoch 10 Batch 703/703  AvgLoss: 1.6030  Time: 339.9s\n",
      "Epoch 10 VALID -> Loss: 1.7280  Top1: 73.440  Top5: 93.500\n",
      "\n",
      "Epoch 11/120 - lr: 0.024884 - starting training...\n",
      "Epoch 11 Batch 50/703  AvgLoss: 1.4958  Time: 26.5s\n",
      "Epoch 11 Batch 100/703  AvgLoss: 1.5663  Time: 49.5s\n",
      "Epoch 11 Batch 150/703  AvgLoss: 1.6395  Time: 74.0s\n",
      "Epoch 11 Batch 200/703  AvgLoss: 1.6391  Time: 98.4s\n",
      "Epoch 11 Batch 250/703  AvgLoss: 1.6257  Time: 123.2s\n",
      "Epoch 11 Batch 300/703  AvgLoss: 1.6305  Time: 147.9s\n",
      "Epoch 11 Batch 350/703  AvgLoss: 1.6133  Time: 172.5s\n",
      "Epoch 11 Batch 400/703  AvgLoss: 1.6030  Time: 197.4s\n",
      "Epoch 11 Batch 450/703  AvgLoss: 1.6016  Time: 222.2s\n",
      "Epoch 11 Batch 500/703  AvgLoss: 1.5993  Time: 245.0s\n",
      "Epoch 11 Batch 550/703  AvgLoss: 1.5960  Time: 269.8s\n",
      "Epoch 11 Batch 600/703  AvgLoss: 1.6095  Time: 294.7s\n",
      "Epoch 11 Batch 650/703  AvgLoss: 1.6213  Time: 319.2s\n",
      "Epoch 11 Batch 700/703  AvgLoss: 1.6248  Time: 343.5s\n",
      "Epoch 11 Batch 703/703  AvgLoss: 1.6253  Time: 344.9s\n",
      "Epoch 11 VALID -> Loss: 1.5897  Top1: 72.740  Top5: 92.980\n",
      "\n",
      "Epoch 12/120 - lr: 0.024832 - starting training...\n",
      "Epoch 12 Batch 50/703  AvgLoss: 1.6282  Time: 25.4s\n",
      "Epoch 12 Batch 100/703  AvgLoss: 1.6146  Time: 49.6s\n",
      "Epoch 12 Batch 150/703  AvgLoss: 1.5951  Time: 71.7s\n",
      "Epoch 12 Batch 200/703  AvgLoss: 1.5909  Time: 95.7s\n",
      "Epoch 12 Batch 250/703  AvgLoss: 1.6091  Time: 119.9s\n",
      "Epoch 12 Batch 300/703  AvgLoss: 1.5728  Time: 143.6s\n",
      "Epoch 12 Batch 350/703  AvgLoss: 1.5665  Time: 167.8s\n",
      "Epoch 12 Batch 400/703  AvgLoss: 1.5612  Time: 191.7s\n",
      "Epoch 12 Batch 450/703  AvgLoss: 1.5567  Time: 215.7s\n",
      "Epoch 12 Batch 500/703  AvgLoss: 1.5305  Time: 239.1s\n",
      "Epoch 12 Batch 550/703  AvgLoss: 1.5210  Time: 262.5s\n",
      "Epoch 12 Batch 600/703  AvgLoss: 1.5289  Time: 286.8s\n",
      "Epoch 12 Batch 650/703  AvgLoss: 1.5303  Time: 311.3s\n",
      "Epoch 12 Batch 700/703  AvgLoss: 1.5330  Time: 335.3s\n",
      "Epoch 12 Batch 703/703  AvgLoss: 1.5359  Time: 336.8s\n",
      "Epoch 12 VALID -> Loss: 1.4624  Top1: 72.720  Top5: 93.100\n",
      "\n",
      "Epoch 13/120 - lr: 0.024772 - starting training...\n",
      "Epoch 13 Batch 50/703  AvgLoss: 1.5382  Time: 25.5s\n",
      "Epoch 13 Batch 100/703  AvgLoss: 1.5233  Time: 49.6s\n",
      "Epoch 13 Batch 150/703  AvgLoss: 1.5448  Time: 74.3s\n",
      "Epoch 13 Batch 200/703  AvgLoss: 1.5970  Time: 99.1s\n",
      "Epoch 13 Batch 250/703  AvgLoss: 1.5628  Time: 121.8s\n",
      "Epoch 13 Batch 300/703  AvgLoss: 1.5858  Time: 145.5s\n",
      "Epoch 13 Batch 350/703  AvgLoss: 1.5796  Time: 169.4s\n",
      "Epoch 13 Batch 400/703  AvgLoss: 1.5917  Time: 193.5s\n",
      "Epoch 13 Batch 450/703  AvgLoss: 1.5767  Time: 217.3s\n",
      "Epoch 13 Batch 500/703  AvgLoss: 1.5766  Time: 240.8s\n",
      "Epoch 13 Batch 550/703  AvgLoss: 1.5851  Time: 265.3s\n",
      "Epoch 13 Batch 600/703  AvgLoss: 1.5880  Time: 288.7s\n",
      "Epoch 13 Batch 650/703  AvgLoss: 1.5905  Time: 312.3s\n",
      "Epoch 13 Batch 700/703  AvgLoss: 1.5885  Time: 337.2s\n",
      "Epoch 13 Batch 703/703  AvgLoss: 1.5885  Time: 338.7s\n",
      "Epoch 13 VALID -> Loss: 1.3512  Top1: 73.020  Top5: 93.340\n",
      "\n",
      "Epoch 14/120 - lr: 0.024703 - starting training...\n",
      "Epoch 14 Batch 50/703  AvgLoss: 1.6044  Time: 26.2s\n",
      "Epoch 14 Batch 100/703  AvgLoss: 1.4989  Time: 51.2s\n",
      "Epoch 14 Batch 150/703  AvgLoss: 1.4657  Time: 75.5s\n",
      "Epoch 14 Batch 200/703  AvgLoss: 1.4456  Time: 100.3s\n",
      "Epoch 14 Batch 250/703  AvgLoss: 1.4524  Time: 122.9s\n",
      "Epoch 14 Batch 300/703  AvgLoss: 1.4488  Time: 147.8s\n",
      "Epoch 14 Batch 350/703  AvgLoss: 1.4790  Time: 172.6s\n",
      "Epoch 14 Batch 400/703  AvgLoss: 1.4834  Time: 197.3s\n",
      "Epoch 14 Batch 450/703  AvgLoss: 1.4713  Time: 221.9s\n",
      "Epoch 14 Batch 500/703  AvgLoss: 1.4856  Time: 245.9s\n",
      "Epoch 14 Batch 550/703  AvgLoss: 1.4764  Time: 269.6s\n",
      "Epoch 14 Batch 600/703  AvgLoss: 1.4714  Time: 293.6s\n",
      "Epoch 14 Batch 650/703  AvgLoss: 1.4853  Time: 315.6s\n",
      "Epoch 14 Batch 700/703  AvgLoss: 1.4843  Time: 339.3s\n",
      "Epoch 14 Batch 703/703  AvgLoss: 1.4813  Time: 340.7s\n",
      "Epoch 14 VALID -> Loss: 1.2430  Top1: 73.840  Top5: 93.820\n",
      "\n",
      "Epoch 15/120 - lr: 0.024624 - starting training...\n",
      "Epoch 15 Batch 50/703  AvgLoss: 1.3970  Time: 25.5s\n",
      "Epoch 15 Batch 100/703  AvgLoss: 1.4925  Time: 49.3s\n",
      "Epoch 15 Batch 150/703  AvgLoss: 1.5177  Time: 73.3s\n",
      "Epoch 15 Batch 200/703  AvgLoss: 1.5342  Time: 97.2s\n",
      "Epoch 15 Batch 250/703  AvgLoss: 1.4914  Time: 120.3s\n",
      "Epoch 15 Batch 300/703  AvgLoss: 1.4705  Time: 143.2s\n",
      "Epoch 15 Batch 350/703  AvgLoss: 1.4713  Time: 167.3s\n",
      "Epoch 15 Batch 400/703  AvgLoss: 1.4733  Time: 191.3s\n",
      "Epoch 15 Batch 450/703  AvgLoss: 1.4926  Time: 215.1s\n",
      "Epoch 15 Batch 500/703  AvgLoss: 1.4951  Time: 239.1s\n",
      "Epoch 15 Batch 550/703  AvgLoss: 1.5041  Time: 262.9s\n",
      "Epoch 15 Batch 600/703  AvgLoss: 1.5057  Time: 287.0s\n",
      "Epoch 15 Batch 650/703  AvgLoss: 1.5074  Time: 310.3s\n",
      "Epoch 15 Batch 700/703  AvgLoss: 1.5070  Time: 336.0s\n",
      "Epoch 15 Batch 703/703  AvgLoss: 1.5048  Time: 337.5s\n",
      "Epoch 15 VALID -> Loss: 1.1535  Top1: 74.880  Top5: 94.460\n",
      "\n",
      "Epoch 16/120 - lr: 0.024536 - starting training...\n",
      "Epoch 16 Batch 50/703  AvgLoss: 1.6264  Time: 25.8s\n",
      "Epoch 16 Batch 100/703  AvgLoss: 1.5920  Time: 48.9s\n",
      "Epoch 16 Batch 150/703  AvgLoss: 1.5597  Time: 72.3s\n",
      "Epoch 16 Batch 200/703  AvgLoss: 1.5122  Time: 95.9s\n",
      "Epoch 16 Batch 250/703  AvgLoss: 1.5152  Time: 119.6s\n",
      "Epoch 16 Batch 300/703  AvgLoss: 1.5215  Time: 143.5s\n",
      "Epoch 16 Batch 350/703  AvgLoss: 1.5013  Time: 166.8s\n",
      "Epoch 16 Batch 400/703  AvgLoss: 1.4913  Time: 189.5s\n",
      "Epoch 16 Batch 450/703  AvgLoss: 1.4963  Time: 213.8s\n",
      "Epoch 16 Batch 500/703  AvgLoss: 1.5222  Time: 238.4s\n",
      "Epoch 16 Batch 550/703  AvgLoss: 1.5209  Time: 262.5s\n",
      "Epoch 16 Batch 600/703  AvgLoss: 1.5186  Time: 286.8s\n",
      "Epoch 16 Batch 650/703  AvgLoss: 1.5186  Time: 311.2s\n",
      "Epoch 16 Batch 700/703  AvgLoss: 1.5117  Time: 336.0s\n",
      "Epoch 16 Batch 703/703  AvgLoss: 1.5138  Time: 337.5s\n",
      "Epoch 16 VALID -> Loss: 1.0741  Top1: 76.180  Top5: 95.120\n",
      "\n",
      "Epoch 17/120 - lr: 0.024440 - starting training...\n",
      "Epoch 17 Batch 50/703  AvgLoss: 1.4044  Time: 26.6s\n",
      "Epoch 17 Batch 100/703  AvgLoss: 1.4583  Time: 50.8s\n",
      "Epoch 17 Batch 150/703  AvgLoss: 1.4716  Time: 75.1s\n",
      "Epoch 17 Batch 200/703  AvgLoss: 1.4594  Time: 99.5s\n",
      "Epoch 17 Batch 250/703  AvgLoss: 1.4517  Time: 123.7s\n",
      "Epoch 17 Batch 300/703  AvgLoss: 1.4470  Time: 148.6s\n",
      "Epoch 17 Batch 350/703  AvgLoss: 1.4380  Time: 172.7s\n",
      "Epoch 17 Batch 400/703  AvgLoss: 1.4362  Time: 195.0s\n",
      "Epoch 17 Batch 450/703  AvgLoss: 1.4332  Time: 218.5s\n",
      "Epoch 17 Batch 500/703  AvgLoss: 1.4307  Time: 242.4s\n",
      "Epoch 17 Batch 550/703  AvgLoss: 1.4591  Time: 266.1s\n",
      "Epoch 17 Batch 600/703  AvgLoss: 1.4534  Time: 289.9s\n",
      "Epoch 17 Batch 650/703  AvgLoss: 1.4425  Time: 313.8s\n",
      "Epoch 17 Batch 700/703  AvgLoss: 1.4474  Time: 337.7s\n",
      "Epoch 17 Batch 703/703  AvgLoss: 1.4489  Time: 339.2s\n",
      "Epoch 17 VALID -> Loss: 1.0024  Top1: 77.240  Top5: 95.540\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 18/120 - lr: 0.024334 - starting training...\n",
      "Epoch 18 Batch 50/703  AvgLoss: 1.4014  Time: 24.1s\n",
      "Epoch 18 Batch 100/703  AvgLoss: 1.4463  Time: 47.7s\n",
      "Epoch 18 Batch 150/703  AvgLoss: 1.4213  Time: 71.5s\n",
      "Epoch 18 Batch 200/703  AvgLoss: 1.3880  Time: 95.7s\n",
      "Epoch 18 Batch 250/703  AvgLoss: 1.3939  Time: 120.2s\n",
      "Epoch 18 Batch 300/703  AvgLoss: 1.3854  Time: 143.9s\n",
      "Epoch 18 Batch 350/703  AvgLoss: 1.4076  Time: 168.3s\n",
      "Epoch 18 Batch 400/703  AvgLoss: 1.4218  Time: 191.8s\n",
      "Epoch 18 Batch 450/703  AvgLoss: 1.4345  Time: 218.0s\n",
      "Epoch 18 Batch 500/703  AvgLoss: 1.4353  Time: 240.8s\n",
      "Epoch 18 Batch 550/703  AvgLoss: 1.4283  Time: 264.6s\n",
      "Epoch 18 Batch 600/703  AvgLoss: 1.4342  Time: 288.4s\n",
      "Epoch 18 Batch 650/703  AvgLoss: 1.4359  Time: 312.3s\n",
      "Epoch 18 Batch 700/703  AvgLoss: 1.4402  Time: 336.1s\n",
      "Epoch 18 Batch 703/703  AvgLoss: 1.4419  Time: 337.5s\n",
      "Epoch 18 VALID -> Loss: 0.9354  Top1: 78.760  Top5: 95.860\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 19/120 - lr: 0.024220 - starting training...\n",
      "Epoch 19 Batch 50/703  AvgLoss: 1.5066  Time: 25.5s\n",
      "Epoch 19 Batch 100/703  AvgLoss: 1.3881  Time: 47.9s\n",
      "Epoch 19 Batch 150/703  AvgLoss: 1.4114  Time: 71.8s\n",
      "Epoch 19 Batch 200/703  AvgLoss: 1.3691  Time: 95.7s\n",
      "Epoch 19 Batch 250/703  AvgLoss: 1.3943  Time: 119.3s\n",
      "Epoch 19 Batch 300/703  AvgLoss: 1.4080  Time: 143.8s\n",
      "Epoch 19 Batch 350/703  AvgLoss: 1.4304  Time: 168.3s\n",
      "Epoch 19 Batch 400/703  AvgLoss: 1.4444  Time: 193.0s\n",
      "Epoch 19 Batch 450/703  AvgLoss: 1.4640  Time: 217.6s\n",
      "Epoch 19 Batch 500/703  AvgLoss: 1.4679  Time: 240.2s\n",
      "Epoch 19 Batch 550/703  AvgLoss: 1.4683  Time: 264.4s\n",
      "Epoch 19 Batch 600/703  AvgLoss: 1.4558  Time: 288.7s\n",
      "Epoch 19 Batch 650/703  AvgLoss: 1.4479  Time: 313.3s\n",
      "Epoch 19 Batch 700/703  AvgLoss: 1.4545  Time: 337.8s\n",
      "Epoch 19 Batch 703/703  AvgLoss: 1.4521  Time: 339.2s\n",
      "Epoch 19 VALID -> Loss: 0.8780  Top1: 79.560  Top5: 96.100\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 20/120 - lr: 0.024097 - starting training...\n",
      "Epoch 20 Batch 50/703  AvgLoss: 1.4702  Time: 26.0s\n",
      "Epoch 20 Batch 100/703  AvgLoss: 1.4746  Time: 50.7s\n",
      "Epoch 20 Batch 150/703  AvgLoss: 1.4049  Time: 72.9s\n",
      "Epoch 20 Batch 200/703  AvgLoss: 1.4115  Time: 96.6s\n",
      "Epoch 20 Batch 250/703  AvgLoss: 1.4321  Time: 120.7s\n",
      "Epoch 20 Batch 300/703  AvgLoss: 1.4395  Time: 144.6s\n",
      "Epoch 20 Batch 350/703  AvgLoss: 1.4578  Time: 168.1s\n",
      "Epoch 20 Batch 400/703  AvgLoss: 1.4617  Time: 192.0s\n",
      "Epoch 20 Batch 450/703  AvgLoss: 1.4517  Time: 215.7s\n",
      "Epoch 20 Batch 500/703  AvgLoss: 1.4376  Time: 239.6s\n",
      "Epoch 20 Batch 550/703  AvgLoss: 1.4386  Time: 261.5s\n",
      "Epoch 20 Batch 600/703  AvgLoss: 1.4365  Time: 285.2s\n",
      "Epoch 20 Batch 650/703  AvgLoss: 1.4352  Time: 309.0s\n",
      "Epoch 20 Batch 700/703  AvgLoss: 1.4333  Time: 332.6s\n",
      "Epoch 20 Batch 703/703  AvgLoss: 1.4359  Time: 334.1s\n",
      "Epoch 20 VALID -> Loss: 0.8251  Top1: 80.220  Top5: 96.320\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 21/120 - lr: 0.023965 - starting training...\n",
      "Epoch 21 Batch 50/703  AvgLoss: 1.4823  Time: 25.8s\n",
      "Epoch 21 Batch 100/703  AvgLoss: 1.4328  Time: 49.8s\n",
      "Epoch 21 Batch 150/703  AvgLoss: 1.4102  Time: 73.4s\n",
      "Epoch 21 Batch 200/703  AvgLoss: 1.4006  Time: 98.9s\n",
      "Epoch 21 Batch 250/703  AvgLoss: 1.4198  Time: 122.2s\n",
      "Epoch 21 Batch 300/703  AvgLoss: 1.4096  Time: 146.1s\n",
      "Epoch 21 Batch 350/703  AvgLoss: 1.3872  Time: 169.8s\n",
      "Epoch 21 Batch 400/703  AvgLoss: 1.3825  Time: 194.2s\n",
      "Epoch 21 Batch 450/703  AvgLoss: 1.3635  Time: 217.8s\n",
      "Epoch 21 Batch 500/703  AvgLoss: 1.3755  Time: 241.5s\n",
      "Epoch 21 Batch 550/703  AvgLoss: 1.3785  Time: 265.2s\n",
      "Epoch 21 Batch 600/703  AvgLoss: 1.3876  Time: 288.9s\n",
      "Epoch 21 Batch 650/703  AvgLoss: 1.3804  Time: 311.1s\n",
      "Epoch 21 Batch 700/703  AvgLoss: 1.3899  Time: 335.3s\n",
      "Epoch 21 Batch 703/703  AvgLoss: 1.3898  Time: 336.8s\n",
      "Epoch 21 VALID -> Loss: 0.7801  Top1: 81.120  Top5: 96.560\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 22/120 - lr: 0.023825 - starting training...\n",
      "Epoch 22 Batch 50/703  AvgLoss: 1.4170  Time: 25.1s\n",
      "Epoch 22 Batch 100/703  AvgLoss: 1.3533  Time: 49.4s\n",
      "Epoch 22 Batch 150/703  AvgLoss: 1.3970  Time: 74.0s\n",
      "Epoch 22 Batch 200/703  AvgLoss: 1.3749  Time: 98.6s\n",
      "Epoch 22 Batch 250/703  AvgLoss: 1.3893  Time: 123.1s\n",
      "Epoch 22 Batch 300/703  AvgLoss: 1.3855  Time: 146.3s\n",
      "Epoch 22 Batch 350/703  AvgLoss: 1.3760  Time: 170.9s\n",
      "Epoch 22 Batch 400/703  AvgLoss: 1.3866  Time: 195.7s\n",
      "Epoch 22 Batch 450/703  AvgLoss: 1.3819  Time: 220.1s\n",
      "Epoch 22 Batch 500/703  AvgLoss: 1.3989  Time: 244.6s\n",
      "Epoch 22 Batch 550/703  AvgLoss: 1.4029  Time: 269.0s\n",
      "Epoch 22 Batch 600/703  AvgLoss: 1.3987  Time: 293.3s\n",
      "Epoch 22 Batch 650/703  AvgLoss: 1.3988  Time: 317.6s\n",
      "Epoch 22 Batch 700/703  AvgLoss: 1.3930  Time: 340.3s\n",
      "Epoch 22 Batch 703/703  AvgLoss: 1.3937  Time: 341.8s\n",
      "Epoch 22 VALID -> Loss: 0.7411  Top1: 82.100  Top5: 96.640\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 23/120 - lr: 0.023676 - starting training...\n",
      "Epoch 23 Batch 50/703  AvgLoss: 1.3653  Time: 25.6s\n",
      "Epoch 23 Batch 100/703  AvgLoss: 1.3442  Time: 50.0s\n",
      "Epoch 23 Batch 150/703  AvgLoss: 1.3743  Time: 74.0s\n",
      "Epoch 23 Batch 200/703  AvgLoss: 1.3956  Time: 97.8s\n",
      "Epoch 23 Batch 250/703  AvgLoss: 1.3366  Time: 121.6s\n",
      "Epoch 23 Batch 300/703  AvgLoss: 1.3280  Time: 143.9s\n",
      "Epoch 23 Batch 350/703  AvgLoss: 1.3587  Time: 167.6s\n",
      "Epoch 23 Batch 400/703  AvgLoss: 1.3685  Time: 191.5s\n",
      "Epoch 23 Batch 450/703  AvgLoss: 1.3680  Time: 215.7s\n",
      "Epoch 23 Batch 500/703  AvgLoss: 1.3722  Time: 239.5s\n",
      "Epoch 23 Batch 550/703  AvgLoss: 1.3730  Time: 263.4s\n",
      "Epoch 23 Batch 600/703  AvgLoss: 1.3783  Time: 286.9s\n",
      "Epoch 23 Batch 650/703  AvgLoss: 1.3849  Time: 310.7s\n",
      "Epoch 23 Batch 700/703  AvgLoss: 1.3788  Time: 335.3s\n",
      "Epoch 23 Batch 703/703  AvgLoss: 1.3815  Time: 336.8s\n",
      "Epoch 23 VALID -> Loss: 0.7078  Top1: 82.800  Top5: 96.920\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 24/120 - lr: 0.023519 - starting training...\n",
      "Epoch 24 Batch 50/703  AvgLoss: 1.3951  Time: 26.2s\n",
      "Epoch 24 Batch 100/703  AvgLoss: 1.4253  Time: 50.0s\n",
      "Epoch 24 Batch 150/703  AvgLoss: 1.3798  Time: 74.1s\n",
      "Epoch 24 Batch 200/703  AvgLoss: 1.3714  Time: 97.8s\n",
      "Epoch 24 Batch 250/703  AvgLoss: 1.3925  Time: 121.8s\n",
      "Epoch 24 Batch 300/703  AvgLoss: 1.3904  Time: 146.0s\n",
      "Epoch 24 Batch 350/703  AvgLoss: 1.3967  Time: 169.9s\n",
      "Epoch 24 Batch 400/703  AvgLoss: 1.4072  Time: 192.3s\n",
      "Epoch 24 Batch 450/703  AvgLoss: 1.4068  Time: 216.0s\n",
      "Epoch 24 Batch 500/703  AvgLoss: 1.4099  Time: 239.9s\n",
      "Epoch 24 Batch 550/703  AvgLoss: 1.4022  Time: 263.6s\n",
      "Epoch 24 Batch 600/703  AvgLoss: 1.4041  Time: 287.6s\n",
      "Epoch 24 Batch 650/703  AvgLoss: 1.4136  Time: 311.3s\n",
      "Epoch 24 Batch 700/703  AvgLoss: 1.4125  Time: 335.3s\n",
      "Epoch 24 Batch 703/703  AvgLoss: 1.4131  Time: 336.7s\n",
      "Epoch 24 VALID -> Loss: 0.6801  Top1: 83.620  Top5: 96.900\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 25/120 - lr: 0.023354 - starting training...\n",
      "Epoch 25 Batch 50/703  AvgLoss: 1.4185  Time: 26.0s\n",
      "Epoch 25 Batch 100/703  AvgLoss: 1.4350  Time: 50.8s\n",
      "Epoch 25 Batch 150/703  AvgLoss: 1.4249  Time: 75.6s\n",
      "Epoch 25 Batch 200/703  AvgLoss: 1.4057  Time: 100.5s\n",
      "Epoch 25 Batch 250/703  AvgLoss: 1.3642  Time: 125.2s\n",
      "Epoch 25 Batch 300/703  AvgLoss: 1.3430  Time: 149.7s\n",
      "Epoch 25 Batch 350/703  AvgLoss: 1.3274  Time: 174.4s\n",
      "Epoch 25 Batch 400/703  AvgLoss: 1.3382  Time: 197.3s\n",
      "Epoch 25 Batch 450/703  AvgLoss: 1.3222  Time: 221.5s\n",
      "Epoch 25 Batch 500/703  AvgLoss: 1.3307  Time: 246.2s\n",
      "Epoch 25 Batch 550/703  AvgLoss: 1.3306  Time: 270.8s\n",
      "Epoch 25 Batch 600/703  AvgLoss: 1.3410  Time: 295.3s\n",
      "Epoch 25 Batch 650/703  AvgLoss: 1.3429  Time: 319.4s\n",
      "Epoch 25 Batch 700/703  AvgLoss: 1.3425  Time: 343.7s\n",
      "Epoch 25 Batch 703/703  AvgLoss: 1.3395  Time: 345.1s\n",
      "Epoch 25 VALID -> Loss: 0.6559  Top1: 84.020  Top5: 97.060\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 26/120 - lr: 0.023180 - starting training...\n",
      "Epoch 26 Batch 50/703  AvgLoss: 1.3547  Time: 24.4s\n",
      "Epoch 26 Batch 100/703  AvgLoss: 1.3440  Time: 48.2s\n",
      "Epoch 26 Batch 150/703  AvgLoss: 1.3412  Time: 72.1s\n",
      "Epoch 26 Batch 200/703  AvgLoss: 1.3567  Time: 96.0s\n",
      "Epoch 26 Batch 250/703  AvgLoss: 1.3360  Time: 119.9s\n",
      "Epoch 26 Batch 300/703  AvgLoss: 1.3238  Time: 143.8s\n",
      "Epoch 26 Batch 350/703  AvgLoss: 1.3261  Time: 167.8s\n",
      "Epoch 26 Batch 400/703  AvgLoss: 1.3286  Time: 191.9s\n",
      "Epoch 26 Batch 450/703  AvgLoss: 1.3383  Time: 217.0s\n",
      "Epoch 26 Batch 500/703  AvgLoss: 1.3640  Time: 240.6s\n",
      "Epoch 26 Batch 550/703  AvgLoss: 1.3483  Time: 264.4s\n",
      "Epoch 26 Batch 600/703  AvgLoss: 1.3613  Time: 288.4s\n",
      "Epoch 26 Batch 650/703  AvgLoss: 1.3481  Time: 312.1s\n",
      "Epoch 26 Batch 700/703  AvgLoss: 1.3555  Time: 336.2s\n",
      "Epoch 26 Batch 703/703  AvgLoss: 1.3582  Time: 337.5s\n",
      "Epoch 26 VALID -> Loss: 0.6361  Top1: 84.740  Top5: 97.120\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 27/120 - lr: 0.022999 - starting training...\n",
      "Epoch 27 Batch 50/703  AvgLoss: 1.3316  Time: 25.2s\n",
      "Epoch 27 Batch 100/703  AvgLoss: 1.3217  Time: 49.0s\n",
      "Epoch 27 Batch 150/703  AvgLoss: 1.3225  Time: 71.0s\n",
      "Epoch 27 Batch 200/703  AvgLoss: 1.3485  Time: 95.1s\n",
      "Epoch 27 Batch 250/703  AvgLoss: 1.3680  Time: 119.2s\n",
      "Epoch 27 Batch 300/703  AvgLoss: 1.3885  Time: 142.6s\n",
      "Epoch 27 Batch 350/703  AvgLoss: 1.3754  Time: 166.1s\n",
      "Epoch 27 Batch 400/703  AvgLoss: 1.3787  Time: 190.1s\n",
      "Epoch 27 Batch 450/703  AvgLoss: 1.4008  Time: 213.6s\n",
      "Epoch 27 Batch 500/703  AvgLoss: 1.4087  Time: 237.3s\n",
      "Epoch 27 Batch 550/703  AvgLoss: 1.4118  Time: 259.8s\n",
      "Epoch 27 Batch 600/703  AvgLoss: 1.3963  Time: 284.6s\n",
      "Epoch 27 Batch 650/703  AvgLoss: 1.3947  Time: 308.2s\n",
      "Epoch 27 Batch 700/703  AvgLoss: 1.3869  Time: 332.6s\n",
      "Epoch 27 Batch 703/703  AvgLoss: 1.3872  Time: 334.1s\n",
      "Epoch 27 VALID -> Loss: 0.6203  Top1: 84.960  Top5: 97.220\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 28/120 - lr: 0.022810 - starting training...\n",
      "Epoch 28 Batch 50/703  AvgLoss: 1.2665  Time: 26.1s\n",
      "Epoch 28 Batch 100/703  AvgLoss: 1.2693  Time: 50.4s\n",
      "Epoch 28 Batch 150/703  AvgLoss: 1.3001  Time: 73.2s\n",
      "Epoch 28 Batch 200/703  AvgLoss: 1.3379  Time: 97.7s\n",
      "Epoch 28 Batch 250/703  AvgLoss: 1.3300  Time: 122.0s\n",
      "Epoch 28 Batch 300/703  AvgLoss: 1.2956  Time: 146.5s\n",
      "Epoch 28 Batch 350/703  AvgLoss: 1.2934  Time: 171.1s\n",
      "Epoch 28 Batch 400/703  AvgLoss: 1.3037  Time: 195.2s\n",
      "Epoch 28 Batch 450/703  AvgLoss: 1.3129  Time: 219.7s\n",
      "Epoch 28 Batch 500/703  AvgLoss: 1.3257  Time: 243.6s\n",
      "Epoch 28 Batch 550/703  AvgLoss: 1.3234  Time: 265.6s\n",
      "Epoch 28 Batch 600/703  AvgLoss: 1.3261  Time: 289.4s\n",
      "Epoch 28 Batch 650/703  AvgLoss: 1.3338  Time: 313.1s\n",
      "Epoch 28 Batch 700/703  AvgLoss: 1.3253  Time: 337.0s\n",
      "Epoch 28 Batch 703/703  AvgLoss: 1.3223  Time: 338.4s\n",
      "Epoch 28 VALID -> Loss: 0.6073  Top1: 85.360  Top5: 97.320\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 29/120 - lr: 0.022613 - starting training...\n",
      "Epoch 29 Batch 50/703  AvgLoss: 1.3522  Time: 25.9s\n",
      "Epoch 29 Batch 100/703  AvgLoss: 1.4464  Time: 50.0s\n",
      "Epoch 29 Batch 150/703  AvgLoss: 1.4014  Time: 73.9s\n",
      "Epoch 29 Batch 200/703  AvgLoss: 1.3389  Time: 98.1s\n",
      "Epoch 29 Batch 250/703  AvgLoss: 1.3439  Time: 121.7s\n",
      "Epoch 29 Batch 300/703  AvgLoss: 1.3407  Time: 141.1s\n",
      "Epoch 29 Batch 350/703  AvgLoss: 1.3524  Time: 155.7s\n",
      "Epoch 29 Batch 400/703  AvgLoss: 1.3356  Time: 170.9s\n",
      "Epoch 29 Batch 450/703  AvgLoss: 1.3379  Time: 185.4s\n",
      "Epoch 29 Batch 500/703  AvgLoss: 1.3353  Time: 199.7s\n",
      "Epoch 29 Batch 550/703  AvgLoss: 1.3350  Time: 214.0s\n",
      "Epoch 29 Batch 600/703  AvgLoss: 1.3380  Time: 228.3s\n",
      "Epoch 29 Batch 650/703  AvgLoss: 1.3240  Time: 242.7s\n",
      "Epoch 29 Batch 700/703  AvgLoss: 1.3323  Time: 257.0s\n",
      "Epoch 29 Batch 703/703  AvgLoss: 1.3327  Time: 257.9s\n",
      "Epoch 29 VALID -> Loss: 0.5962  Top1: 85.880  Top5: 97.360\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 30/120 - lr: 0.022408 - starting training...\n",
      "Epoch 30 Batch 50/703  AvgLoss: 1.3748  Time: 15.5s\n",
      "Epoch 30 Batch 100/703  AvgLoss: 1.3964  Time: 29.7s\n",
      "Epoch 30 Batch 150/703  AvgLoss: 1.3647  Time: 44.0s\n",
      "Epoch 30 Batch 200/703  AvgLoss: 1.3658  Time: 58.2s\n",
      "Epoch 30 Batch 250/703  AvgLoss: 1.3589  Time: 72.6s\n",
      "Epoch 30 Batch 300/703  AvgLoss: 1.3304  Time: 86.9s\n",
      "Epoch 30 Batch 350/703  AvgLoss: 1.3127  Time: 101.2s\n",
      "Epoch 30 Batch 400/703  AvgLoss: 1.3316  Time: 115.5s\n",
      "Epoch 30 Batch 450/703  AvgLoss: 1.3429  Time: 129.7s\n",
      "Epoch 30 Batch 500/703  AvgLoss: 1.3502  Time: 143.9s\n",
      "Epoch 30 Batch 550/703  AvgLoss: 1.3534  Time: 158.2s\n",
      "Epoch 30 Batch 600/703  AvgLoss: 1.3390  Time: 172.5s\n",
      "Epoch 30 Batch 650/703  AvgLoss: 1.3359  Time: 186.7s\n",
      "Epoch 30 Batch 700/703  AvgLoss: 1.3310  Time: 200.8s\n",
      "Epoch 30 Batch 703/703  AvgLoss: 1.3289  Time: 201.7s\n",
      "Epoch 30 VALID -> Loss: 0.5877  Top1: 86.160  Top5: 97.420\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 31/120 - lr: 0.022197 - starting training...\n",
      "Epoch 31 Batch 50/703  AvgLoss: 1.2606  Time: 15.8s\n",
      "Epoch 31 Batch 100/703  AvgLoss: 1.3288  Time: 30.1s\n",
      "Epoch 31 Batch 150/703  AvgLoss: 1.3231  Time: 44.5s\n",
      "Epoch 31 Batch 200/703  AvgLoss: 1.3210  Time: 58.8s\n",
      "Epoch 31 Batch 250/703  AvgLoss: 1.3217  Time: 73.0s\n",
      "Epoch 31 Batch 300/703  AvgLoss: 1.3174  Time: 87.3s\n",
      "Epoch 31 Batch 350/703  AvgLoss: 1.3157  Time: 101.5s\n",
      "Epoch 31 Batch 400/703  AvgLoss: 1.3369  Time: 115.8s\n",
      "Epoch 31 Batch 450/703  AvgLoss: 1.3346  Time: 130.2s\n",
      "Epoch 31 Batch 500/703  AvgLoss: 1.3463  Time: 144.5s\n",
      "Epoch 31 Batch 550/703  AvgLoss: 1.3572  Time: 158.8s\n",
      "Epoch 31 Batch 600/703  AvgLoss: 1.3664  Time: 173.1s\n",
      "Epoch 31 Batch 650/703  AvgLoss: 1.3698  Time: 187.4s\n",
      "Epoch 31 Batch 700/703  AvgLoss: 1.3765  Time: 201.6s\n",
      "Epoch 31 Batch 703/703  AvgLoss: 1.3737  Time: 202.5s\n",
      "Epoch 31 VALID -> Loss: 0.5818  Top1: 86.360  Top5: 97.360\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 32/120 - lr: 0.021977 - starting training...\n",
      "Epoch 32 Batch 50/703  AvgLoss: 1.4058  Time: 16.1s\n",
      "Epoch 32 Batch 100/703  AvgLoss: 1.4083  Time: 30.4s\n",
      "Epoch 32 Batch 150/703  AvgLoss: 1.3814  Time: 44.6s\n",
      "Epoch 32 Batch 200/703  AvgLoss: 1.4059  Time: 58.9s\n",
      "Epoch 32 Batch 250/703  AvgLoss: 1.4369  Time: 73.3s\n",
      "Epoch 32 Batch 300/703  AvgLoss: 1.4234  Time: 87.6s\n",
      "Epoch 32 Batch 350/703  AvgLoss: 1.4178  Time: 102.0s\n",
      "Epoch 32 Batch 400/703  AvgLoss: 1.4070  Time: 116.2s\n",
      "Epoch 32 Batch 450/703  AvgLoss: 1.3983  Time: 130.4s\n",
      "Epoch 32 Batch 500/703  AvgLoss: 1.3965  Time: 144.7s\n",
      "Epoch 32 Batch 550/703  AvgLoss: 1.3909  Time: 159.0s\n",
      "Epoch 32 Batch 600/703  AvgLoss: 1.3854  Time: 173.2s\n",
      "Epoch 32 Batch 650/703  AvgLoss: 1.3739  Time: 187.6s\n",
      "Epoch 32 Batch 700/703  AvgLoss: 1.3700  Time: 202.0s\n",
      "Epoch 32 Batch 703/703  AvgLoss: 1.3714  Time: 202.8s\n",
      "Epoch 32 VALID -> Loss: 0.5771  Top1: 86.500  Top5: 97.320\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 33/120 - lr: 0.021751 - starting training...\n",
      "Epoch 33 Batch 50/703  AvgLoss: 1.1935  Time: 15.6s\n",
      "Epoch 33 Batch 100/703  AvgLoss: 1.2092  Time: 29.9s\n",
      "Epoch 33 Batch 150/703  AvgLoss: 1.2081  Time: 44.2s\n",
      "Epoch 33 Batch 200/703  AvgLoss: 1.2266  Time: 58.5s\n",
      "Epoch 33 Batch 250/703  AvgLoss: 1.2557  Time: 73.0s\n",
      "Epoch 33 Batch 300/703  AvgLoss: 1.2949  Time: 87.3s\n",
      "Epoch 33 Batch 350/703  AvgLoss: 1.3053  Time: 101.6s\n",
      "Epoch 33 Batch 400/703  AvgLoss: 1.3269  Time: 115.9s\n",
      "Epoch 33 Batch 450/703  AvgLoss: 1.3226  Time: 130.3s\n",
      "Epoch 33 Batch 500/703  AvgLoss: 1.3160  Time: 144.5s\n",
      "Epoch 33 Batch 550/703  AvgLoss: 1.3116  Time: 158.8s\n",
      "Epoch 33 Batch 600/703  AvgLoss: 1.2912  Time: 173.2s\n",
      "Epoch 33 Batch 650/703  AvgLoss: 1.2965  Time: 187.4s\n",
      "Epoch 33 Batch 700/703  AvgLoss: 1.2938  Time: 201.6s\n",
      "Epoch 33 Batch 703/703  AvgLoss: 1.2920  Time: 202.5s\n",
      "Epoch 33 VALID -> Loss: 0.5730  Top1: 86.780  Top5: 97.320\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 34/120 - lr: 0.021518 - starting training...\n",
      "Epoch 34 Batch 50/703  AvgLoss: 1.1544  Time: 16.0s\n",
      "Epoch 34 Batch 100/703  AvgLoss: 1.1572  Time: 30.3s\n",
      "Epoch 34 Batch 150/703  AvgLoss: 1.1889  Time: 44.7s\n",
      "Epoch 34 Batch 200/703  AvgLoss: 1.2308  Time: 58.9s\n",
      "Epoch 34 Batch 250/703  AvgLoss: 1.2727  Time: 73.1s\n",
      "Epoch 34 Batch 300/703  AvgLoss: 1.2881  Time: 87.4s\n",
      "Epoch 34 Batch 350/703  AvgLoss: 1.2898  Time: 101.6s\n",
      "Epoch 34 Batch 400/703  AvgLoss: 1.3087  Time: 115.9s\n",
      "Epoch 34 Batch 450/703  AvgLoss: 1.3072  Time: 130.2s\n",
      "Epoch 34 Batch 500/703  AvgLoss: 1.3129  Time: 144.5s\n",
      "Epoch 34 Batch 550/703  AvgLoss: 1.3119  Time: 158.8s\n",
      "Epoch 34 Batch 600/703  AvgLoss: 1.3084  Time: 173.0s\n",
      "Epoch 34 Batch 650/703  AvgLoss: 1.3082  Time: 187.3s\n",
      "Epoch 34 Batch 700/703  AvgLoss: 1.3042  Time: 201.7s\n",
      "Epoch 34 Batch 703/703  AvgLoss: 1.3042  Time: 202.5s\n",
      "Epoch 34 VALID -> Loss: 0.5704  Top1: 86.820  Top5: 97.380\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 35/120 - lr: 0.021278 - starting training...\n",
      "Epoch 35 Batch 50/703  AvgLoss: 1.4592  Time: 16.1s\n",
      "Epoch 35 Batch 100/703  AvgLoss: 1.4324  Time: 30.5s\n",
      "Epoch 35 Batch 150/703  AvgLoss: 1.3841  Time: 44.8s\n",
      "Epoch 35 Batch 200/703  AvgLoss: 1.3458  Time: 59.1s\n",
      "Epoch 35 Batch 250/703  AvgLoss: 1.3425  Time: 73.4s\n",
      "Epoch 35 Batch 300/703  AvgLoss: 1.3461  Time: 87.9s\n",
      "Epoch 35 Batch 350/703  AvgLoss: 1.3360  Time: 102.2s\n",
      "Epoch 35 Batch 400/703  AvgLoss: 1.3343  Time: 116.5s\n",
      "Epoch 35 Batch 450/703  AvgLoss: 1.3439  Time: 130.8s\n",
      "Epoch 35 Batch 500/703  AvgLoss: 1.3469  Time: 145.1s\n",
      "Epoch 35 Batch 550/703  AvgLoss: 1.3590  Time: 159.4s\n",
      "Epoch 35 Batch 600/703  AvgLoss: 1.3446  Time: 173.8s\n",
      "Epoch 35 Batch 650/703  AvgLoss: 1.3397  Time: 188.1s\n",
      "Epoch 35 Batch 700/703  AvgLoss: 1.3352  Time: 202.4s\n",
      "Epoch 35 Batch 703/703  AvgLoss: 1.3342  Time: 203.3s\n",
      "Epoch 35 VALID -> Loss: 0.5689  Top1: 86.700  Top5: 97.380\n",
      "\n",
      "Epoch 36/120 - lr: 0.021032 - starting training...\n",
      "Epoch 36 Batch 50/703  AvgLoss: 1.3283  Time: 15.9s\n",
      "Epoch 36 Batch 100/703  AvgLoss: 1.3403  Time: 30.2s\n",
      "Epoch 36 Batch 150/703  AvgLoss: 1.3569  Time: 44.6s\n",
      "Epoch 36 Batch 200/703  AvgLoss: 1.3292  Time: 58.8s\n",
      "Epoch 36 Batch 250/703  AvgLoss: 1.3162  Time: 73.0s\n",
      "Epoch 36 Batch 300/703  AvgLoss: 1.3015  Time: 87.3s\n",
      "Epoch 36 Batch 350/703  AvgLoss: 1.3089  Time: 101.5s\n",
      "Epoch 36 Batch 400/703  AvgLoss: 1.3133  Time: 115.7s\n",
      "Epoch 36 Batch 450/703  AvgLoss: 1.3092  Time: 129.9s\n",
      "Epoch 36 Batch 500/703  AvgLoss: 1.3035  Time: 144.3s\n",
      "Epoch 36 Batch 550/703  AvgLoss: 1.3052  Time: 158.5s\n",
      "Epoch 36 Batch 600/703  AvgLoss: 1.2927  Time: 172.8s\n",
      "Epoch 36 Batch 650/703  AvgLoss: 1.2860  Time: 187.0s\n",
      "Epoch 36 Batch 700/703  AvgLoss: 1.2973  Time: 201.2s\n",
      "Epoch 36 Batch 703/703  AvgLoss: 1.2979  Time: 202.0s\n",
      "Epoch 36 VALID -> Loss: 0.5673  Top1: 86.800  Top5: 97.340\n",
      "\n",
      "Epoch 37/120 - lr: 0.020779 - starting training...\n",
      "Epoch 37 Batch 50/703  AvgLoss: 1.1788  Time: 15.9s\n",
      "Epoch 37 Batch 100/703  AvgLoss: 1.2786  Time: 30.1s\n",
      "Epoch 37 Batch 150/703  AvgLoss: 1.2464  Time: 44.3s\n",
      "Epoch 37 Batch 200/703  AvgLoss: 1.2354  Time: 58.6s\n",
      "Epoch 37 Batch 250/703  AvgLoss: 1.2417  Time: 72.9s\n",
      "Epoch 37 Batch 300/703  AvgLoss: 1.2307  Time: 87.2s\n",
      "Epoch 37 Batch 350/703  AvgLoss: 1.2247  Time: 101.5s\n",
      "Epoch 37 Batch 400/703  AvgLoss: 1.2261  Time: 115.9s\n",
      "Epoch 37 Batch 450/703  AvgLoss: 1.2324  Time: 130.2s\n",
      "Epoch 37 Batch 500/703  AvgLoss: 1.2392  Time: 144.5s\n",
      "Epoch 37 Batch 550/703  AvgLoss: 1.2366  Time: 158.8s\n",
      "Epoch 37 Batch 600/703  AvgLoss: 1.2402  Time: 173.2s\n",
      "Epoch 37 Batch 650/703  AvgLoss: 1.2372  Time: 187.6s\n",
      "Epoch 37 Batch 700/703  AvgLoss: 1.2397  Time: 202.0s\n",
      "Epoch 37 Batch 703/703  AvgLoss: 1.2384  Time: 202.9s\n",
      "Epoch 37 VALID -> Loss: 0.5668  Top1: 86.820  Top5: 97.340\n",
      "\n",
      "Epoch 38/120 - lr: 0.020520 - starting training...\n",
      "Epoch 38 Batch 50/703  AvgLoss: 1.0002  Time: 15.8s\n",
      "Epoch 38 Batch 100/703  AvgLoss: 1.1507  Time: 30.1s\n",
      "Epoch 38 Batch 150/703  AvgLoss: 1.1735  Time: 44.3s\n",
      "Epoch 38 Batch 200/703  AvgLoss: 1.1843  Time: 58.6s\n",
      "Epoch 38 Batch 250/703  AvgLoss: 1.1972  Time: 73.0s\n",
      "Epoch 38 Batch 300/703  AvgLoss: 1.2094  Time: 87.2s\n",
      "Epoch 38 Batch 350/703  AvgLoss: 1.2013  Time: 101.4s\n",
      "Epoch 38 Batch 400/703  AvgLoss: 1.2097  Time: 115.6s\n",
      "Epoch 38 Batch 450/703  AvgLoss: 1.2159  Time: 129.8s\n",
      "Epoch 38 Batch 500/703  AvgLoss: 1.2194  Time: 144.0s\n",
      "Epoch 38 Batch 550/703  AvgLoss: 1.2248  Time: 158.4s\n",
      "Epoch 38 Batch 600/703  AvgLoss: 1.2384  Time: 172.7s\n",
      "Epoch 38 Batch 650/703  AvgLoss: 1.2402  Time: 187.1s\n",
      "Epoch 38 Batch 700/703  AvgLoss: 1.2402  Time: 201.3s\n",
      "Epoch 38 Batch 703/703  AvgLoss: 1.2422  Time: 202.1s\n",
      "Epoch 38 VALID -> Loss: 0.5668  Top1: 86.800  Top5: 97.360\n",
      "\n",
      "Epoch 39/120 - lr: 0.020256 - starting training...\n",
      "Epoch 39 Batch 50/703  AvgLoss: 1.2063  Time: 15.5s\n",
      "Epoch 39 Batch 100/703  AvgLoss: 1.2509  Time: 29.8s\n",
      "Epoch 39 Batch 150/703  AvgLoss: 1.2620  Time: 44.2s\n",
      "Epoch 39 Batch 200/703  AvgLoss: 1.2125  Time: 58.4s\n",
      "Epoch 39 Batch 250/703  AvgLoss: 1.2268  Time: 72.6s\n",
      "Epoch 39 Batch 300/703  AvgLoss: 1.2346  Time: 86.9s\n",
      "Epoch 39 Batch 350/703  AvgLoss: 1.2301  Time: 101.1s\n",
      "Epoch 39 Batch 400/703  AvgLoss: 1.2190  Time: 115.3s\n",
      "Epoch 39 Batch 450/703  AvgLoss: 1.2355  Time: 129.7s\n",
      "Epoch 39 Batch 500/703  AvgLoss: 1.2400  Time: 143.9s\n",
      "Epoch 39 Batch 550/703  AvgLoss: 1.2421  Time: 158.2s\n",
      "Epoch 39 Batch 600/703  AvgLoss: 1.2368  Time: 172.5s\n",
      "Epoch 39 Batch 650/703  AvgLoss: 1.2445  Time: 186.8s\n",
      "Epoch 39 Batch 700/703  AvgLoss: 1.2427  Time: 201.1s\n",
      "Epoch 39 Batch 703/703  AvgLoss: 1.2425  Time: 202.0s\n",
      "Epoch 39 VALID -> Loss: 0.5671  Top1: 86.680  Top5: 97.440\n",
      "\n",
      "Epoch 40/120 - lr: 0.019985 - starting training...\n",
      "Epoch 40 Batch 50/703  AvgLoss: 1.1619  Time: 15.6s\n",
      "Epoch 40 Batch 100/703  AvgLoss: 1.2242  Time: 30.0s\n",
      "Epoch 40 Batch 150/703  AvgLoss: 1.1902  Time: 44.3s\n",
      "Epoch 40 Batch 200/703  AvgLoss: 1.2205  Time: 58.6s\n",
      "Epoch 40 Batch 250/703  AvgLoss: 1.2488  Time: 72.8s\n",
      "Epoch 40 Batch 300/703  AvgLoss: 1.2476  Time: 87.3s\n",
      "Epoch 40 Batch 350/703  AvgLoss: 1.2608  Time: 101.7s\n",
      "Epoch 40 Batch 400/703  AvgLoss: 1.2408  Time: 116.1s\n",
      "Epoch 40 Batch 450/703  AvgLoss: 1.2206  Time: 130.4s\n",
      "Epoch 40 Batch 500/703  AvgLoss: 1.2256  Time: 144.7s\n",
      "Epoch 40 Batch 550/703  AvgLoss: 1.2133  Time: 159.1s\n",
      "Epoch 40 Batch 600/703  AvgLoss: 1.2106  Time: 173.4s\n",
      "Epoch 40 Batch 650/703  AvgLoss: 1.2199  Time: 188.0s\n",
      "Epoch 40 Batch 700/703  AvgLoss: 1.2278  Time: 202.2s\n",
      "Epoch 40 Batch 703/703  AvgLoss: 1.2283  Time: 203.1s\n",
      "Epoch 40 VALID -> Loss: 0.5676  Top1: 86.680  Top5: 97.280\n",
      "\n",
      "Epoch 41/120 - lr: 0.019709 - starting training...\n",
      "Epoch 41 Batch 50/703  AvgLoss: 1.1426  Time: 15.7s\n",
      "Epoch 41 Batch 100/703  AvgLoss: 1.2525  Time: 30.0s\n",
      "Epoch 41 Batch 150/703  AvgLoss: 1.2722  Time: 44.3s\n",
      "Epoch 41 Batch 200/703  AvgLoss: 1.2834  Time: 58.8s\n",
      "Epoch 41 Batch 250/703  AvgLoss: 1.2717  Time: 73.1s\n",
      "Epoch 41 Batch 300/703  AvgLoss: 1.2653  Time: 87.4s\n",
      "Epoch 41 Batch 350/703  AvgLoss: 1.2743  Time: 101.8s\n",
      "Epoch 41 Batch 400/703  AvgLoss: 1.2602  Time: 116.1s\n",
      "Epoch 41 Batch 450/703  AvgLoss: 1.2685  Time: 130.4s\n",
      "Epoch 41 Batch 500/703  AvgLoss: 1.2667  Time: 144.9s\n",
      "Epoch 41 Batch 550/703  AvgLoss: 1.2712  Time: 159.3s\n",
      "Epoch 41 Batch 600/703  AvgLoss: 1.2730  Time: 173.6s\n",
      "Epoch 41 Batch 650/703  AvgLoss: 1.2685  Time: 188.0s\n",
      "Epoch 41 Batch 700/703  AvgLoss: 1.2566  Time: 202.3s\n",
      "Epoch 41 Batch 703/703  AvgLoss: 1.2567  Time: 203.1s\n",
      "Epoch 41 VALID -> Loss: 0.5699  Top1: 86.720  Top5: 97.220\n",
      "\n",
      "Epoch 42/120 - lr: 0.019427 - starting training...\n",
      "Epoch 42 Batch 50/703  AvgLoss: 1.2296  Time: 16.2s\n",
      "Epoch 42 Batch 100/703  AvgLoss: 1.2696  Time: 30.5s\n",
      "Epoch 42 Batch 150/703  AvgLoss: 1.2377  Time: 44.8s\n",
      "Epoch 42 Batch 200/703  AvgLoss: 1.2374  Time: 59.2s\n",
      "Epoch 42 Batch 250/703  AvgLoss: 1.2464  Time: 73.5s\n",
      "Epoch 42 Batch 300/703  AvgLoss: 1.2324  Time: 87.8s\n",
      "Epoch 42 Batch 350/703  AvgLoss: 1.2235  Time: 102.3s\n",
      "Epoch 42 Batch 400/703  AvgLoss: 1.2149  Time: 116.6s\n",
      "Epoch 42 Batch 450/703  AvgLoss: 1.1980  Time: 130.9s\n",
      "Epoch 42 Batch 500/703  AvgLoss: 1.2085  Time: 145.2s\n",
      "Epoch 42 Batch 550/703  AvgLoss: 1.2038  Time: 159.6s\n",
      "Epoch 42 Batch 600/703  AvgLoss: 1.2012  Time: 173.9s\n",
      "Epoch 42 Batch 650/703  AvgLoss: 1.2079  Time: 188.5s\n",
      "Epoch 42 Batch 700/703  AvgLoss: 1.2143  Time: 202.7s\n",
      "Epoch 42 Batch 703/703  AvgLoss: 1.2152  Time: 203.6s\n",
      "Epoch 42 VALID -> Loss: 0.5714  Top1: 86.640  Top5: 97.240\n",
      "\n",
      "Epoch 43/120 - lr: 0.019140 - starting training...\n",
      "Epoch 43 Batch 50/703  AvgLoss: 1.3405  Time: 15.9s\n",
      "Epoch 43 Batch 100/703  AvgLoss: 1.2611  Time: 30.2s\n",
      "Epoch 43 Batch 150/703  AvgLoss: 1.2138  Time: 44.5s\n",
      "Epoch 43 Batch 200/703  AvgLoss: 1.2280  Time: 58.9s\n",
      "Epoch 43 Batch 250/703  AvgLoss: 1.2197  Time: 73.4s\n",
      "Epoch 43 Batch 300/703  AvgLoss: 1.2459  Time: 87.6s\n",
      "Epoch 43 Batch 350/703  AvgLoss: 1.2577  Time: 102.0s\n",
      "Epoch 43 Batch 400/703  AvgLoss: 1.2487  Time: 116.3s\n",
      "Epoch 43 Batch 450/703  AvgLoss: 1.2612  Time: 130.6s\n",
      "Epoch 43 Batch 500/703  AvgLoss: 1.2563  Time: 144.9s\n",
      "Epoch 43 Batch 550/703  AvgLoss: 1.2571  Time: 159.4s\n",
      "Epoch 43 Batch 600/703  AvgLoss: 1.2690  Time: 173.8s\n",
      "Epoch 43 Batch 650/703  AvgLoss: 1.2650  Time: 188.1s\n",
      "Epoch 43 Batch 700/703  AvgLoss: 1.2695  Time: 202.4s\n",
      "Epoch 43 Batch 703/703  AvgLoss: 1.2696  Time: 203.2s\n",
      "Epoch 43 VALID -> Loss: 0.5735  Top1: 86.700  Top5: 97.240\n",
      "\n",
      "Epoch 44/120 - lr: 0.018849 - starting training...\n",
      "Epoch 44 Batch 50/703  AvgLoss: 1.1614  Time: 16.2s\n",
      "Epoch 44 Batch 100/703  AvgLoss: 1.2007  Time: 30.5s\n",
      "Epoch 44 Batch 150/703  AvgLoss: 1.1963  Time: 44.9s\n",
      "Epoch 44 Batch 200/703  AvgLoss: 1.1931  Time: 59.5s\n",
      "Epoch 44 Batch 250/703  AvgLoss: 1.2098  Time: 73.9s\n",
      "Epoch 44 Batch 300/703  AvgLoss: 1.2227  Time: 88.5s\n",
      "Epoch 44 Batch 350/703  AvgLoss: 1.2112  Time: 103.0s\n",
      "Epoch 44 Batch 400/703  AvgLoss: 1.2242  Time: 117.3s\n",
      "Epoch 44 Batch 450/703  AvgLoss: 1.2203  Time: 132.1s\n",
      "Epoch 44 Batch 500/703  AvgLoss: 1.2231  Time: 146.3s\n",
      "Epoch 44 Batch 550/703  AvgLoss: 1.2208  Time: 160.5s\n",
      "Epoch 44 Batch 600/703  AvgLoss: 1.2206  Time: 174.8s\n",
      "Epoch 44 Batch 650/703  AvgLoss: 1.2286  Time: 189.3s\n",
      "Epoch 44 Batch 700/703  AvgLoss: 1.2315  Time: 203.8s\n",
      "Epoch 44 Batch 703/703  AvgLoss: 1.2331  Time: 204.7s\n",
      "Epoch 44 VALID -> Loss: 0.5765  Top1: 86.760  Top5: 97.200\n",
      "\n",
      "Epoch 45/120 - lr: 0.018552 - starting training...\n",
      "Epoch 45 Batch 50/703  AvgLoss: 1.2101  Time: 16.2s\n",
      "Epoch 45 Batch 100/703  AvgLoss: 1.2414  Time: 30.6s\n",
      "Epoch 45 Batch 150/703  AvgLoss: 1.2186  Time: 44.8s\n",
      "Epoch 45 Batch 200/703  AvgLoss: 1.2251  Time: 59.0s\n",
      "Epoch 45 Batch 250/703  AvgLoss: 1.2110  Time: 73.1s\n",
      "Epoch 45 Batch 300/703  AvgLoss: 1.2101  Time: 87.4s\n",
      "Epoch 45 Batch 350/703  AvgLoss: 1.2031  Time: 101.8s\n",
      "Epoch 45 Batch 400/703  AvgLoss: 1.2062  Time: 116.4s\n",
      "Epoch 45 Batch 450/703  AvgLoss: 1.2197  Time: 130.8s\n",
      "Epoch 45 Batch 500/703  AvgLoss: 1.2086  Time: 145.2s\n",
      "Epoch 45 Batch 550/703  AvgLoss: 1.2227  Time: 159.7s\n",
      "Epoch 45 Batch 600/703  AvgLoss: 1.2019  Time: 174.2s\n",
      "Epoch 45 Batch 650/703  AvgLoss: 1.2002  Time: 188.6s\n",
      "Epoch 45 Batch 700/703  AvgLoss: 1.2158  Time: 203.3s\n",
      "Epoch 45 Batch 703/703  AvgLoss: 1.2160  Time: 204.2s\n",
      "Epoch 45 VALID -> Loss: 0.5788  Top1: 86.740  Top5: 97.160\n",
      "\n",
      "Epoch 46/120 - lr: 0.018251 - starting training...\n",
      "Epoch 46 Batch 50/703  AvgLoss: 1.1674  Time: 15.7s\n",
      "Epoch 46 Batch 100/703  AvgLoss: 1.2233  Time: 30.1s\n",
      "Epoch 46 Batch 150/703  AvgLoss: 1.2124  Time: 44.3s\n",
      "Epoch 46 Batch 200/703  AvgLoss: 1.2244  Time: 58.7s\n",
      "Epoch 46 Batch 250/703  AvgLoss: 1.2147  Time: 73.1s\n",
      "Epoch 46 Batch 300/703  AvgLoss: 1.2211  Time: 87.6s\n",
      "Epoch 46 Batch 350/703  AvgLoss: 1.2214  Time: 101.8s\n",
      "Epoch 46 Batch 400/703  AvgLoss: 1.2199  Time: 116.4s\n",
      "Epoch 46 Batch 450/703  AvgLoss: 1.2193  Time: 131.0s\n",
      "Epoch 46 Batch 500/703  AvgLoss: 1.2076  Time: 145.5s\n",
      "Epoch 46 Batch 550/703  AvgLoss: 1.2181  Time: 160.0s\n",
      "Epoch 46 Batch 600/703  AvgLoss: 1.2270  Time: 174.8s\n",
      "Epoch 46 Batch 650/703  AvgLoss: 1.2313  Time: 189.4s\n",
      "Epoch 46 Batch 700/703  AvgLoss: 1.2416  Time: 203.8s\n",
      "Epoch 46 Batch 703/703  AvgLoss: 1.2411  Time: 204.6s\n",
      "Epoch 46 VALID -> Loss: 0.5817  Top1: 86.800  Top5: 97.140\n",
      "\n",
      "Epoch 47/120 - lr: 0.017946 - starting training...\n",
      "Epoch 47 Batch 50/703  AvgLoss: 1.3027  Time: 15.6s\n",
      "Epoch 47 Batch 100/703  AvgLoss: 1.2841  Time: 29.8s\n",
      "Epoch 47 Batch 150/703  AvgLoss: 1.2993  Time: 44.1s\n",
      "Epoch 47 Batch 200/703  AvgLoss: 1.3137  Time: 58.7s\n",
      "Epoch 47 Batch 250/703  AvgLoss: 1.2932  Time: 73.2s\n",
      "Epoch 47 Batch 300/703  AvgLoss: 1.2936  Time: 87.5s\n",
      "Epoch 47 Batch 350/703  AvgLoss: 1.2763  Time: 102.1s\n",
      "Epoch 47 Batch 400/703  AvgLoss: 1.2594  Time: 116.4s\n",
      "Epoch 47 Batch 450/703  AvgLoss: 1.2642  Time: 130.7s\n",
      "Epoch 47 Batch 500/703  AvgLoss: 1.2595  Time: 144.8s\n",
      "Epoch 47 Batch 550/703  AvgLoss: 1.2465  Time: 159.3s\n",
      "Epoch 47 Batch 600/703  AvgLoss: 1.2371  Time: 173.9s\n",
      "Epoch 47 Batch 650/703  AvgLoss: 1.2358  Time: 188.3s\n",
      "Epoch 47 Batch 700/703  AvgLoss: 1.2337  Time: 202.7s\n",
      "Epoch 47 Batch 703/703  AvgLoss: 1.2305  Time: 203.5s\n",
      "Epoch 47 VALID -> Loss: 0.5854  Top1: 86.840  Top5: 97.180\n",
      "Saved new best checkpoint.\n",
      "\n",
      "Epoch 48/120 - lr: 0.017636 - starting training...\n",
      "Epoch 48 Batch 50/703  AvgLoss: 1.2747  Time: 16.5s\n",
      "Epoch 48 Batch 100/703  AvgLoss: 1.2628  Time: 31.0s\n",
      "Epoch 48 Batch 150/703  AvgLoss: 1.2057  Time: 45.6s\n",
      "Epoch 48 Batch 200/703  AvgLoss: 1.1950  Time: 60.1s\n",
      "Epoch 48 Batch 250/703  AvgLoss: 1.1598  Time: 74.6s\n",
      "Epoch 48 Batch 300/703  AvgLoss: 1.1583  Time: 89.1s\n",
      "Epoch 48 Batch 350/703  AvgLoss: 1.1543  Time: 103.5s\n",
      "Epoch 48 Batch 400/703  AvgLoss: 1.1555  Time: 117.7s\n",
      "Epoch 48 Batch 450/703  AvgLoss: 1.1522  Time: 132.0s\n",
      "Epoch 48 Batch 500/703  AvgLoss: 1.1615  Time: 146.1s\n",
      "Epoch 48 Batch 550/703  AvgLoss: 1.1673  Time: 160.3s\n",
      "Epoch 48 Batch 600/703  AvgLoss: 1.1840  Time: 174.7s\n",
      "Epoch 48 Batch 650/703  AvgLoss: 1.1825  Time: 189.3s\n",
      "Epoch 48 Batch 700/703  AvgLoss: 1.1849  Time: 203.9s\n",
      "Epoch 48 Batch 703/703  AvgLoss: 1.1850  Time: 204.8s\n",
      "Epoch 48 VALID -> Loss: 0.5881  Top1: 86.700  Top5: 97.060\n",
      "\n",
      "Epoch 49/120 - lr: 0.017323 - starting training...\n",
      "Epoch 49 Batch 50/703  AvgLoss: 1.2260  Time: 15.8s\n",
      "Epoch 49 Batch 100/703  AvgLoss: 1.1766  Time: 30.0s\n",
      "Epoch 49 Batch 150/703  AvgLoss: 1.1582  Time: 44.5s\n",
      "Epoch 49 Batch 200/703  AvgLoss: 1.1694  Time: 59.1s\n",
      "Epoch 49 Batch 250/703  AvgLoss: 1.1451  Time: 73.9s\n",
      "Epoch 49 Batch 300/703  AvgLoss: 1.1469  Time: 88.4s\n",
      "Epoch 49 Batch 350/703  AvgLoss: 1.1518  Time: 102.6s\n",
      "Epoch 49 Batch 400/703  AvgLoss: 1.1814  Time: 117.3s\n",
      "Epoch 49 Batch 450/703  AvgLoss: 1.1624  Time: 131.9s\n",
      "Epoch 49 Batch 500/703  AvgLoss: 1.1698  Time: 146.4s\n",
      "Epoch 49 Batch 550/703  AvgLoss: 1.1687  Time: 161.0s\n",
      "Epoch 49 Batch 600/703  AvgLoss: 1.1752  Time: 175.5s\n",
      "Epoch 49 Batch 650/703  AvgLoss: 1.1799  Time: 189.9s\n",
      "Epoch 49 Batch 700/703  AvgLoss: 1.1871  Time: 204.3s\n",
      "Epoch 49 Batch 703/703  AvgLoss: 1.1854  Time: 205.1s\n",
      "Epoch 49 VALID -> Loss: 0.5913  Top1: 86.700  Top5: 97.080\n",
      "\n",
      "Epoch 50/120 - lr: 0.017006 - starting training...\n",
      "Epoch 50 Batch 50/703  AvgLoss: 1.1997  Time: 16.1s\n",
      "Epoch 50 Batch 100/703  AvgLoss: 1.2386  Time: 30.7s\n",
      "Epoch 50 Batch 150/703  AvgLoss: 1.2034  Time: 45.2s\n",
      "Epoch 50 Batch 200/703  AvgLoss: 1.1751  Time: 59.5s\n",
      "Epoch 50 Batch 250/703  AvgLoss: 1.1610  Time: 73.8s\n",
      "Epoch 50 Batch 300/703  AvgLoss: 1.1487  Time: 88.1s\n",
      "Epoch 50 Batch 350/703  AvgLoss: 1.1499  Time: 102.7s\n",
      "Epoch 50 Batch 400/703  AvgLoss: 1.1425  Time: 117.5s\n",
      "Epoch 50 Batch 450/703  AvgLoss: 1.1497  Time: 132.1s\n",
      "Epoch 50 Batch 500/703  AvgLoss: 1.1549  Time: 146.7s\n",
      "Epoch 50 Batch 550/703  AvgLoss: 1.1587  Time: 161.2s\n",
      "Epoch 50 Batch 600/703  AvgLoss: 1.1625  Time: 175.8s\n",
      "Epoch 50 Batch 650/703  AvgLoss: 1.1605  Time: 190.3s\n",
      "Epoch 50 Batch 700/703  AvgLoss: 1.1639  Time: 205.0s\n",
      "Epoch 50 Batch 703/703  AvgLoss: 1.1645  Time: 205.8s\n",
      "Epoch 50 VALID -> Loss: 0.5946  Top1: 86.720  Top5: 97.020\n",
      "\n",
      "Epoch 51/120 - lr: 0.016686 - starting training...\n",
      "Epoch 51 Batch 50/703  AvgLoss: 1.1391  Time: 15.5s\n",
      "Epoch 51 Batch 100/703  AvgLoss: 1.1873  Time: 29.7s\n",
      "Epoch 51 Batch 150/703  AvgLoss: 1.1299  Time: 44.1s\n",
      "Epoch 51 Batch 200/703  AvgLoss: 1.1403  Time: 58.3s\n",
      "Epoch 51 Batch 250/703  AvgLoss: 1.1882  Time: 72.7s\n",
      "Epoch 51 Batch 300/703  AvgLoss: 1.1922  Time: 87.0s\n",
      "Epoch 51 Batch 350/703  AvgLoss: 1.1942  Time: 101.3s\n",
      "Epoch 51 Batch 400/703  AvgLoss: 1.1740  Time: 115.7s\n",
      "Epoch 51 Batch 450/703  AvgLoss: 1.1608  Time: 129.9s\n",
      "Epoch 51 Batch 500/703  AvgLoss: 1.1593  Time: 144.4s\n",
      "Epoch 51 Batch 550/703  AvgLoss: 1.1633  Time: 159.0s\n",
      "Epoch 51 Batch 600/703  AvgLoss: 1.1590  Time: 173.4s\n",
      "Epoch 51 Batch 650/703  AvgLoss: 1.1581  Time: 187.8s\n",
      "Epoch 51 Batch 700/703  AvgLoss: 1.1650  Time: 202.1s\n",
      "Epoch 51 Batch 703/703  AvgLoss: 1.1645  Time: 203.0s\n",
      "Epoch 51 VALID -> Loss: 0.5978  Top1: 86.540  Top5: 97.000\n",
      "\n",
      "Epoch 52/120 - lr: 0.016363 - starting training...\n",
      "Epoch 52 Batch 50/703  AvgLoss: 1.0624  Time: 16.0s\n",
      "Epoch 52 Batch 100/703  AvgLoss: 1.1308  Time: 30.7s\n",
      "Epoch 52 Batch 150/703  AvgLoss: 1.1510  Time: 45.0s\n",
      "Epoch 52 Batch 200/703  AvgLoss: 1.1987  Time: 59.4s\n",
      "Epoch 52 Batch 250/703  AvgLoss: 1.1829  Time: 73.6s\n",
      "Epoch 52 Batch 300/703  AvgLoss: 1.1942  Time: 87.9s\n",
      "Epoch 52 Batch 350/703  AvgLoss: 1.1963  Time: 102.2s\n",
      "Epoch 52 Batch 400/703  AvgLoss: 1.2032  Time: 116.8s\n",
      "Epoch 52 Batch 450/703  AvgLoss: 1.1996  Time: 131.2s\n",
      "Epoch 52 Batch 500/703  AvgLoss: 1.2097  Time: 145.7s\n",
      "Epoch 52 Batch 550/703  AvgLoss: 1.1993  Time: 160.3s\n",
      "Epoch 52 Batch 600/703  AvgLoss: 1.2006  Time: 174.8s\n",
      "Epoch 52 Batch 650/703  AvgLoss: 1.2003  Time: 189.4s\n",
      "Epoch 52 Batch 700/703  AvgLoss: 1.1959  Time: 204.2s\n",
      "Epoch 52 Batch 703/703  AvgLoss: 1.1943  Time: 205.1s\n",
      "Epoch 52 VALID -> Loss: 0.6013  Top1: 86.520  Top5: 96.960\n",
      "\n",
      "Epoch 53/120 - lr: 0.016037 - starting training...\n",
      "Epoch 53 Batch 50/703  AvgLoss: 1.1599  Time: 15.8s\n",
      "Epoch 53 Batch 100/703  AvgLoss: 1.1753  Time: 30.4s\n",
      "Epoch 53 Batch 150/703  AvgLoss: 1.2046  Time: 44.8s\n",
      "Epoch 53 Batch 200/703  AvgLoss: 1.2238  Time: 59.2s\n",
      "Epoch 53 Batch 250/703  AvgLoss: 1.2075  Time: 74.0s\n",
      "Epoch 53 Batch 300/703  AvgLoss: 1.2055  Time: 88.6s\n",
      "Epoch 53 Batch 350/703  AvgLoss: 1.2093  Time: 102.8s\n",
      "Epoch 53 Batch 400/703  AvgLoss: 1.2071  Time: 117.3s\n",
      "Epoch 53 Batch 450/703  AvgLoss: 1.2135  Time: 131.5s\n",
      "Epoch 53 Batch 500/703  AvgLoss: 1.2113  Time: 145.9s\n",
      "Epoch 53 Batch 550/703  AvgLoss: 1.2063  Time: 160.3s\n",
      "Epoch 53 Batch 600/703  AvgLoss: 1.1953  Time: 174.9s\n",
      "Epoch 53 Batch 650/703  AvgLoss: 1.2000  Time: 189.3s\n",
      "Epoch 53 Batch 700/703  AvgLoss: 1.1949  Time: 203.7s\n",
      "Epoch 53 Batch 703/703  AvgLoss: 1.1957  Time: 204.6s\n",
      "Epoch 53 VALID -> Loss: 0.6042  Top1: 86.600  Top5: 96.980\n",
      "\n",
      "Epoch 54/120 - lr: 0.015708 - starting training...\n",
      "Epoch 54 Batch 50/703  AvgLoss: 1.1503  Time: 16.2s\n",
      "Epoch 54 Batch 100/703  AvgLoss: 1.0552  Time: 30.8s\n",
      "Epoch 54 Batch 150/703  AvgLoss: 1.0794  Time: 45.2s\n",
      "Epoch 54 Batch 200/703  AvgLoss: 1.1122  Time: 59.6s\n",
      "Epoch 54 Batch 250/703  AvgLoss: 1.1412  Time: 74.1s\n",
      "Epoch 54 Batch 300/703  AvgLoss: 1.1376  Time: 88.6s\n",
      "Epoch 54 Batch 350/703  AvgLoss: 1.1378  Time: 102.9s\n",
      "Epoch 54 Batch 400/703  AvgLoss: 1.1382  Time: 117.6s\n",
      "Epoch 54 Batch 450/703  AvgLoss: 1.1572  Time: 132.1s\n",
      "Epoch 54 Batch 500/703  AvgLoss: 1.1701  Time: 146.6s\n",
      "Epoch 54 Batch 550/703  AvgLoss: 1.1614  Time: 161.3s\n",
      "Epoch 54 Batch 600/703  AvgLoss: 1.1522  Time: 175.8s\n",
      "Epoch 54 Batch 650/703  AvgLoss: 1.1523  Time: 190.0s\n",
      "Epoch 54 Batch 700/703  AvgLoss: 1.1580  Time: 204.6s\n",
      "Epoch 54 Batch 703/703  AvgLoss: 1.1579  Time: 205.4s\n",
      "Epoch 54 VALID -> Loss: 0.6072  Top1: 86.480  Top5: 96.900\n",
      "\n",
      "Epoch 55/120 - lr: 0.015377 - starting training...\n",
      "Epoch 55 Batch 50/703  AvgLoss: 1.2377  Time: 15.9s\n",
      "Epoch 55 Batch 100/703  AvgLoss: 1.1816  Time: 30.3s\n",
      "Epoch 55 Batch 150/703  AvgLoss: 1.1599  Time: 44.7s\n",
      "Epoch 55 Batch 200/703  AvgLoss: 1.1597  Time: 59.1s\n",
      "Epoch 55 Batch 250/703  AvgLoss: 1.1565  Time: 73.7s\n",
      "Epoch 55 Batch 300/703  AvgLoss: 1.1551  Time: 88.4s\n",
      "Epoch 55 Batch 350/703  AvgLoss: 1.1596  Time: 102.9s\n",
      "Epoch 55 Batch 400/703  AvgLoss: 1.1654  Time: 117.4s\n",
      "Epoch 55 Batch 450/703  AvgLoss: 1.1597  Time: 132.0s\n",
      "Epoch 55 Batch 500/703  AvgLoss: 1.1662  Time: 146.6s\n",
      "Epoch 55 Batch 550/703  AvgLoss: 1.1796  Time: 161.4s\n",
      "Epoch 55 Batch 600/703  AvgLoss: 1.1836  Time: 175.8s\n",
      "Epoch 55 Batch 650/703  AvgLoss: 1.1837  Time: 190.2s\n",
      "Epoch 55 Batch 700/703  AvgLoss: 1.1883  Time: 204.8s\n",
      "Epoch 55 Batch 703/703  AvgLoss: 1.1892  Time: 205.6s\n",
      "Epoch 55 VALID -> Loss: 0.6111  Top1: 86.480  Top5: 96.920\n",
      "\n",
      "Epoch 56/120 - lr: 0.015044 - starting training...\n",
      "Epoch 56 Batch 50/703  AvgLoss: 1.0663  Time: 16.3s\n",
      "Epoch 56 Batch 100/703  AvgLoss: 1.0929  Time: 30.9s\n",
      "Epoch 56 Batch 150/703  AvgLoss: 1.0880  Time: 45.5s\n",
      "Epoch 56 Batch 200/703  AvgLoss: 1.0676  Time: 60.1s\n",
      "Epoch 56 Batch 250/703  AvgLoss: 1.1021  Time: 74.5s\n",
      "Epoch 56 Batch 300/703  AvgLoss: 1.1068  Time: 89.0s\n",
      "Epoch 56 Batch 350/703  AvgLoss: 1.1169  Time: 103.3s\n",
      "Epoch 56 Batch 400/703  AvgLoss: 1.1200  Time: 117.7s\n",
      "Epoch 56 Batch 450/703  AvgLoss: 1.1161  Time: 132.0s\n",
      "Epoch 56 Batch 500/703  AvgLoss: 1.1154  Time: 146.3s\n",
      "Epoch 56 Batch 550/703  AvgLoss: 1.1130  Time: 160.8s\n",
      "Epoch 56 Batch 600/703  AvgLoss: 1.1091  Time: 175.3s\n",
      "Epoch 56 Batch 650/703  AvgLoss: 1.1176  Time: 189.8s\n",
      "Epoch 56 Batch 700/703  AvgLoss: 1.1256  Time: 204.4s\n",
      "Epoch 56 Batch 703/703  AvgLoss: 1.1259  Time: 205.2s\n",
      "Epoch 56 VALID -> Loss: 0.6141  Top1: 86.420  Top5: 96.860\n",
      "\n",
      "Epoch 57/120 - lr: 0.014708 - starting training...\n",
      "Epoch 57 Batch 50/703  AvgLoss: 1.1607  Time: 16.3s\n",
      "Epoch 57 Batch 100/703  AvgLoss: 1.1508  Time: 30.8s\n",
      "Epoch 57 Batch 150/703  AvgLoss: 1.1143  Time: 45.4s\n",
      "Epoch 57 Batch 200/703  AvgLoss: 1.1528  Time: 59.7s\n",
      "Epoch 57 Batch 250/703  AvgLoss: 1.1294  Time: 74.1s\n",
      "Epoch 57 Batch 300/703  AvgLoss: 1.1309  Time: 88.7s\n",
      "Epoch 57 Batch 350/703  AvgLoss: 1.1254  Time: 103.1s\n",
      "Epoch 57 Batch 400/703  AvgLoss: 1.1307  Time: 117.6s\n",
      "Epoch 57 Batch 450/703  AvgLoss: 1.1181  Time: 132.0s\n",
      "Epoch 57 Batch 500/703  AvgLoss: 1.1232  Time: 146.4s\n",
      "Epoch 57 Batch 550/703  AvgLoss: 1.1333  Time: 160.8s\n",
      "Epoch 57 Batch 600/703  AvgLoss: 1.1466  Time: 175.3s\n",
      "Epoch 57 Batch 650/703  AvgLoss: 1.1490  Time: 190.0s\n",
      "Epoch 57 Batch 700/703  AvgLoss: 1.1466  Time: 204.4s\n",
      "Epoch 57 Batch 703/703  AvgLoss: 1.1447  Time: 205.3s\n",
      "Epoch 57 VALID -> Loss: 0.6168  Top1: 86.380  Top5: 96.980\n",
      "\n",
      "Epoch 58/120 - lr: 0.014371 - starting training...\n",
      "Epoch 58 Batch 50/703  AvgLoss: 1.1061  Time: 16.0s\n",
      "Epoch 58 Batch 100/703  AvgLoss: 1.0922  Time: 30.7s\n",
      "Epoch 58 Batch 150/703  AvgLoss: 1.1746  Time: 45.2s\n",
      "Epoch 58 Batch 200/703  AvgLoss: 1.1565  Time: 59.8s\n",
      "Epoch 58 Batch 250/703  AvgLoss: 1.1332  Time: 74.3s\n",
      "Epoch 58 Batch 300/703  AvgLoss: 1.1161  Time: 88.8s\n",
      "Epoch 58 Batch 350/703  AvgLoss: 1.1129  Time: 103.3s\n",
      "Epoch 58 Batch 400/703  AvgLoss: 1.1215  Time: 117.8s\n",
      "Epoch 58 Batch 450/703  AvgLoss: 1.1233  Time: 132.3s\n",
      "Epoch 58 Batch 500/703  AvgLoss: 1.1229  Time: 146.5s\n",
      "Epoch 58 Batch 550/703  AvgLoss: 1.1328  Time: 160.8s\n",
      "Epoch 58 Batch 600/703  AvgLoss: 1.1231  Time: 174.9s\n",
      "Epoch 58 Batch 650/703  AvgLoss: 1.1242  Time: 189.2s\n",
      "Epoch 58 Batch 700/703  AvgLoss: 1.1310  Time: 203.4s\n",
      "Epoch 58 Batch 703/703  AvgLoss: 1.1325  Time: 204.3s\n",
      "Epoch 58 VALID -> Loss: 0.6192  Top1: 86.260  Top5: 96.940\n",
      "\n",
      "Epoch 59/120 - lr: 0.014033 - starting training...\n",
      "Epoch 59 Batch 50/703  AvgLoss: 0.9788  Time: 16.1s\n",
      "Epoch 59 Batch 100/703  AvgLoss: 1.0529  Time: 30.6s\n",
      "Epoch 59 Batch 150/703  AvgLoss: 1.0956  Time: 45.3s\n",
      "Epoch 59 Batch 200/703  AvgLoss: 1.1236  Time: 59.4s\n",
      "Epoch 59 Batch 250/703  AvgLoss: 1.0988  Time: 73.9s\n",
      "Epoch 59 Batch 300/703  AvgLoss: 1.1084  Time: 88.4s\n",
      "Epoch 59 Batch 350/703  AvgLoss: 1.1240  Time: 102.9s\n",
      "Epoch 59 Batch 400/703  AvgLoss: 1.1304  Time: 117.3s\n",
      "Epoch 59 Batch 450/703  AvgLoss: 1.1343  Time: 132.1s\n",
      "Epoch 59 Batch 500/703  AvgLoss: 1.1295  Time: 146.6s\n",
      "Epoch 59 Batch 550/703  AvgLoss: 1.1291  Time: 161.2s\n",
      "Epoch 59 Batch 600/703  AvgLoss: 1.1315  Time: 175.7s\n",
      "Epoch 59 Batch 650/703  AvgLoss: 1.1251  Time: 190.1s\n",
      "Epoch 59 Batch 700/703  AvgLoss: 1.1154  Time: 204.6s\n",
      "Epoch 59 Batch 703/703  AvgLoss: 1.1171  Time: 205.5s\n",
      "Epoch 59 VALID -> Loss: 0.6220  Top1: 86.360  Top5: 96.880\n",
      "\n",
      "Epoch 60/120 - lr: 0.013694 - starting training...\n",
      "Epoch 60 Batch 50/703  AvgLoss: 1.2193  Time: 16.0s\n",
      "Epoch 60 Batch 100/703  AvgLoss: 1.1751  Time: 30.5s\n",
      "Epoch 60 Batch 150/703  AvgLoss: 1.1927  Time: 44.8s\n",
      "Epoch 60 Batch 200/703  AvgLoss: 1.1934  Time: 59.4s\n",
      "Epoch 60 Batch 250/703  AvgLoss: 1.1944  Time: 73.7s\n",
      "Epoch 60 Batch 300/703  AvgLoss: 1.1980  Time: 88.0s\n",
      "Epoch 60 Batch 350/703  AvgLoss: 1.1858  Time: 102.4s\n",
      "Epoch 60 Batch 400/703  AvgLoss: 1.1818  Time: 117.1s\n",
      "Epoch 60 Batch 450/703  AvgLoss: 1.1841  Time: 131.5s\n",
      "Epoch 60 Batch 500/703  AvgLoss: 1.1839  Time: 145.8s\n",
      "Epoch 60 Batch 550/703  AvgLoss: 1.1813  Time: 160.3s\n",
      "Epoch 60 Batch 600/703  AvgLoss: 1.1721  Time: 174.9s\n",
      "Epoch 60 Batch 650/703  AvgLoss: 1.1657  Time: 189.3s\n",
      "Epoch 60 Batch 700/703  AvgLoss: 1.1651  Time: 203.7s\n",
      "Epoch 60 Batch 703/703  AvgLoss: 1.1636  Time: 204.6s\n",
      "Epoch 60 VALID -> Loss: 0.6251  Top1: 86.360  Top5: 96.940\n",
      "\n",
      "Epoch 61/120 - lr: 0.013353 - starting training...\n",
      "Epoch 61 Batch 50/703  AvgLoss: 1.0217  Time: 15.6s\n",
      "Epoch 61 Batch 100/703  AvgLoss: 1.0535  Time: 30.0s\n",
      "Epoch 61 Batch 150/703  AvgLoss: 1.0967  Time: 44.5s\n",
      "Epoch 61 Batch 200/703  AvgLoss: 1.1019  Time: 59.1s\n",
      "Epoch 61 Batch 250/703  AvgLoss: 1.1284  Time: 73.7s\n",
      "Epoch 61 Batch 300/703  AvgLoss: 1.1250  Time: 88.6s\n",
      "Epoch 61 Batch 350/703  AvgLoss: 1.1242  Time: 102.9s\n",
      "Epoch 61 Batch 400/703  AvgLoss: 1.1327  Time: 117.0s\n",
      "Epoch 61 Batch 450/703  AvgLoss: 1.1247  Time: 131.3s\n",
      "Epoch 61 Batch 500/703  AvgLoss: 1.1126  Time: 145.7s\n",
      "Epoch 61 Batch 550/703  AvgLoss: 1.1141  Time: 160.1s\n",
      "Epoch 61 Batch 600/703  AvgLoss: 1.1130  Time: 174.9s\n",
      "Epoch 61 Batch 650/703  AvgLoss: 1.1118  Time: 189.4s\n",
      "Epoch 61 Batch 700/703  AvgLoss: 1.1120  Time: 203.8s\n",
      "Epoch 61 Batch 703/703  AvgLoss: 1.1119  Time: 204.6s\n",
      "Epoch 61 VALID -> Loss: 0.6276  Top1: 86.240  Top5: 96.820\n",
      "\n",
      "Epoch 62/120 - lr: 0.013013 - starting training...\n",
      "Epoch 62 Batch 50/703  AvgLoss: 1.1370  Time: 16.1s\n",
      "Epoch 62 Batch 100/703  AvgLoss: 1.1852  Time: 30.4s\n",
      "Epoch 62 Batch 150/703  AvgLoss: 1.1867  Time: 44.8s\n",
      "Epoch 62 Batch 200/703  AvgLoss: 1.1849  Time: 59.6s\n",
      "Epoch 62 Batch 250/703  AvgLoss: 1.1757  Time: 73.9s\n",
      "Epoch 62 Batch 300/703  AvgLoss: 1.1666  Time: 88.4s\n",
      "Epoch 62 Batch 350/703  AvgLoss: 1.1498  Time: 102.8s\n",
      "Epoch 62 Batch 400/703  AvgLoss: 1.1589  Time: 117.1s\n",
      "Epoch 62 Batch 450/703  AvgLoss: 1.1655  Time: 131.8s\n",
      "Epoch 62 Batch 500/703  AvgLoss: 1.1657  Time: 146.2s\n",
      "Epoch 62 Batch 550/703  AvgLoss: 1.1569  Time: 160.7s\n",
      "Epoch 62 Batch 600/703  AvgLoss: 1.1517  Time: 175.1s\n",
      "Epoch 62 Batch 650/703  AvgLoss: 1.1508  Time: 189.5s\n",
      "Epoch 62 Batch 700/703  AvgLoss: 1.1540  Time: 204.0s\n",
      "Epoch 62 Batch 703/703  AvgLoss: 1.1549  Time: 204.8s\n",
      "Epoch 62 VALID -> Loss: 0.6304  Top1: 86.300  Top5: 96.920\n",
      "\n",
      "Epoch 63/120 - lr: 0.012671 - starting training...\n",
      "Epoch 63 Batch 50/703  AvgLoss: 1.0061  Time: 16.2s\n",
      "Epoch 63 Batch 100/703  AvgLoss: 1.0514  Time: 30.8s\n",
      "Epoch 63 Batch 150/703  AvgLoss: 1.0775  Time: 45.2s\n",
      "Epoch 63 Batch 200/703  AvgLoss: 1.0829  Time: 59.6s\n",
      "Epoch 63 Batch 250/703  AvgLoss: 1.0744  Time: 74.0s\n",
      "Epoch 63 Batch 300/703  AvgLoss: 1.0681  Time: 88.2s\n",
      "Epoch 63 Batch 350/703  AvgLoss: 1.0565  Time: 102.8s\n",
      "Epoch 63 Batch 400/703  AvgLoss: 1.0613  Time: 117.2s\n",
      "Epoch 63 Batch 450/703  AvgLoss: 1.0674  Time: 131.5s\n",
      "Epoch 63 Batch 500/703  AvgLoss: 1.0644  Time: 146.1s\n",
      "Epoch 63 Batch 550/703  AvgLoss: 1.0654  Time: 160.7s\n",
      "Epoch 63 Batch 600/703  AvgLoss: 1.0636  Time: 175.1s\n",
      "Epoch 63 Batch 650/703  AvgLoss: 1.0692  Time: 189.5s\n",
      "Epoch 63 Batch 700/703  AvgLoss: 1.0816  Time: 203.8s\n",
      "Epoch 63 Batch 703/703  AvgLoss: 1.0816  Time: 204.6s\n",
      "Epoch 63 VALID -> Loss: 0.6332  Top1: 86.320  Top5: 96.920\n",
      "\n",
      "Epoch 64/120 - lr: 0.012330 - starting training...\n",
      "Epoch 64 Batch 50/703  AvgLoss: 1.1367  Time: 16.1s\n",
      "Epoch 64 Batch 100/703  AvgLoss: 1.1061  Time: 30.6s\n",
      "Epoch 64 Batch 150/703  AvgLoss: 1.0819  Time: 44.9s\n",
      "Epoch 64 Batch 200/703  AvgLoss: 1.1328  Time: 59.8s\n",
      "Epoch 64 Batch 250/703  AvgLoss: 1.1172  Time: 74.4s\n",
      "Epoch 64 Batch 300/703  AvgLoss: 1.1158  Time: 89.1s\n",
      "Epoch 64 Batch 350/703  AvgLoss: 1.1115  Time: 103.6s\n",
      "Epoch 64 Batch 400/703  AvgLoss: 1.1030  Time: 118.2s\n",
      "Epoch 64 Batch 450/703  AvgLoss: 1.1070  Time: 132.6s\n",
      "Epoch 64 Batch 500/703  AvgLoss: 1.1246  Time: 147.0s\n",
      "Epoch 64 Batch 550/703  AvgLoss: 1.1263  Time: 161.5s\n",
      "Epoch 64 Batch 600/703  AvgLoss: 1.1207  Time: 175.7s\n",
      "Epoch 64 Batch 650/703  AvgLoss: 1.1299  Time: 189.8s\n",
      "Epoch 64 Batch 700/703  AvgLoss: 1.1309  Time: 203.9s\n",
      "Epoch 64 Batch 703/703  AvgLoss: 1.1306  Time: 204.8s\n",
      "Epoch 64 VALID -> Loss: 0.6369  Top1: 86.220  Top5: 96.920\n",
      "\n",
      "Epoch 65/120 - lr: 0.011988 - starting training...\n",
      "Epoch 65 Batch 50/703  AvgLoss: 1.0710  Time: 15.9s\n",
      "Epoch 65 Batch 100/703  AvgLoss: 1.0705  Time: 30.2s\n",
      "Epoch 65 Batch 150/703  AvgLoss: 1.0729  Time: 44.8s\n",
      "Epoch 65 Batch 200/703  AvgLoss: 1.0913  Time: 59.4s\n",
      "Epoch 65 Batch 250/703  AvgLoss: 1.0881  Time: 73.7s\n",
      "Epoch 65 Batch 300/703  AvgLoss: 1.1045  Time: 88.1s\n",
      "Epoch 65 Batch 350/703  AvgLoss: 1.1108  Time: 102.5s\n",
      "Epoch 65 Batch 400/703  AvgLoss: 1.1241  Time: 117.2s\n",
      "Epoch 65 Batch 450/703  AvgLoss: 1.1213  Time: 131.7s\n",
      "Epoch 65 Batch 500/703  AvgLoss: 1.1226  Time: 146.3s\n",
      "Epoch 65 Batch 550/703  AvgLoss: 1.1157  Time: 160.7s\n",
      "Epoch 65 Batch 600/703  AvgLoss: 1.1157  Time: 174.9s\n",
      "Epoch 65 Batch 650/703  AvgLoss: 1.1192  Time: 189.3s\n",
      "Epoch 65 Batch 700/703  AvgLoss: 1.1194  Time: 203.7s\n",
      "Epoch 65 Batch 703/703  AvgLoss: 1.1206  Time: 204.5s\n",
      "Epoch 65 VALID -> Loss: 0.6399  Top1: 86.180  Top5: 96.780\n",
      "\n",
      "Epoch 66/120 - lr: 0.011648 - starting training...\n",
      "Epoch 66 Batch 50/703  AvgLoss: 1.1032  Time: 16.2s\n",
      "Epoch 66 Batch 100/703  AvgLoss: 1.0926  Time: 30.7s\n",
      "Epoch 66 Batch 150/703  AvgLoss: 1.0675  Time: 45.1s\n",
      "Epoch 66 Batch 200/703  AvgLoss: 1.0718  Time: 59.5s\n",
      "Epoch 66 Batch 250/703  AvgLoss: 1.0879  Time: 73.9s\n",
      "Epoch 66 Batch 300/703  AvgLoss: 1.0989  Time: 88.5s\n",
      "Epoch 66 Batch 350/703  AvgLoss: 1.1004  Time: 102.8s\n",
      "Epoch 66 Batch 400/703  AvgLoss: 1.1074  Time: 117.1s\n",
      "Epoch 66 Batch 450/703  AvgLoss: 1.1116  Time: 131.4s\n",
      "Epoch 66 Batch 500/703  AvgLoss: 1.1051  Time: 145.7s\n",
      "Epoch 66 Batch 550/703  AvgLoss: 1.1116  Time: 160.2s\n",
      "Epoch 66 Batch 600/703  AvgLoss: 1.1027  Time: 174.7s\n",
      "Epoch 66 Batch 650/703  AvgLoss: 1.1044  Time: 189.0s\n",
      "Epoch 66 Batch 700/703  AvgLoss: 1.1074  Time: 203.3s\n",
      "Epoch 66 Batch 703/703  AvgLoss: 1.1077  Time: 204.1s\n",
      "Epoch 66 VALID -> Loss: 0.6429  Top1: 86.040  Top5: 96.680\n",
      "\n",
      "Epoch 67/120 - lr: 0.011307 - starting training...\n",
      "Epoch 67 Batch 50/703  AvgLoss: 1.0293  Time: 16.3s\n",
      "Epoch 67 Batch 100/703  AvgLoss: 1.0086  Time: 30.9s\n",
      "Epoch 67 Batch 150/703  AvgLoss: 1.0217  Time: 45.6s\n",
      "Epoch 67 Batch 200/703  AvgLoss: 1.0511  Time: 60.2s\n",
      "Epoch 67 Batch 250/703  AvgLoss: 1.0809  Time: 74.8s\n",
      "Epoch 67 Batch 300/703  AvgLoss: 1.0834  Time: 89.2s\n",
      "Epoch 67 Batch 350/703  AvgLoss: 1.0872  Time: 103.8s\n",
      "Epoch 67 Batch 400/703  AvgLoss: 1.0883  Time: 118.3s\n",
      "Epoch 67 Batch 450/703  AvgLoss: 1.0848  Time: 133.0s\n",
      "Epoch 67 Batch 500/703  AvgLoss: 1.0803  Time: 147.6s\n",
      "Epoch 67 Batch 550/703  AvgLoss: 1.0826  Time: 161.8s\n",
      "Epoch 67 Batch 600/703  AvgLoss: 1.0737  Time: 176.1s\n",
      "Epoch 67 Batch 650/703  AvgLoss: 1.0770  Time: 190.2s\n",
      "Epoch 67 Batch 700/703  AvgLoss: 1.0821  Time: 204.4s\n",
      "Epoch 67 Batch 703/703  AvgLoss: 1.0788  Time: 205.3s\n",
      "Epoch 67 VALID -> Loss: 0.6455  Top1: 86.060  Top5: 96.640\n",
      "\n",
      "Epoch 68/120 - lr: 0.010968 - starting training...\n",
      "Epoch 68 Batch 50/703  AvgLoss: 0.9009  Time: 16.7s\n",
      "Epoch 68 Batch 100/703  AvgLoss: 0.9387  Time: 31.0s\n",
      "Epoch 68 Batch 150/703  AvgLoss: 0.9927  Time: 45.4s\n",
      "Epoch 68 Batch 200/703  AvgLoss: 1.0106  Time: 59.8s\n",
      "Epoch 68 Batch 250/703  AvgLoss: 1.0350  Time: 74.2s\n",
      "Epoch 68 Batch 300/703  AvgLoss: 1.0460  Time: 88.7s\n",
      "Epoch 68 Batch 350/703  AvgLoss: 1.0671  Time: 103.1s\n",
      "Epoch 68 Batch 400/703  AvgLoss: 1.0689  Time: 117.9s\n",
      "Epoch 68 Batch 450/703  AvgLoss: 1.0545  Time: 132.5s\n",
      "Epoch 68 Batch 500/703  AvgLoss: 1.0671  Time: 146.9s\n",
      "Epoch 68 Batch 550/703  AvgLoss: 1.0596  Time: 161.5s\n",
      "Epoch 68 Batch 600/703  AvgLoss: 1.0635  Time: 176.0s\n",
      "Epoch 68 Batch 650/703  AvgLoss: 1.0540  Time: 190.6s\n",
      "Epoch 68 Batch 700/703  AvgLoss: 1.0590  Time: 205.4s\n",
      "Epoch 68 Batch 703/703  AvgLoss: 1.0593  Time: 206.2s\n",
      "Epoch 68 VALID -> Loss: 0.6484  Top1: 85.820  Top5: 96.620\n",
      "\n",
      "Epoch 69/120 - lr: 0.010630 - starting training...\n",
      "Epoch 69 Batch 50/703  AvgLoss: 1.0855  Time: 16.3s\n",
      "Epoch 69 Batch 100/703  AvgLoss: 1.0952  Time: 30.7s\n",
      "Epoch 69 Batch 150/703  AvgLoss: 1.0678  Time: 45.1s\n",
      "Epoch 69 Batch 200/703  AvgLoss: 1.0944  Time: 59.4s\n",
      "Epoch 69 Batch 250/703  AvgLoss: 1.0668  Time: 73.6s\n",
      "Epoch 69 Batch 300/703  AvgLoss: 1.0756  Time: 88.0s\n",
      "Epoch 69 Batch 350/703  AvgLoss: 1.0758  Time: 102.3s\n",
      "Epoch 69 Batch 400/703  AvgLoss: 1.0949  Time: 116.6s\n",
      "Epoch 69 Batch 450/703  AvgLoss: 1.0939  Time: 131.1s\n",
      "Epoch 69 Batch 500/703  AvgLoss: 1.0896  Time: 145.5s\n",
      "Epoch 69 Batch 550/703  AvgLoss: 1.0935  Time: 159.7s\n",
      "Epoch 69 Batch 600/703  AvgLoss: 1.0955  Time: 174.2s\n",
      "Epoch 69 Batch 650/703  AvgLoss: 1.0966  Time: 188.6s\n",
      "Epoch 69 Batch 700/703  AvgLoss: 1.0947  Time: 203.0s\n",
      "Epoch 69 Batch 703/703  AvgLoss: 1.0955  Time: 203.9s\n",
      "Epoch 69 VALID -> Loss: 0.6522  Top1: 85.760  Top5: 96.640\n",
      "\n",
      "Epoch 70/120 - lr: 0.010293 - starting training...\n",
      "Epoch 70 Batch 50/703  AvgLoss: 1.0756  Time: 15.9s\n",
      "Epoch 70 Batch 100/703  AvgLoss: 1.0763  Time: 30.3s\n",
      "Epoch 70 Batch 150/703  AvgLoss: 1.0614  Time: 44.7s\n",
      "Epoch 70 Batch 200/703  AvgLoss: 1.0508  Time: 59.3s\n",
      "Epoch 70 Batch 250/703  AvgLoss: 1.0430  Time: 73.9s\n",
      "Epoch 70 Batch 300/703  AvgLoss: 1.0466  Time: 88.2s\n",
      "Epoch 70 Batch 350/703  AvgLoss: 1.0372  Time: 102.5s\n",
      "Epoch 70 Batch 400/703  AvgLoss: 1.0298  Time: 116.8s\n",
      "Epoch 70 Batch 450/703  AvgLoss: 1.0390  Time: 130.9s\n",
      "Epoch 70 Batch 500/703  AvgLoss: 1.0392  Time: 145.7s\n",
      "Epoch 70 Batch 550/703  AvgLoss: 1.0267  Time: 160.2s\n",
      "Epoch 70 Batch 600/703  AvgLoss: 1.0352  Time: 174.8s\n",
      "Epoch 70 Batch 650/703  AvgLoss: 1.0379  Time: 189.1s\n",
      "Epoch 70 Batch 700/703  AvgLoss: 1.0401  Time: 203.5s\n",
      "Epoch 70 Batch 703/703  AvgLoss: 1.0400  Time: 204.4s\n",
      "Epoch 70 VALID -> Loss: 0.6546  Top1: 85.720  Top5: 96.560\n",
      "\n",
      "Epoch 71/120 - lr: 0.009957 - starting training...\n",
      "Epoch 71 Batch 50/703  AvgLoss: 1.1362  Time: 16.0s\n",
      "Epoch 71 Batch 100/703  AvgLoss: 1.0739  Time: 30.6s\n",
      "Epoch 71 Batch 150/703  AvgLoss: 1.0453  Time: 45.1s\n",
      "Epoch 71 Batch 200/703  AvgLoss: 1.0610  Time: 59.5s\n",
      "Epoch 71 Batch 250/703  AvgLoss: 1.0762  Time: 73.8s\n",
      "Epoch 71 Batch 300/703  AvgLoss: 1.0761  Time: 88.3s\n",
      "Epoch 71 Batch 350/703  AvgLoss: 1.0704  Time: 103.0s\n",
      "Epoch 71 Batch 400/703  AvgLoss: 1.0816  Time: 117.4s\n",
      "Epoch 71 Batch 450/703  AvgLoss: 1.0661  Time: 131.8s\n",
      "Epoch 71 Batch 500/703  AvgLoss: 1.0616  Time: 146.4s\n",
      "Epoch 71 Batch 550/703  AvgLoss: 1.0709  Time: 160.7s\n",
      "Epoch 71 Batch 600/703  AvgLoss: 1.0743  Time: 174.8s\n",
      "Epoch 71 Batch 650/703  AvgLoss: 1.0736  Time: 189.1s\n",
      "Epoch 71 Batch 700/703  AvgLoss: 1.0677  Time: 203.5s\n",
      "Epoch 71 Batch 703/703  AvgLoss: 1.0650  Time: 204.4s\n",
      "Epoch 71 VALID -> Loss: 0.6575  Top1: 85.660  Top5: 96.460\n",
      "\n",
      "Epoch 72/120 - lr: 0.009624 - starting training...\n",
      "Epoch 72 Batch 50/703  AvgLoss: 1.0424  Time: 16.0s\n",
      "Epoch 72 Batch 100/703  AvgLoss: 1.0266  Time: 30.3s\n",
      "Epoch 72 Batch 150/703  AvgLoss: 1.0079  Time: 44.7s\n",
      "Epoch 72 Batch 200/703  AvgLoss: 1.0091  Time: 59.2s\n",
      "Epoch 72 Batch 250/703  AvgLoss: 1.0056  Time: 73.9s\n",
      "Epoch 72 Batch 300/703  AvgLoss: 1.0276  Time: 88.3s\n",
      "Epoch 72 Batch 350/703  AvgLoss: 1.0324  Time: 102.7s\n",
      "Epoch 72 Batch 400/703  AvgLoss: 1.0311  Time: 117.3s\n",
      "Epoch 72 Batch 450/703  AvgLoss: 1.0488  Time: 131.7s\n",
      "Epoch 72 Batch 500/703  AvgLoss: 1.0477  Time: 146.0s\n",
      "Epoch 72 Batch 550/703  AvgLoss: 1.0458  Time: 160.3s\n",
      "Epoch 72 Batch 600/703  AvgLoss: 1.0481  Time: 175.0s\n",
      "Epoch 72 Batch 650/703  AvgLoss: 1.0524  Time: 189.5s\n",
      "Epoch 72 Batch 700/703  AvgLoss: 1.0572  Time: 203.8s\n",
      "Epoch 72 Batch 703/703  AvgLoss: 1.0573  Time: 204.6s\n",
      "Epoch 72 VALID -> Loss: 0.6606  Top1: 85.620  Top5: 96.480\n",
      "\n",
      "Epoch 73/120 - lr: 0.009293 - starting training...\n",
      "Epoch 73 Batch 50/703  AvgLoss: 1.0122  Time: 15.6s\n",
      "Epoch 73 Batch 100/703  AvgLoss: 1.0032  Time: 29.9s\n",
      "Epoch 73 Batch 150/703  AvgLoss: 1.0191  Time: 44.2s\n",
      "Epoch 73 Batch 200/703  AvgLoss: 1.0315  Time: 58.6s\n",
      "Epoch 73 Batch 250/703  AvgLoss: 1.0260  Time: 72.8s\n",
      "Epoch 73 Batch 300/703  AvgLoss: 1.0408  Time: 87.1s\n",
      "Epoch 73 Batch 350/703  AvgLoss: 1.0483  Time: 101.3s\n",
      "Epoch 73 Batch 400/703  AvgLoss: 1.0407  Time: 115.4s\n",
      "Epoch 73 Batch 450/703  AvgLoss: 1.0458  Time: 129.8s\n",
      "Epoch 73 Batch 500/703  AvgLoss: 1.0553  Time: 144.0s\n",
      "Epoch 73 Batch 550/703  AvgLoss: 1.0605  Time: 158.2s\n",
      "Epoch 73 Batch 600/703  AvgLoss: 1.0626  Time: 172.3s\n",
      "Epoch 73 Batch 650/703  AvgLoss: 1.0644  Time: 186.5s\n",
      "Epoch 73 Batch 700/703  AvgLoss: 1.0717  Time: 200.7s\n",
      "Epoch 73 Batch 703/703  AvgLoss: 1.0718  Time: 201.6s\n",
      "Epoch 73 VALID -> Loss: 0.6644  Top1: 85.640  Top5: 96.440\n",
      "\n",
      "Epoch 74/120 - lr: 0.008964 - starting training...\n",
      "Epoch 74 Batch 50/703  AvgLoss: 1.0429  Time: 16.1s\n",
      "Epoch 74 Batch 100/703  AvgLoss: 1.0253  Time: 30.3s\n",
      "Epoch 74 Batch 150/703  AvgLoss: 1.0300  Time: 44.5s\n",
      "Epoch 74 Batch 200/703  AvgLoss: 1.0079  Time: 58.7s\n",
      "Epoch 74 Batch 250/703  AvgLoss: 1.0178  Time: 73.2s\n",
      "Epoch 74 Batch 300/703  AvgLoss: 0.9943  Time: 87.8s\n",
      "Epoch 74 Batch 350/703  AvgLoss: 0.9944  Time: 102.5s\n",
      "Epoch 74 Batch 400/703  AvgLoss: 1.0000  Time: 117.0s\n",
      "Epoch 74 Batch 450/703  AvgLoss: 1.0017  Time: 131.4s\n",
      "Epoch 74 Batch 500/703  AvgLoss: 1.0095  Time: 145.6s\n",
      "Epoch 74 Batch 550/703  AvgLoss: 1.0164  Time: 159.9s\n",
      "Epoch 74 Batch 600/703  AvgLoss: 1.0149  Time: 174.6s\n",
      "Epoch 74 Batch 650/703  AvgLoss: 1.0184  Time: 189.1s\n",
      "Epoch 74 Batch 700/703  AvgLoss: 1.0264  Time: 203.7s\n",
      "Epoch 74 Batch 703/703  AvgLoss: 1.0264  Time: 204.6s\n",
      "Epoch 74 VALID -> Loss: 0.6678  Top1: 85.660  Top5: 96.400\n",
      "\n",
      "Epoch 75/120 - lr: 0.008638 - starting training...\n",
      "Epoch 75 Batch 50/703  AvgLoss: 1.0215  Time: 15.8s\n",
      "Epoch 75 Batch 100/703  AvgLoss: 0.9722  Time: 30.1s\n",
      "Epoch 75 Batch 150/703  AvgLoss: 1.0080  Time: 44.5s\n",
      "Epoch 75 Batch 200/703  AvgLoss: 1.0180  Time: 59.1s\n",
      "Epoch 75 Batch 250/703  AvgLoss: 1.0065  Time: 73.5s\n",
      "Epoch 75 Batch 300/703  AvgLoss: 1.0013  Time: 87.8s\n",
      "Epoch 75 Batch 350/703  AvgLoss: 1.0106  Time: 102.1s\n",
      "Epoch 75 Batch 400/703  AvgLoss: 1.0087  Time: 116.5s\n",
      "Epoch 75 Batch 450/703  AvgLoss: 1.0101  Time: 131.1s\n",
      "Epoch 75 Batch 500/703  AvgLoss: 1.0155  Time: 145.8s\n",
      "Epoch 75 Batch 550/703  AvgLoss: 1.0218  Time: 160.1s\n",
      "Epoch 75 Batch 600/703  AvgLoss: 1.0174  Time: 174.6s\n",
      "Epoch 75 Batch 650/703  AvgLoss: 1.0156  Time: 188.9s\n",
      "Epoch 75 Batch 700/703  AvgLoss: 1.0137  Time: 203.3s\n",
      "Epoch 75 Batch 703/703  AvgLoss: 1.0139  Time: 204.2s\n",
      "Epoch 75 VALID -> Loss: 0.6712  Top1: 85.600  Top5: 96.460\n",
      "\n",
      "Epoch 76/120 - lr: 0.008315 - starting training...\n",
      "Epoch 76 Batch 50/703  AvgLoss: 1.0493  Time: 16.2s\n",
      "Epoch 76 Batch 100/703  AvgLoss: 1.0197  Time: 30.5s\n",
      "Epoch 76 Batch 150/703  AvgLoss: 1.0073  Time: 44.7s\n",
      "Epoch 76 Batch 200/703  AvgLoss: 1.0495  Time: 59.0s\n",
      "Epoch 76 Batch 250/703  AvgLoss: 1.0331  Time: 73.2s\n",
      "Epoch 76 Batch 300/703  AvgLoss: 1.0393  Time: 87.6s\n",
      "Epoch 76 Batch 350/703  AvgLoss: 1.0225  Time: 102.3s\n",
      "Epoch 76 Batch 400/703  AvgLoss: 1.0100  Time: 116.8s\n",
      "Epoch 76 Batch 450/703  AvgLoss: 1.0128  Time: 131.4s\n",
      "Epoch 76 Batch 500/703  AvgLoss: 1.0034  Time: 145.9s\n",
      "Epoch 76 Batch 550/703  AvgLoss: 1.0110  Time: 160.5s\n",
      "Epoch 76 Batch 600/703  AvgLoss: 1.0074  Time: 174.6s\n",
      "Epoch 76 Batch 650/703  AvgLoss: 1.0087  Time: 189.2s\n",
      "Epoch 76 Batch 700/703  AvgLoss: 1.0098  Time: 203.4s\n",
      "Epoch 76 Batch 703/703  AvgLoss: 1.0104  Time: 204.3s\n",
      "Epoch 76 VALID -> Loss: 0.6744  Top1: 85.580  Top5: 96.380\n",
      "\n",
      "Epoch 77/120 - lr: 0.007995 - starting training...\n",
      "Epoch 77 Batch 50/703  AvgLoss: 1.0782  Time: 16.1s\n",
      "Epoch 77 Batch 100/703  AvgLoss: 1.0131  Time: 30.4s\n",
      "Epoch 77 Batch 150/703  AvgLoss: 0.9808  Time: 44.7s\n",
      "Epoch 77 Batch 200/703  AvgLoss: 0.9682  Time: 59.4s\n",
      "Epoch 77 Batch 250/703  AvgLoss: 0.9818  Time: 73.8s\n",
      "Epoch 77 Batch 300/703  AvgLoss: 0.9654  Time: 88.1s\n",
      "Epoch 77 Batch 350/703  AvgLoss: 0.9700  Time: 102.5s\n",
      "Epoch 77 Batch 400/703  AvgLoss: 0.9799  Time: 116.9s\n",
      "Epoch 77 Batch 450/703  AvgLoss: 0.9743  Time: 131.4s\n",
      "Epoch 77 Batch 500/703  AvgLoss: 0.9803  Time: 145.8s\n",
      "Epoch 77 Batch 550/703  AvgLoss: 0.9859  Time: 160.4s\n",
      "Epoch 77 Batch 600/703  AvgLoss: 0.9835  Time: 174.5s\n",
      "Epoch 77 Batch 650/703  AvgLoss: 0.9918  Time: 188.8s\n",
      "Epoch 77 Batch 700/703  AvgLoss: 0.9935  Time: 203.2s\n",
      "Epoch 77 Batch 703/703  AvgLoss: 0.9953  Time: 204.0s\n",
      "Epoch 77 VALID -> Loss: 0.6772  Top1: 85.540  Top5: 96.400\n",
      "\n",
      "Epoch 78/120 - lr: 0.007678 - starting training...\n",
      "Epoch 78 Batch 50/703  AvgLoss: 1.0014  Time: 15.9s\n",
      "Epoch 78 Batch 100/703  AvgLoss: 1.0379  Time: 30.4s\n",
      "Epoch 78 Batch 150/703  AvgLoss: 1.0171  Time: 45.2s\n",
      "Epoch 78 Batch 200/703  AvgLoss: 1.0301  Time: 59.5s\n",
      "Epoch 78 Batch 250/703  AvgLoss: 0.9969  Time: 73.8s\n",
      "Epoch 78 Batch 300/703  AvgLoss: 1.0052  Time: 88.3s\n",
      "Epoch 78 Batch 350/703  AvgLoss: 1.0150  Time: 102.8s\n",
      "Epoch 78 Batch 400/703  AvgLoss: 1.0275  Time: 117.4s\n",
      "Epoch 78 Batch 450/703  AvgLoss: 1.0226  Time: 131.8s\n",
      "Epoch 78 Batch 500/703  AvgLoss: 1.0271  Time: 146.1s\n",
      "Epoch 78 Batch 550/703  AvgLoss: 1.0278  Time: 160.4s\n",
      "Epoch 78 Batch 600/703  AvgLoss: 1.0294  Time: 174.8s\n",
      "Epoch 78 Batch 650/703  AvgLoss: 1.0303  Time: 189.2s\n",
      "Epoch 78 Batch 700/703  AvgLoss: 1.0325  Time: 203.9s\n",
      "Epoch 78 Batch 703/703  AvgLoss: 1.0324  Time: 204.7s\n",
      "Epoch 78 VALID -> Loss: 0.6808  Top1: 85.540  Top5: 96.360\n",
      "\n",
      "Epoch 79/120 - lr: 0.007365 - starting training...\n",
      "Epoch 79 Batch 50/703  AvgLoss: 0.9973  Time: 15.7s\n",
      "Epoch 79 Batch 100/703  AvgLoss: 1.0039  Time: 30.0s\n",
      "Epoch 79 Batch 150/703  AvgLoss: 1.0321  Time: 44.4s\n",
      "Epoch 79 Batch 200/703  AvgLoss: 1.0301  Time: 58.8s\n",
      "Epoch 79 Batch 250/703  AvgLoss: 1.0199  Time: 73.1s\n",
      "Epoch 79 Batch 300/703  AvgLoss: 1.0235  Time: 87.7s\n",
      "Epoch 79 Batch 350/703  AvgLoss: 1.0298  Time: 102.2s\n",
      "Epoch 79 Batch 400/703  AvgLoss: 1.0081  Time: 116.4s\n",
      "Epoch 79 Batch 450/703  AvgLoss: 1.0038  Time: 130.9s\n",
      "Epoch 79 Batch 500/703  AvgLoss: 1.0068  Time: 145.4s\n",
      "Epoch 79 Batch 550/703  AvgLoss: 0.9920  Time: 160.0s\n",
      "Epoch 79 Batch 600/703  AvgLoss: 0.9965  Time: 174.7s\n",
      "Epoch 79 Batch 650/703  AvgLoss: 1.0006  Time: 189.2s\n",
      "Epoch 79 Batch 700/703  AvgLoss: 1.0070  Time: 203.8s\n",
      "Epoch 79 Batch 703/703  AvgLoss: 1.0064  Time: 204.6s\n",
      "Epoch 79 VALID -> Loss: 0.6840  Top1: 85.520  Top5: 96.340\n",
      "\n",
      "Epoch 80/120 - lr: 0.007055 - starting training...\n",
      "Epoch 80 Batch 50/703  AvgLoss: 1.0163  Time: 16.0s\n",
      "Epoch 80 Batch 100/703  AvgLoss: 1.0150  Time: 30.3s\n",
      "Epoch 80 Batch 150/703  AvgLoss: 0.9564  Time: 44.7s\n",
      "Epoch 80 Batch 200/703  AvgLoss: 0.9811  Time: 59.1s\n",
      "Epoch 80 Batch 250/703  AvgLoss: 0.9939  Time: 73.8s\n",
      "Epoch 80 Batch 300/703  AvgLoss: 0.9924  Time: 88.4s\n",
      "Epoch 80 Batch 350/703  AvgLoss: 0.9854  Time: 103.0s\n",
      "Epoch 80 Batch 400/703  AvgLoss: 0.9819  Time: 117.5s\n",
      "Epoch 80 Batch 450/703  AvgLoss: 0.9818  Time: 132.1s\n",
      "Epoch 80 Batch 500/703  AvgLoss: 0.9840  Time: 146.3s\n",
      "Epoch 80 Batch 550/703  AvgLoss: 0.9816  Time: 160.8s\n",
      "Epoch 80 Batch 600/703  AvgLoss: 0.9831  Time: 175.4s\n",
      "Epoch 80 Batch 650/703  AvgLoss: 0.9854  Time: 189.8s\n",
      "Epoch 80 Batch 700/703  AvgLoss: 0.9774  Time: 204.4s\n",
      "Epoch 80 Batch 703/703  AvgLoss: 0.9761  Time: 205.3s\n",
      "Epoch 80 VALID -> Loss: 0.6865  Top1: 85.220  Top5: 96.360\n",
      "\n",
      "Epoch 81/120 - lr: 0.006750 - starting training...\n",
      "Epoch 81 Batch 50/703  AvgLoss: 0.9150  Time: 15.6s\n",
      "Epoch 81 Batch 100/703  AvgLoss: 0.8933  Time: 29.9s\n",
      "Epoch 81 Batch 150/703  AvgLoss: 0.9129  Time: 44.5s\n",
      "Epoch 81 Batch 200/703  AvgLoss: 0.9221  Time: 59.0s\n",
      "Epoch 81 Batch 250/703  AvgLoss: 0.9422  Time: 73.3s\n",
      "Epoch 81 Batch 300/703  AvgLoss: 0.9481  Time: 87.7s\n",
      "Epoch 81 Batch 350/703  AvgLoss: 0.9403  Time: 102.1s\n",
      "Epoch 81 Batch 400/703  AvgLoss: 0.9591  Time: 116.6s\n",
      "Epoch 81 Batch 450/703  AvgLoss: 0.9636  Time: 130.9s\n",
      "Epoch 81 Batch 500/703  AvgLoss: 0.9728  Time: 145.2s\n",
      "Epoch 81 Batch 550/703  AvgLoss: 0.9745  Time: 159.5s\n",
      "Epoch 81 Batch 600/703  AvgLoss: 0.9747  Time: 173.8s\n",
      "Epoch 81 Batch 650/703  AvgLoss: 0.9754  Time: 188.2s\n",
      "Epoch 81 Batch 700/703  AvgLoss: 0.9742  Time: 202.6s\n",
      "Epoch 81 Batch 703/703  AvgLoss: 0.9747  Time: 203.5s\n",
      "Epoch 81 VALID -> Loss: 0.6884  Top1: 85.260  Top5: 96.440\n",
      "\n",
      "Epoch 82/120 - lr: 0.006449 - starting training...\n",
      "Epoch 82 Batch 50/703  AvgLoss: 0.9761  Time: 16.2s\n",
      "Epoch 82 Batch 100/703  AvgLoss: 0.9978  Time: 30.4s\n",
      "Epoch 82 Batch 150/703  AvgLoss: 0.9894  Time: 44.6s\n",
      "Epoch 82 Batch 200/703  AvgLoss: 1.0041  Time: 58.7s\n",
      "Epoch 82 Batch 250/703  AvgLoss: 1.0131  Time: 73.2s\n",
      "Epoch 82 Batch 300/703  AvgLoss: 1.0190  Time: 87.8s\n",
      "Epoch 82 Batch 350/703  AvgLoss: 1.0227  Time: 102.4s\n",
      "Epoch 82 Batch 400/703  AvgLoss: 1.0053  Time: 116.8s\n",
      "Epoch 82 Batch 450/703  AvgLoss: 1.0094  Time: 131.4s\n",
      "Epoch 82 Batch 500/703  AvgLoss: 1.0016  Time: 146.0s\n",
      "Epoch 82 Batch 550/703  AvgLoss: 0.9949  Time: 160.6s\n",
      "Epoch 82 Batch 600/703  AvgLoss: 0.9924  Time: 175.4s\n",
      "Epoch 82 Batch 650/703  AvgLoss: 0.9910  Time: 189.8s\n",
      "Epoch 82 Batch 700/703  AvgLoss: 0.9909  Time: 204.1s\n",
      "Epoch 82 Batch 703/703  AvgLoss: 0.9909  Time: 205.0s\n",
      "Epoch 82 VALID -> Loss: 0.6912  Top1: 85.140  Top5: 96.380\n",
      "\n",
      "Epoch 83/120 - lr: 0.006152 - starting training...\n",
      "Epoch 83 Batch 50/703  AvgLoss: 0.8629  Time: 15.6s\n",
      "Epoch 83 Batch 100/703  AvgLoss: 0.8948  Time: 30.1s\n",
      "Epoch 83 Batch 150/703  AvgLoss: 0.8990  Time: 44.3s\n",
      "Epoch 83 Batch 200/703  AvgLoss: 0.9194  Time: 58.9s\n",
      "Epoch 83 Batch 250/703  AvgLoss: 0.9371  Time: 73.3s\n",
      "Epoch 83 Batch 300/703  AvgLoss: 0.9397  Time: 87.9s\n",
      "Epoch 83 Batch 350/703  AvgLoss: 0.9494  Time: 102.4s\n",
      "Epoch 83 Batch 400/703  AvgLoss: 0.9450  Time: 117.0s\n",
      "Epoch 83 Batch 450/703  AvgLoss: 0.9394  Time: 131.5s\n",
      "Epoch 83 Batch 500/703  AvgLoss: 0.9402  Time: 146.3s\n",
      "Epoch 83 Batch 550/703  AvgLoss: 0.9350  Time: 160.5s\n",
      "Epoch 83 Batch 600/703  AvgLoss: 0.9467  Time: 174.7s\n",
      "Epoch 83 Batch 650/703  AvgLoss: 0.9480  Time: 189.0s\n",
      "Epoch 83 Batch 700/703  AvgLoss: 0.9522  Time: 203.3s\n",
      "Epoch 83 Batch 703/703  AvgLoss: 0.9537  Time: 204.1s\n",
      "Epoch 83 VALID -> Loss: 0.6937  Top1: 85.100  Top5: 96.360\n",
      "\n",
      "Epoch 84/120 - lr: 0.005861 - starting training...\n",
      "Epoch 84 Batch 50/703  AvgLoss: 0.9639  Time: 16.5s\n",
      "Epoch 84 Batch 100/703  AvgLoss: 0.9277  Time: 31.0s\n",
      "Epoch 84 Batch 150/703  AvgLoss: 0.9002  Time: 45.5s\n",
      "Epoch 84 Batch 200/703  AvgLoss: 0.9238  Time: 59.9s\n",
      "Epoch 84 Batch 250/703  AvgLoss: 0.9293  Time: 74.4s\n",
      "Epoch 84 Batch 300/703  AvgLoss: 0.9491  Time: 89.0s\n",
      "Epoch 84 Batch 350/703  AvgLoss: 0.9574  Time: 103.4s\n",
      "Epoch 84 Batch 400/703  AvgLoss: 0.9546  Time: 117.6s\n",
      "Epoch 84 Batch 450/703  AvgLoss: 0.9559  Time: 132.0s\n",
      "Epoch 84 Batch 500/703  AvgLoss: 0.9672  Time: 146.5s\n",
      "Epoch 84 Batch 550/703  AvgLoss: 0.9704  Time: 160.9s\n",
      "Epoch 84 Batch 600/703  AvgLoss: 0.9682  Time: 175.4s\n",
      "Epoch 84 Batch 650/703  AvgLoss: 0.9596  Time: 190.1s\n",
      "Epoch 84 Batch 700/703  AvgLoss: 0.9592  Time: 204.5s\n",
      "Epoch 84 Batch 703/703  AvgLoss: 0.9597  Time: 205.4s\n",
      "Epoch 84 VALID -> Loss: 0.6960  Top1: 85.060  Top5: 96.360\n",
      "\n",
      "Epoch 85/120 - lr: 0.005574 - starting training...\n",
      "Epoch 85 Batch 50/703  AvgLoss: 1.0417  Time: 15.9s\n",
      "Epoch 85 Batch 100/703  AvgLoss: 0.9847  Time: 30.2s\n",
      "Epoch 85 Batch 150/703  AvgLoss: 0.9765  Time: 44.5s\n",
      "Epoch 85 Batch 200/703  AvgLoss: 0.9900  Time: 59.0s\n",
      "Epoch 85 Batch 250/703  AvgLoss: 0.9686  Time: 73.2s\n",
      "Epoch 85 Batch 300/703  AvgLoss: 0.9704  Time: 87.4s\n",
      "Epoch 85 Batch 350/703  AvgLoss: 0.9797  Time: 101.9s\n",
      "Epoch 85 Batch 400/703  AvgLoss: 0.9738  Time: 116.2s\n",
      "Epoch 85 Batch 450/703  AvgLoss: 0.9664  Time: 130.6s\n",
      "Epoch 85 Batch 500/703  AvgLoss: 0.9654  Time: 145.5s\n",
      "Epoch 85 Batch 550/703  AvgLoss: 0.9666  Time: 160.0s\n",
      "Epoch 85 Batch 600/703  AvgLoss: 0.9640  Time: 174.6s\n",
      "Epoch 85 Batch 650/703  AvgLoss: 0.9617  Time: 189.1s\n",
      "Epoch 85 Batch 700/703  AvgLoss: 0.9626  Time: 203.5s\n",
      "Epoch 85 Batch 703/703  AvgLoss: 0.9628  Time: 204.3s\n",
      "Epoch 85 VALID -> Loss: 0.6985  Top1: 84.960  Top5: 96.340\n",
      "\n",
      "Epoch 86/120 - lr: 0.005292 - starting training...\n",
      "Epoch 86 Batch 50/703  AvgLoss: 0.9473  Time: 15.4s\n",
      "Epoch 86 Batch 100/703  AvgLoss: 0.9143  Time: 30.0s\n",
      "Epoch 86 Batch 150/703  AvgLoss: 0.9155  Time: 44.1s\n",
      "Epoch 86 Batch 200/703  AvgLoss: 0.9237  Time: 58.3s\n",
      "Epoch 86 Batch 250/703  AvgLoss: 0.9409  Time: 72.4s\n",
      "Epoch 86 Batch 300/703  AvgLoss: 0.9464  Time: 86.8s\n",
      "Epoch 86 Batch 350/703  AvgLoss: 0.9499  Time: 101.1s\n",
      "Epoch 86 Batch 400/703  AvgLoss: 0.9546  Time: 115.5s\n",
      "Epoch 86 Batch 450/703  AvgLoss: 0.9618  Time: 129.8s\n",
      "Epoch 86 Batch 500/703  AvgLoss: 0.9652  Time: 144.1s\n",
      "Epoch 86 Batch 550/703  AvgLoss: 0.9627  Time: 158.4s\n",
      "Epoch 86 Batch 600/703  AvgLoss: 0.9662  Time: 172.6s\n",
      "Epoch 86 Batch 650/703  AvgLoss: 0.9587  Time: 186.7s\n",
      "Epoch 86 Batch 700/703  AvgLoss: 0.9566  Time: 201.1s\n",
      "Epoch 86 Batch 703/703  AvgLoss: 0.9573  Time: 202.0s\n",
      "Epoch 86 VALID -> Loss: 0.7016  Top1: 84.780  Top5: 96.360\n",
      "\n",
      "Epoch 87/120 - lr: 0.005016 - starting training...\n",
      "Epoch 87 Batch 50/703  AvgLoss: 0.9450  Time: 16.4s\n",
      "Epoch 87 Batch 100/703  AvgLoss: 0.9513  Time: 31.1s\n",
      "Epoch 87 Batch 150/703  AvgLoss: 0.9427  Time: 45.6s\n",
      "Epoch 87 Batch 200/703  AvgLoss: 0.9519  Time: 60.0s\n",
      "Epoch 87 Batch 250/703  AvgLoss: 0.9632  Time: 74.4s\n",
      "Epoch 87 Batch 300/703  AvgLoss: 0.9640  Time: 89.2s\n",
      "Epoch 87 Batch 350/703  AvgLoss: 0.9444  Time: 103.8s\n",
      "Epoch 87 Batch 400/703  AvgLoss: 0.9524  Time: 118.3s\n",
      "Epoch 87 Batch 450/703  AvgLoss: 0.9437  Time: 132.9s\n",
      "Epoch 87 Batch 500/703  AvgLoss: 0.9490  Time: 147.3s\n",
      "Epoch 87 Batch 550/703  AvgLoss: 0.9511  Time: 161.6s\n",
      "Epoch 87 Batch 600/703  AvgLoss: 0.9498  Time: 175.9s\n",
      "Epoch 87 Batch 650/703  AvgLoss: 0.9562  Time: 190.5s\n",
      "Epoch 87 Batch 700/703  AvgLoss: 0.9482  Time: 204.9s\n",
      "Epoch 87 Batch 703/703  AvgLoss: 0.9483  Time: 205.8s\n",
      "Epoch 87 VALID -> Loss: 0.7045  Top1: 84.800  Top5: 96.260\n",
      "\n",
      "Epoch 88/120 - lr: 0.004745 - starting training...\n",
      "Epoch 88 Batch 50/703  AvgLoss: 0.8716  Time: 16.0s\n",
      "Epoch 88 Batch 100/703  AvgLoss: 0.9024  Time: 30.5s\n",
      "Epoch 88 Batch 150/703  AvgLoss: 0.9410  Time: 44.8s\n",
      "Epoch 88 Batch 200/703  AvgLoss: 0.9489  Time: 59.1s\n",
      "Epoch 88 Batch 250/703  AvgLoss: 0.9434  Time: 73.7s\n",
      "Epoch 88 Batch 300/703  AvgLoss: 0.9294  Time: 87.9s\n",
      "Epoch 88 Batch 350/703  AvgLoss: 0.9193  Time: 102.3s\n",
      "Epoch 88 Batch 400/703  AvgLoss: 0.9168  Time: 116.5s\n",
      "Epoch 88 Batch 450/703  AvgLoss: 0.9239  Time: 130.7s\n",
      "Epoch 88 Batch 500/703  AvgLoss: 0.9220  Time: 144.9s\n",
      "Epoch 88 Batch 550/703  AvgLoss: 0.9155  Time: 159.4s\n",
      "Epoch 88 Batch 600/703  AvgLoss: 0.9087  Time: 173.8s\n",
      "Epoch 88 Batch 650/703  AvgLoss: 0.9172  Time: 188.2s\n",
      "Epoch 88 Batch 700/703  AvgLoss: 0.9149  Time: 202.5s\n",
      "Epoch 88 Batch 703/703  AvgLoss: 0.9161  Time: 203.4s\n",
      "Epoch 88 VALID -> Loss: 0.7074  Top1: 84.780  Top5: 96.200\n",
      "\n",
      "Epoch 89/120 - lr: 0.004481 - starting training...\n",
      "Epoch 89 Batch 50/703  AvgLoss: 0.9259  Time: 15.7s\n",
      "Epoch 89 Batch 100/703  AvgLoss: 0.8940  Time: 30.1s\n",
      "Epoch 89 Batch 150/703  AvgLoss: 0.9063  Time: 44.9s\n",
      "Epoch 89 Batch 200/703  AvgLoss: 0.9357  Time: 59.4s\n",
      "Epoch 89 Batch 250/703  AvgLoss: 0.9267  Time: 73.8s\n",
      "Epoch 89 Batch 300/703  AvgLoss: 0.9228  Time: 88.4s\n",
      "Epoch 89 Batch 350/703  AvgLoss: 0.9194  Time: 102.9s\n",
      "Epoch 89 Batch 400/703  AvgLoss: 0.9118  Time: 117.4s\n",
      "Epoch 89 Batch 450/703  AvgLoss: 0.9152  Time: 132.2s\n",
      "Epoch 89 Batch 500/703  AvgLoss: 0.9159  Time: 146.7s\n",
      "Epoch 89 Batch 550/703  AvgLoss: 0.9196  Time: 161.2s\n",
      "Epoch 89 Batch 600/703  AvgLoss: 0.9238  Time: 175.5s\n",
      "Epoch 89 Batch 650/703  AvgLoss: 0.9225  Time: 190.0s\n",
      "Epoch 89 Batch 700/703  AvgLoss: 0.9191  Time: 204.8s\n",
      "Epoch 89 Batch 703/703  AvgLoss: 0.9187  Time: 205.6s\n",
      "Epoch 89 VALID -> Loss: 0.7106  Top1: 84.720  Top5: 96.180\n",
      "\n",
      "Epoch 90/120 - lr: 0.004222 - starting training...\n",
      "Epoch 90 Batch 50/703  AvgLoss: 0.8824  Time: 15.9s\n",
      "Epoch 90 Batch 100/703  AvgLoss: 0.9140  Time: 30.4s\n",
      "Epoch 90 Batch 150/703  AvgLoss: 0.9351  Time: 44.8s\n",
      "Epoch 90 Batch 200/703  AvgLoss: 0.9579  Time: 59.1s\n",
      "Epoch 90 Batch 250/703  AvgLoss: 0.9453  Time: 73.5s\n",
      "Epoch 90 Batch 300/703  AvgLoss: 0.9372  Time: 88.1s\n",
      "Epoch 90 Batch 350/703  AvgLoss: 0.9283  Time: 102.7s\n",
      "Epoch 90 Batch 400/703  AvgLoss: 0.9435  Time: 117.0s\n",
      "Epoch 90 Batch 450/703  AvgLoss: 0.9495  Time: 131.3s\n",
      "Epoch 90 Batch 500/703  AvgLoss: 0.9342  Time: 145.6s\n",
      "Epoch 90 Batch 550/703  AvgLoss: 0.9344  Time: 160.1s\n",
      "Epoch 90 Batch 600/703  AvgLoss: 0.9325  Time: 174.5s\n",
      "Epoch 90 Batch 650/703  AvgLoss: 0.9275  Time: 189.4s\n",
      "Epoch 90 Batch 700/703  AvgLoss: 0.9210  Time: 203.9s\n",
      "Epoch 90 Batch 703/703  AvgLoss: 0.9198  Time: 204.7s\n",
      "Epoch 90 VALID -> Loss: 0.7143  Top1: 84.680  Top5: 96.080\n",
      "\n",
      "Epoch 91/120 - lr: 0.003969 - starting training...\n",
      "Epoch 91 Batch 50/703  AvgLoss: 0.8816  Time: 16.0s\n",
      "Epoch 91 Batch 100/703  AvgLoss: 0.8850  Time: 30.6s\n",
      "Epoch 91 Batch 150/703  AvgLoss: 0.8746  Time: 45.1s\n",
      "Epoch 91 Batch 200/703  AvgLoss: 0.8957  Time: 59.7s\n",
      "Epoch 91 Batch 250/703  AvgLoss: 0.9112  Time: 74.5s\n",
      "Epoch 91 Batch 300/703  AvgLoss: 0.9138  Time: 88.9s\n",
      "Epoch 91 Batch 350/703  AvgLoss: 0.9136  Time: 103.3s\n",
      "Epoch 91 Batch 400/703  AvgLoss: 0.9223  Time: 117.9s\n",
      "Epoch 91 Batch 450/703  AvgLoss: 0.9233  Time: 132.1s\n",
      "Epoch 91 Batch 500/703  AvgLoss: 0.9268  Time: 146.3s\n",
      "Epoch 91 Batch 550/703  AvgLoss: 0.9293  Time: 160.8s\n",
      "Epoch 91 Batch 600/703  AvgLoss: 0.9271  Time: 175.4s\n",
      "Epoch 91 Batch 650/703  AvgLoss: 0.9284  Time: 189.8s\n",
      "Epoch 91 Batch 700/703  AvgLoss: 0.9281  Time: 204.4s\n",
      "Epoch 91 Batch 703/703  AvgLoss: 0.9274  Time: 205.3s\n",
      "Epoch 91 VALID -> Loss: 0.7177  Top1: 84.460  Top5: 96.140\n",
      "\n",
      "Epoch 92/120 - lr: 0.003723 - starting training...\n",
      "Epoch 92 Batch 50/703  AvgLoss: 0.9568  Time: 16.0s\n",
      "Epoch 92 Batch 100/703  AvgLoss: 0.9128  Time: 30.5s\n",
      "Epoch 92 Batch 150/703  AvgLoss: 0.9271  Time: 45.3s\n",
      "Epoch 92 Batch 200/703  AvgLoss: 0.9294  Time: 59.5s\n",
      "Epoch 92 Batch 250/703  AvgLoss: 0.9329  Time: 73.9s\n",
      "Epoch 92 Batch 300/703  AvgLoss: 0.9409  Time: 88.4s\n",
      "Epoch 92 Batch 350/703  AvgLoss: 0.9330  Time: 102.9s\n",
      "Epoch 92 Batch 400/703  AvgLoss: 0.9267  Time: 117.2s\n",
      "Epoch 92 Batch 450/703  AvgLoss: 0.9209  Time: 131.7s\n",
      "Epoch 92 Batch 500/703  AvgLoss: 0.9150  Time: 146.1s\n",
      "Epoch 92 Batch 550/703  AvgLoss: 0.9149  Time: 160.5s\n",
      "Epoch 92 Batch 600/703  AvgLoss: 0.9122  Time: 174.8s\n",
      "Epoch 92 Batch 650/703  AvgLoss: 0.9074  Time: 189.4s\n",
      "Epoch 92 Batch 700/703  AvgLoss: 0.9021  Time: 203.9s\n",
      "Epoch 92 Batch 703/703  AvgLoss: 0.9022  Time: 204.8s\n",
      "Epoch 92 VALID -> Loss: 0.7206  Top1: 84.560  Top5: 96.080\n",
      "\n",
      "Epoch 93/120 - lr: 0.003483 - starting training...\n",
      "Epoch 93 Batch 50/703  AvgLoss: 0.9466  Time: 16.2s\n",
      "Epoch 93 Batch 100/703  AvgLoss: 0.9486  Time: 30.8s\n",
      "Epoch 93 Batch 150/703  AvgLoss: 0.9312  Time: 45.1s\n",
      "Epoch 93 Batch 200/703  AvgLoss: 0.9389  Time: 59.4s\n",
      "Epoch 93 Batch 250/703  AvgLoss: 0.9373  Time: 73.9s\n",
      "Epoch 93 Batch 300/703  AvgLoss: 0.9327  Time: 88.2s\n",
      "Epoch 93 Batch 350/703  AvgLoss: 0.9347  Time: 102.7s\n",
      "Epoch 93 Batch 400/703  AvgLoss: 0.9367  Time: 117.0s\n",
      "Epoch 93 Batch 450/703  AvgLoss: 0.9359  Time: 131.4s\n",
      "Epoch 93 Batch 500/703  AvgLoss: 0.9394  Time: 145.5s\n",
      "Epoch 93 Batch 550/703  AvgLoss: 0.9361  Time: 159.8s\n",
      "Epoch 93 Batch 600/703  AvgLoss: 0.9397  Time: 174.1s\n",
      "Epoch 93 Batch 650/703  AvgLoss: 0.9383  Time: 188.7s\n",
      "Epoch 93 Batch 700/703  AvgLoss: 0.9382  Time: 203.1s\n",
      "Epoch 93 Batch 703/703  AvgLoss: 0.9392  Time: 204.0s\n",
      "Epoch 93 VALID -> Loss: 0.7241  Top1: 84.400  Top5: 96.020\n",
      "\n",
      "Epoch 94/120 - lr: 0.003250 - starting training...\n",
      "Epoch 94 Batch 50/703  AvgLoss: 0.9298  Time: 15.9s\n",
      "Epoch 94 Batch 100/703  AvgLoss: 0.9451  Time: 30.6s\n",
      "Epoch 94 Batch 150/703  AvgLoss: 0.9577  Time: 45.0s\n",
      "Epoch 94 Batch 200/703  AvgLoss: 0.9613  Time: 59.3s\n",
      "Epoch 94 Batch 250/703  AvgLoss: 0.9206  Time: 73.7s\n",
      "Epoch 94 Batch 300/703  AvgLoss: 0.9217  Time: 87.8s\n",
      "Epoch 94 Batch 350/703  AvgLoss: 0.9098  Time: 102.0s\n",
      "Epoch 94 Batch 400/703  AvgLoss: 0.9017  Time: 116.3s\n",
      "Epoch 94 Batch 450/703  AvgLoss: 0.9127  Time: 130.6s\n",
      "Epoch 94 Batch 500/703  AvgLoss: 0.9124  Time: 145.1s\n",
      "Epoch 94 Batch 550/703  AvgLoss: 0.9018  Time: 159.7s\n",
      "Epoch 94 Batch 600/703  AvgLoss: 0.9015  Time: 174.5s\n",
      "Epoch 94 Batch 650/703  AvgLoss: 0.9018  Time: 189.0s\n",
      "Epoch 94 Batch 700/703  AvgLoss: 0.9066  Time: 203.4s\n",
      "Epoch 94 Batch 703/703  AvgLoss: 0.9081  Time: 204.2s\n",
      "Epoch 94 VALID -> Loss: 0.7273  Top1: 84.340  Top5: 95.940\n",
      "\n",
      "Epoch 95/120 - lr: 0.003024 - starting training...\n",
      "Epoch 95 Batch 50/703  AvgLoss: 0.8681  Time: 15.9s\n",
      "Epoch 95 Batch 100/703  AvgLoss: 0.8556  Time: 30.3s\n",
      "Epoch 95 Batch 150/703  AvgLoss: 0.8244  Time: 44.7s\n",
      "Epoch 95 Batch 200/703  AvgLoss: 0.8419  Time: 59.3s\n",
      "Epoch 95 Batch 250/703  AvgLoss: 0.8378  Time: 73.6s\n",
      "Epoch 95 Batch 300/703  AvgLoss: 0.8449  Time: 87.8s\n",
      "Epoch 95 Batch 350/703  AvgLoss: 0.8441  Time: 102.0s\n",
      "Epoch 95 Batch 400/703  AvgLoss: 0.8592  Time: 116.3s\n",
      "Epoch 95 Batch 450/703  AvgLoss: 0.8580  Time: 130.8s\n",
      "Epoch 95 Batch 500/703  AvgLoss: 0.8586  Time: 145.6s\n",
      "Epoch 95 Batch 550/703  AvgLoss: 0.8697  Time: 160.2s\n",
      "Epoch 95 Batch 600/703  AvgLoss: 0.8714  Time: 174.8s\n",
      "Epoch 95 Batch 650/703  AvgLoss: 0.8750  Time: 189.2s\n",
      "Epoch 95 Batch 700/703  AvgLoss: 0.8745  Time: 203.4s\n",
      "Epoch 95 Batch 703/703  AvgLoss: 0.8746  Time: 204.3s\n",
      "Epoch 95 VALID -> Loss: 0.7305  Top1: 84.260  Top5: 95.900\n",
      "\n",
      "Epoch 96/120 - lr: 0.002804 - starting training...\n",
      "Epoch 96 Batch 50/703  AvgLoss: 0.9405  Time: 15.8s\n",
      "Epoch 96 Batch 100/703  AvgLoss: 0.9259  Time: 30.3s\n",
      "Epoch 96 Batch 150/703  AvgLoss: 0.9172  Time: 44.7s\n",
      "Epoch 96 Batch 200/703  AvgLoss: 0.9029  Time: 59.0s\n",
      "Epoch 96 Batch 250/703  AvgLoss: 0.8974  Time: 73.3s\n",
      "Epoch 96 Batch 300/703  AvgLoss: 0.8935  Time: 87.6s\n",
      "Epoch 96 Batch 350/703  AvgLoss: 0.9082  Time: 102.2s\n",
      "Epoch 96 Batch 400/703  AvgLoss: 0.9059  Time: 117.0s\n",
      "Epoch 96 Batch 450/703  AvgLoss: 0.9079  Time: 131.4s\n",
      "Epoch 96 Batch 500/703  AvgLoss: 0.9032  Time: 145.8s\n",
      "Epoch 96 Batch 550/703  AvgLoss: 0.8965  Time: 160.3s\n",
      "Epoch 96 Batch 600/703  AvgLoss: 0.9028  Time: 174.8s\n",
      "Epoch 96 Batch 650/703  AvgLoss: 0.8930  Time: 189.1s\n",
      "Epoch 96 Batch 700/703  AvgLoss: 0.8964  Time: 203.5s\n",
      "Epoch 96 Batch 703/703  AvgLoss: 0.8967  Time: 204.3s\n",
      "Epoch 96 VALID -> Loss: 0.7343  Top1: 84.280  Top5: 95.920\n",
      "\n",
      "Epoch 97/120 - lr: 0.002593 - starting training...\n",
      "Epoch 97 Batch 50/703  AvgLoss: 0.9152  Time: 15.8s\n",
      "Epoch 97 Batch 100/703  AvgLoss: 0.8994  Time: 30.1s\n",
      "Epoch 97 Batch 150/703  AvgLoss: 0.9064  Time: 44.6s\n",
      "Epoch 97 Batch 200/703  AvgLoss: 0.9400  Time: 59.1s\n",
      "Epoch 97 Batch 250/703  AvgLoss: 0.9319  Time: 73.4s\n",
      "Epoch 97 Batch 300/703  AvgLoss: 0.9370  Time: 87.6s\n",
      "Epoch 97 Batch 350/703  AvgLoss: 0.9355  Time: 102.0s\n",
      "Epoch 97 Batch 400/703  AvgLoss: 0.9348  Time: 116.5s\n",
      "Epoch 97 Batch 450/703  AvgLoss: 0.9220  Time: 130.9s\n",
      "Epoch 97 Batch 500/703  AvgLoss: 0.9042  Time: 145.1s\n",
      "Epoch 97 Batch 550/703  AvgLoss: 0.9011  Time: 159.3s\n",
      "Epoch 97 Batch 600/703  AvgLoss: 0.8999  Time: 173.6s\n",
      "Epoch 97 Batch 650/703  AvgLoss: 0.9033  Time: 188.3s\n",
      "Epoch 97 Batch 700/703  AvgLoss: 0.9001  Time: 202.8s\n",
      "Epoch 97 Batch 703/703  AvgLoss: 0.9004  Time: 203.7s\n",
      "Epoch 97 VALID -> Loss: 0.7381  Top1: 84.240  Top5: 95.800\n",
      "\n",
      "Epoch 98/120 - lr: 0.002388 - starting training...\n",
      "Epoch 98 Batch 50/703  AvgLoss: 0.9229  Time: 16.2s\n",
      "Epoch 98 Batch 100/703  AvgLoss: 0.9302  Time: 30.8s\n",
      "Epoch 98 Batch 150/703  AvgLoss: 0.9113  Time: 45.1s\n",
      "Epoch 98 Batch 200/703  AvgLoss: 0.9175  Time: 59.5s\n",
      "Epoch 98 Batch 250/703  AvgLoss: 0.9154  Time: 74.0s\n",
      "Epoch 98 Batch 300/703  AvgLoss: 0.9041  Time: 88.6s\n",
      "Epoch 98 Batch 350/703  AvgLoss: 0.9031  Time: 103.0s\n",
      "Epoch 98 Batch 400/703  AvgLoss: 0.9059  Time: 117.7s\n",
      "Epoch 98 Batch 450/703  AvgLoss: 0.9013  Time: 132.3s\n",
      "Epoch 98 Batch 500/703  AvgLoss: 0.8936  Time: 146.9s\n",
      "Epoch 98 Batch 550/703  AvgLoss: 0.8907  Time: 161.5s\n",
      "Epoch 98 Batch 600/703  AvgLoss: 0.8848  Time: 176.1s\n",
      "Epoch 98 Batch 650/703  AvgLoss: 0.8843  Time: 190.6s\n",
      "Epoch 98 Batch 700/703  AvgLoss: 0.8948  Time: 205.1s\n",
      "Epoch 98 Batch 703/703  AvgLoss: 0.8959  Time: 205.9s\n",
      "Epoch 98 VALID -> Loss: 0.7420  Top1: 84.220  Top5: 95.740\n",
      "\n",
      "Epoch 99/120 - lr: 0.002191 - starting training...\n",
      "Epoch 99 Batch 50/703  AvgLoss: 0.8750  Time: 16.1s\n",
      "Epoch 99 Batch 100/703  AvgLoss: 0.8544  Time: 30.6s\n",
      "Epoch 99 Batch 150/703  AvgLoss: 0.8918  Time: 45.0s\n",
      "Epoch 99 Batch 200/703  AvgLoss: 0.9035  Time: 59.4s\n",
      "Epoch 99 Batch 250/703  AvgLoss: 0.8981  Time: 73.7s\n",
      "Epoch 99 Batch 300/703  AvgLoss: 0.8953  Time: 88.1s\n",
      "Epoch 99 Batch 350/703  AvgLoss: 0.9038  Time: 102.5s\n",
      "Epoch 99 Batch 400/703  AvgLoss: 0.9020  Time: 117.2s\n",
      "Epoch 99 Batch 450/703  AvgLoss: 0.9027  Time: 131.4s\n",
      "Epoch 99 Batch 500/703  AvgLoss: 0.9053  Time: 145.5s\n",
      "Epoch 99 Batch 550/703  AvgLoss: 0.8999  Time: 159.7s\n",
      "Epoch 99 Batch 600/703  AvgLoss: 0.8991  Time: 174.1s\n",
      "Epoch 99 Batch 650/703  AvgLoss: 0.8949  Time: 188.7s\n",
      "Epoch 99 Batch 700/703  AvgLoss: 0.8982  Time: 203.1s\n",
      "Epoch 99 Batch 703/703  AvgLoss: 0.8986  Time: 203.9s\n",
      "Epoch 99 VALID -> Loss: 0.7461  Top1: 84.020  Top5: 95.620\n",
      "\n",
      "Epoch 100/120 - lr: 0.002002 - starting training...\n",
      "Epoch 100 Batch 50/703  AvgLoss: 0.8553  Time: 15.9s\n",
      "Epoch 100 Batch 100/703  AvgLoss: 0.8276  Time: 30.3s\n",
      "Epoch 100 Batch 150/703  AvgLoss: 0.8579  Time: 44.7s\n",
      "Epoch 100 Batch 200/703  AvgLoss: 0.8425  Time: 59.0s\n",
      "Epoch 100 Batch 250/703  AvgLoss: 0.8322  Time: 73.5s\n",
      "Epoch 100 Batch 300/703  AvgLoss: 0.8201  Time: 87.8s\n",
      "Epoch 100 Batch 350/703  AvgLoss: 0.8103  Time: 102.2s\n",
      "Epoch 100 Batch 400/703  AvgLoss: 0.8175  Time: 116.4s\n",
      "Epoch 100 Batch 450/703  AvgLoss: 0.8168  Time: 130.9s\n",
      "Epoch 100 Batch 500/703  AvgLoss: 0.8215  Time: 145.4s\n",
      "Epoch 100 Batch 550/703  AvgLoss: 0.8313  Time: 160.0s\n",
      "Epoch 100 Batch 600/703  AvgLoss: 0.8333  Time: 174.2s\n",
      "Epoch 100 Batch 650/703  AvgLoss: 0.8321  Time: 188.3s\n",
      "Epoch 100 Batch 700/703  AvgLoss: 0.8387  Time: 202.4s\n",
      "Epoch 100 Batch 703/703  AvgLoss: 0.8380  Time: 203.3s\n",
      "Epoch 100 VALID -> Loss: 0.7493  Top1: 83.980  Top5: 95.600\n",
      "\n",
      "Epoch 101/120 - lr: 0.001821 - starting training...\n",
      "Epoch 101 Batch 50/703  AvgLoss: 0.8828  Time: 15.6s\n",
      "Epoch 101 Batch 100/703  AvgLoss: 0.9043  Time: 30.2s\n",
      "Epoch 101 Batch 150/703  AvgLoss: 0.9109  Time: 44.6s\n",
      "Epoch 101 Batch 200/703  AvgLoss: 0.9118  Time: 59.0s\n",
      "Epoch 101 Batch 250/703  AvgLoss: 0.9131  Time: 73.4s\n",
      "Epoch 101 Batch 300/703  AvgLoss: 0.9129  Time: 87.8s\n",
      "Epoch 101 Batch 350/703  AvgLoss: 0.9170  Time: 102.1s\n",
      "Epoch 101 Batch 400/703  AvgLoss: 0.9070  Time: 116.6s\n",
      "Epoch 101 Batch 450/703  AvgLoss: 0.8979  Time: 131.3s\n",
      "Epoch 101 Batch 500/703  AvgLoss: 0.8845  Time: 145.7s\n",
      "Epoch 101 Batch 550/703  AvgLoss: 0.8843  Time: 160.2s\n",
      "Epoch 101 Batch 600/703  AvgLoss: 0.8883  Time: 174.7s\n",
      "Epoch 101 Batch 650/703  AvgLoss: 0.8855  Time: 189.0s\n",
      "Epoch 101 Batch 700/703  AvgLoss: 0.8830  Time: 203.4s\n",
      "Epoch 101 Batch 703/703  AvgLoss: 0.8817  Time: 204.2s\n",
      "Epoch 101 VALID -> Loss: 0.7528  Top1: 83.920  Top5: 95.520\n",
      "\n",
      "Epoch 102/120 - lr: 0.001647 - starting training...\n",
      "Epoch 102 Batch 50/703  AvgLoss: 0.8672  Time: 15.7s\n",
      "Epoch 102 Batch 100/703  AvgLoss: 0.8834  Time: 30.0s\n",
      "Epoch 102 Batch 150/703  AvgLoss: 0.8740  Time: 44.4s\n",
      "Epoch 102 Batch 200/703  AvgLoss: 0.8886  Time: 58.8s\n",
      "Epoch 102 Batch 250/703  AvgLoss: 0.8889  Time: 73.0s\n",
      "Epoch 102 Batch 300/703  AvgLoss: 0.8683  Time: 87.5s\n",
      "Epoch 102 Batch 350/703  AvgLoss: 0.8801  Time: 102.1s\n",
      "Epoch 102 Batch 400/703  AvgLoss: 0.8836  Time: 116.6s\n",
      "Epoch 102 Batch 450/703  AvgLoss: 0.8764  Time: 131.1s\n",
      "Epoch 102 Batch 500/703  AvgLoss: 0.8758  Time: 145.4s\n",
      "Epoch 102 Batch 550/703  AvgLoss: 0.8775  Time: 159.7s\n",
      "Epoch 102 Batch 600/703  AvgLoss: 0.8762  Time: 174.0s\n",
      "Epoch 102 Batch 650/703  AvgLoss: 0.8761  Time: 188.8s\n",
      "Epoch 102 Batch 700/703  AvgLoss: 0.8789  Time: 203.1s\n",
      "Epoch 102 Batch 703/703  AvgLoss: 0.8798  Time: 204.0s\n",
      "Epoch 102 VALID -> Loss: 0.7564  Top1: 83.800  Top5: 95.380\n",
      "\n",
      "Epoch 103/120 - lr: 0.001482 - starting training...\n",
      "Epoch 103 Batch 50/703  AvgLoss: 0.8957  Time: 15.8s\n",
      "Epoch 103 Batch 100/703  AvgLoss: 0.8748  Time: 30.2s\n",
      "Epoch 103 Batch 150/703  AvgLoss: 0.8410  Time: 44.7s\n",
      "Epoch 103 Batch 200/703  AvgLoss: 0.8486  Time: 59.2s\n",
      "Epoch 103 Batch 250/703  AvgLoss: 0.8502  Time: 73.6s\n",
      "Epoch 103 Batch 300/703  AvgLoss: 0.8584  Time: 88.1s\n",
      "Epoch 103 Batch 350/703  AvgLoss: 0.8653  Time: 102.6s\n",
      "Epoch 103 Batch 400/703  AvgLoss: 0.8529  Time: 117.1s\n",
      "Epoch 103 Batch 450/703  AvgLoss: 0.8563  Time: 131.5s\n",
      "Epoch 103 Batch 500/703  AvgLoss: 0.8611  Time: 145.8s\n",
      "Epoch 103 Batch 550/703  AvgLoss: 0.8608  Time: 160.3s\n",
      "Epoch 103 Batch 600/703  AvgLoss: 0.8628  Time: 174.8s\n",
      "Epoch 103 Batch 650/703  AvgLoss: 0.8642  Time: 189.4s\n",
      "Epoch 103 Batch 700/703  AvgLoss: 0.8650  Time: 203.8s\n",
      "Epoch 103 Batch 703/703  AvgLoss: 0.8650  Time: 204.6s\n",
      "Epoch 103 VALID -> Loss: 0.7598  Top1: 83.700  Top5: 95.340\n",
      "\n",
      "Epoch 104/120 - lr: 0.001325 - starting training...\n",
      "Epoch 104 Batch 50/703  AvgLoss: 0.7905  Time: 16.1s\n",
      "Epoch 104 Batch 100/703  AvgLoss: 0.8557  Time: 30.3s\n",
      "Epoch 104 Batch 150/703  AvgLoss: 0.8724  Time: 44.9s\n",
      "Epoch 104 Batch 200/703  AvgLoss: 0.8454  Time: 59.5s\n",
      "Epoch 104 Batch 250/703  AvgLoss: 0.8608  Time: 74.0s\n",
      "Epoch 104 Batch 300/703  AvgLoss: 0.8506  Time: 88.6s\n",
      "Epoch 104 Batch 350/703  AvgLoss: 0.8507  Time: 103.3s\n",
      "Epoch 104 Batch 400/703  AvgLoss: 0.8550  Time: 117.6s\n",
      "Epoch 104 Batch 450/703  AvgLoss: 0.8535  Time: 131.8s\n",
      "Epoch 104 Batch 500/703  AvgLoss: 0.8471  Time: 146.3s\n",
      "Epoch 104 Batch 550/703  AvgLoss: 0.8556  Time: 160.8s\n",
      "Epoch 104 Batch 600/703  AvgLoss: 0.8645  Time: 175.4s\n",
      "Epoch 104 Batch 650/703  AvgLoss: 0.8598  Time: 190.0s\n",
      "Epoch 104 Batch 700/703  AvgLoss: 0.8656  Time: 204.7s\n",
      "Epoch 104 Batch 703/703  AvgLoss: 0.8651  Time: 205.6s\n",
      "Epoch 104 VALID -> Loss: 0.7632  Top1: 83.760  Top5: 95.360\n",
      "\n",
      "Epoch 105/120 - lr: 0.001176 - starting training...\n",
      "Epoch 105 Batch 50/703  AvgLoss: 0.7904  Time: 15.5s\n",
      "Epoch 105 Batch 100/703  AvgLoss: 0.8454  Time: 29.8s\n",
      "Epoch 105 Batch 150/703  AvgLoss: 0.8164  Time: 44.1s\n",
      "Epoch 105 Batch 200/703  AvgLoss: 0.8318  Time: 58.3s\n",
      "Epoch 105 Batch 250/703  AvgLoss: 0.8384  Time: 72.7s\n",
      "Epoch 105 Batch 300/703  AvgLoss: 0.8434  Time: 87.3s\n",
      "Epoch 105 Batch 350/703  AvgLoss: 0.8462  Time: 101.6s\n",
      "Epoch 105 Batch 400/703  AvgLoss: 0.8487  Time: 115.7s\n",
      "Epoch 105 Batch 450/703  AvgLoss: 0.8536  Time: 130.1s\n",
      "Epoch 105 Batch 500/703  AvgLoss: 0.8546  Time: 144.5s\n",
      "Epoch 105 Batch 550/703  AvgLoss: 0.8512  Time: 158.8s\n",
      "Epoch 105 Batch 600/703  AvgLoss: 0.8560  Time: 173.3s\n",
      "Epoch 105 Batch 650/703  AvgLoss: 0.8546  Time: 187.6s\n",
      "Epoch 105 Batch 700/703  AvgLoss: 0.8530  Time: 202.0s\n",
      "Epoch 105 Batch 703/703  AvgLoss: 0.8532  Time: 202.9s\n",
      "Epoch 105 VALID -> Loss: 0.7663  Top1: 83.620  Top5: 95.400\n",
      "\n",
      "Epoch 106/120 - lr: 0.001036 - starting training...\n",
      "Epoch 106 Batch 50/703  AvgLoss: 0.8105  Time: 16.4s\n",
      "Epoch 106 Batch 100/703  AvgLoss: 0.8458  Time: 31.1s\n",
      "Epoch 106 Batch 150/703  AvgLoss: 0.8220  Time: 45.7s\n",
      "Epoch 106 Batch 200/703  AvgLoss: 0.8377  Time: 59.9s\n",
      "Epoch 106 Batch 250/703  AvgLoss: 0.8366  Time: 74.2s\n",
      "Epoch 106 Batch 300/703  AvgLoss: 0.8433  Time: 88.7s\n",
      "Epoch 106 Batch 350/703  AvgLoss: 0.8414  Time: 103.2s\n",
      "Epoch 106 Batch 400/703  AvgLoss: 0.8491  Time: 117.7s\n",
      "Epoch 106 Batch 450/703  AvgLoss: 0.8455  Time: 132.1s\n",
      "Epoch 106 Batch 500/703  AvgLoss: 0.8506  Time: 146.5s\n",
      "Epoch 106 Batch 550/703  AvgLoss: 0.8525  Time: 161.0s\n",
      "Epoch 106 Batch 600/703  AvgLoss: 0.8529  Time: 175.4s\n",
      "Epoch 106 Batch 650/703  AvgLoss: 0.8511  Time: 190.0s\n",
      "Epoch 106 Batch 700/703  AvgLoss: 0.8463  Time: 204.5s\n",
      "Epoch 106 Batch 703/703  AvgLoss: 0.8467  Time: 205.4s\n",
      "Epoch 106 VALID -> Loss: 0.7694  Top1: 83.580  Top5: 95.440\n",
      "\n",
      "Epoch 107/120 - lr: 0.000904 - starting training...\n",
      "Epoch 107 Batch 50/703  AvgLoss: 0.7421  Time: 15.9s\n",
      "Epoch 107 Batch 100/703  AvgLoss: 0.7917  Time: 30.4s\n",
      "Epoch 107 Batch 150/703  AvgLoss: 0.7989  Time: 44.9s\n",
      "Epoch 107 Batch 200/703  AvgLoss: 0.8141  Time: 59.4s\n",
      "Epoch 107 Batch 250/703  AvgLoss: 0.8260  Time: 73.7s\n",
      "Epoch 107 Batch 300/703  AvgLoss: 0.8359  Time: 88.1s\n",
      "Epoch 107 Batch 350/703  AvgLoss: 0.8388  Time: 102.9s\n",
      "Epoch 107 Batch 400/703  AvgLoss: 0.8421  Time: 117.3s\n",
      "Epoch 107 Batch 450/703  AvgLoss: 0.8431  Time: 131.9s\n",
      "Epoch 107 Batch 500/703  AvgLoss: 0.8553  Time: 146.3s\n",
      "Epoch 107 Batch 550/703  AvgLoss: 0.8574  Time: 160.5s\n",
      "Epoch 107 Batch 600/703  AvgLoss: 0.8603  Time: 174.9s\n",
      "Epoch 107 Batch 650/703  AvgLoss: 0.8608  Time: 189.2s\n",
      "Epoch 107 Batch 700/703  AvgLoss: 0.8602  Time: 203.4s\n",
      "Epoch 107 Batch 703/703  AvgLoss: 0.8606  Time: 204.3s\n",
      "Epoch 107 VALID -> Loss: 0.7731  Top1: 83.520  Top5: 95.420\n",
      "\n",
      "Epoch 108/120 - lr: 0.000781 - starting training...\n",
      "Epoch 108 Batch 50/703  AvgLoss: 0.8383  Time: 16.3s\n",
      "Epoch 108 Batch 100/703  AvgLoss: 0.8351  Time: 30.7s\n",
      "Epoch 108 Batch 150/703  AvgLoss: 0.8444  Time: 45.3s\n",
      "Epoch 108 Batch 200/703  AvgLoss: 0.8377  Time: 59.7s\n",
      "Epoch 108 Batch 250/703  AvgLoss: 0.8526  Time: 74.4s\n",
      "Epoch 108 Batch 300/703  AvgLoss: 0.8511  Time: 88.8s\n",
      "Epoch 108 Batch 350/703  AvgLoss: 0.8502  Time: 103.3s\n",
      "Epoch 108 Batch 400/703  AvgLoss: 0.8506  Time: 117.6s\n",
      "Epoch 108 Batch 450/703  AvgLoss: 0.8537  Time: 131.9s\n",
      "Epoch 108 Batch 500/703  AvgLoss: 0.8574  Time: 146.2s\n",
      "Epoch 108 Batch 550/703  AvgLoss: 0.8618  Time: 160.7s\n",
      "Epoch 108 Batch 600/703  AvgLoss: 0.8597  Time: 175.0s\n",
      "Epoch 108 Batch 650/703  AvgLoss: 0.8619  Time: 189.4s\n",
      "Epoch 108 Batch 700/703  AvgLoss: 0.8631  Time: 204.0s\n",
      "Epoch 108 Batch 703/703  AvgLoss: 0.8633  Time: 204.9s\n",
      "Epoch 108 VALID -> Loss: 0.7764  Top1: 83.340  Top5: 95.400\n",
      "\n",
      "Epoch 109/120 - lr: 0.000667 - starting training...\n",
      "Epoch 109 Batch 50/703  AvgLoss: 0.7918  Time: 15.8s\n",
      "Epoch 109 Batch 100/703  AvgLoss: 0.8214  Time: 30.6s\n",
      "Epoch 109 Batch 150/703  AvgLoss: 0.8195  Time: 45.0s\n",
      "Epoch 109 Batch 200/703  AvgLoss: 0.8255  Time: 59.6s\n",
      "Epoch 109 Batch 250/703  AvgLoss: 0.8255  Time: 74.2s\n",
      "Epoch 109 Batch 300/703  AvgLoss: 0.8168  Time: 88.5s\n",
      "Epoch 109 Batch 350/703  AvgLoss: 0.8180  Time: 102.8s\n",
      "Epoch 109 Batch 400/703  AvgLoss: 0.8310  Time: 117.4s\n",
      "Epoch 109 Batch 450/703  AvgLoss: 0.8303  Time: 132.1s\n",
      "Epoch 109 Batch 500/703  AvgLoss: 0.8407  Time: 146.6s\n",
      "Epoch 109 Batch 550/703  AvgLoss: 0.8477  Time: 161.0s\n",
      "Epoch 109 Batch 600/703  AvgLoss: 0.8480  Time: 175.4s\n",
      "Epoch 109 Batch 650/703  AvgLoss: 0.8464  Time: 189.7s\n",
      "Epoch 109 Batch 700/703  AvgLoss: 0.8513  Time: 204.0s\n",
      "Epoch 109 Batch 703/703  AvgLoss: 0.8510  Time: 204.9s\n",
      "Epoch 109 VALID -> Loss: 0.7795  Top1: 83.280  Top5: 95.380\n",
      "\n",
      "Epoch 110/120 - lr: 0.000561 - starting training...\n",
      "Epoch 110 Batch 50/703  AvgLoss: 0.9146  Time: 15.9s\n",
      "Epoch 110 Batch 100/703  AvgLoss: 0.8574  Time: 30.4s\n",
      "Epoch 110 Batch 150/703  AvgLoss: 0.8525  Time: 44.8s\n",
      "Epoch 110 Batch 200/703  AvgLoss: 0.8473  Time: 59.2s\n",
      "Epoch 110 Batch 250/703  AvgLoss: 0.8553  Time: 73.5s\n",
      "Epoch 110 Batch 300/703  AvgLoss: 0.8560  Time: 88.1s\n",
      "Epoch 110 Batch 350/703  AvgLoss: 0.8562  Time: 102.7s\n",
      "Epoch 110 Batch 400/703  AvgLoss: 0.8592  Time: 117.3s\n",
      "Epoch 110 Batch 450/703  AvgLoss: 0.8611  Time: 131.7s\n",
      "Epoch 110 Batch 500/703  AvgLoss: 0.8671  Time: 146.1s\n",
      "Epoch 110 Batch 550/703  AvgLoss: 0.8703  Time: 160.5s\n",
      "Epoch 110 Batch 600/703  AvgLoss: 0.8689  Time: 175.0s\n",
      "Epoch 110 Batch 650/703  AvgLoss: 0.8698  Time: 189.6s\n",
      "Epoch 110 Batch 700/703  AvgLoss: 0.8645  Time: 203.9s\n",
      "Epoch 110 Batch 703/703  AvgLoss: 0.8639  Time: 204.8s\n",
      "Epoch 110 VALID -> Loss: 0.7830  Top1: 83.180  Top5: 95.380\n",
      "\n",
      "Epoch 111/120 - lr: 0.000465 - starting training...\n",
      "Epoch 111 Batch 50/703  AvgLoss: 0.9274  Time: 15.7s\n",
      "Epoch 111 Batch 100/703  AvgLoss: 0.8498  Time: 30.0s\n",
      "Epoch 111 Batch 150/703  AvgLoss: 0.8779  Time: 44.3s\n",
      "Epoch 111 Batch 200/703  AvgLoss: 0.8887  Time: 58.6s\n",
      "Epoch 111 Batch 250/703  AvgLoss: 0.8782  Time: 73.0s\n",
      "Epoch 111 Batch 300/703  AvgLoss: 0.8783  Time: 87.3s\n",
      "Epoch 111 Batch 350/703  AvgLoss: 0.8855  Time: 101.7s\n",
      "Epoch 111 Batch 400/703  AvgLoss: 0.8750  Time: 116.0s\n",
      "Epoch 111 Batch 450/703  AvgLoss: 0.8678  Time: 130.4s\n",
      "Epoch 111 Batch 500/703  AvgLoss: 0.8668  Time: 144.8s\n",
      "Epoch 111 Batch 550/703  AvgLoss: 0.8688  Time: 159.3s\n",
      "Epoch 111 Batch 600/703  AvgLoss: 0.8728  Time: 173.7s\n",
      "Epoch 111 Batch 650/703  AvgLoss: 0.8665  Time: 188.3s\n",
      "Epoch 111 Batch 700/703  AvgLoss: 0.8628  Time: 202.8s\n",
      "Epoch 111 Batch 703/703  AvgLoss: 0.8634  Time: 203.7s\n",
      "Epoch 111 VALID -> Loss: 0.7859  Top1: 83.120  Top5: 95.380\n",
      "\n",
      "Epoch 112/120 - lr: 0.000377 - starting training...\n",
      "Epoch 112 Batch 50/703  AvgLoss: 0.7760  Time: 16.1s\n",
      "Epoch 112 Batch 100/703  AvgLoss: 0.7967  Time: 30.8s\n",
      "Epoch 112 Batch 150/703  AvgLoss: 0.8129  Time: 45.4s\n",
      "Epoch 112 Batch 200/703  AvgLoss: 0.8291  Time: 60.0s\n",
      "Epoch 112 Batch 250/703  AvgLoss: 0.8319  Time: 74.6s\n",
      "Epoch 112 Batch 300/703  AvgLoss: 0.8423  Time: 89.1s\n",
      "Epoch 112 Batch 350/703  AvgLoss: 0.8430  Time: 103.6s\n",
      "Epoch 112 Batch 400/703  AvgLoss: 0.8521  Time: 118.3s\n",
      "Epoch 112 Batch 450/703  AvgLoss: 0.8438  Time: 132.9s\n",
      "Epoch 112 Batch 500/703  AvgLoss: 0.8409  Time: 147.4s\n",
      "Epoch 112 Batch 550/703  AvgLoss: 0.8475  Time: 161.5s\n",
      "Epoch 112 Batch 600/703  AvgLoss: 0.8515  Time: 175.7s\n",
      "Epoch 112 Batch 650/703  AvgLoss: 0.8465  Time: 190.0s\n",
      "Epoch 112 Batch 700/703  AvgLoss: 0.8417  Time: 204.6s\n",
      "Epoch 112 Batch 703/703  AvgLoss: 0.8423  Time: 205.4s\n",
      "Epoch 112 VALID -> Loss: 0.7885  Top1: 83.120  Top5: 95.400\n",
      "\n",
      "Epoch 113/120 - lr: 0.000298 - starting training...\n",
      "Epoch 113 Batch 50/703  AvgLoss: 0.8281  Time: 15.7s\n",
      "Epoch 113 Batch 100/703  AvgLoss: 0.8531  Time: 30.3s\n",
      "Epoch 113 Batch 150/703  AvgLoss: 0.8614  Time: 44.8s\n",
      "Epoch 113 Batch 200/703  AvgLoss: 0.8466  Time: 59.4s\n",
      "Epoch 113 Batch 250/703  AvgLoss: 0.8203  Time: 74.0s\n",
      "Epoch 113 Batch 300/703  AvgLoss: 0.8199  Time: 88.7s\n",
      "Epoch 113 Batch 350/703  AvgLoss: 0.8205  Time: 103.3s\n",
      "Epoch 113 Batch 400/703  AvgLoss: 0.8201  Time: 117.8s\n",
      "Epoch 113 Batch 450/703  AvgLoss: 0.8122  Time: 131.9s\n",
      "Epoch 113 Batch 500/703  AvgLoss: 0.8093  Time: 146.2s\n",
      "Epoch 113 Batch 550/703  AvgLoss: 0.8153  Time: 160.4s\n",
      "Epoch 113 Batch 600/703  AvgLoss: 0.8190  Time: 174.7s\n",
      "Epoch 113 Batch 650/703  AvgLoss: 0.8246  Time: 188.9s\n",
      "Epoch 113 Batch 700/703  AvgLoss: 0.8244  Time: 203.0s\n",
      "Epoch 113 Batch 703/703  AvgLoss: 0.8255  Time: 203.9s\n",
      "Epoch 113 VALID -> Loss: 0.7907  Top1: 83.080  Top5: 95.320\n",
      "\n",
      "Epoch 114/120 - lr: 0.000229 - starting training...\n",
      "Epoch 114 Batch 50/703  AvgLoss: 0.8556  Time: 15.8s\n",
      "Epoch 114 Batch 100/703  AvgLoss: 0.8348  Time: 30.2s\n",
      "Epoch 114 Batch 150/703  AvgLoss: 0.8268  Time: 45.0s\n",
      "Epoch 114 Batch 200/703  AvgLoss: 0.8308  Time: 59.2s\n",
      "Epoch 114 Batch 250/703  AvgLoss: 0.8305  Time: 73.7s\n",
      "Epoch 114 Batch 300/703  AvgLoss: 0.8447  Time: 88.0s\n",
      "Epoch 114 Batch 350/703  AvgLoss: 0.8488  Time: 102.2s\n",
      "Epoch 114 Batch 400/703  AvgLoss: 0.8533  Time: 116.5s\n",
      "Epoch 114 Batch 450/703  AvgLoss: 0.8588  Time: 131.0s\n",
      "Epoch 114 Batch 500/703  AvgLoss: 0.8569  Time: 145.3s\n",
      "Epoch 114 Batch 550/703  AvgLoss: 0.8501  Time: 159.8s\n",
      "Epoch 114 Batch 600/703  AvgLoss: 0.8489  Time: 174.4s\n",
      "Epoch 114 Batch 650/703  AvgLoss: 0.8466  Time: 188.9s\n",
      "Epoch 114 Batch 700/703  AvgLoss: 0.8475  Time: 203.4s\n",
      "Epoch 114 Batch 703/703  AvgLoss: 0.8484  Time: 204.2s\n",
      "Epoch 114 VALID -> Loss: 0.7935  Top1: 82.820  Top5: 95.300\n",
      "\n",
      "Epoch 115/120 - lr: 0.000169 - starting training...\n",
      "Epoch 115 Batch 50/703  AvgLoss: 0.8566  Time: 16.8s\n",
      "Epoch 115 Batch 100/703  AvgLoss: 0.8506  Time: 31.2s\n",
      "Epoch 115 Batch 150/703  AvgLoss: 0.8326  Time: 45.5s\n",
      "Epoch 115 Batch 200/703  AvgLoss: 0.8563  Time: 59.9s\n",
      "Epoch 115 Batch 250/703  AvgLoss: 0.8531  Time: 74.3s\n",
      "Epoch 115 Batch 300/703  AvgLoss: 0.8524  Time: 88.8s\n",
      "Epoch 115 Batch 350/703  AvgLoss: 0.8489  Time: 103.2s\n",
      "Epoch 115 Batch 400/703  AvgLoss: 0.8389  Time: 117.5s\n",
      "Epoch 115 Batch 450/703  AvgLoss: 0.8417  Time: 131.8s\n",
      "Epoch 115 Batch 500/703  AvgLoss: 0.8437  Time: 146.1s\n",
      "Epoch 115 Batch 550/703  AvgLoss: 0.8388  Time: 160.7s\n",
      "Epoch 115 Batch 600/703  AvgLoss: 0.8341  Time: 175.5s\n",
      "Epoch 115 Batch 650/703  AvgLoss: 0.8353  Time: 190.1s\n",
      "Epoch 115 Batch 700/703  AvgLoss: 0.8286  Time: 204.6s\n",
      "Epoch 115 Batch 703/703  AvgLoss: 0.8295  Time: 205.5s\n",
      "Epoch 115 VALID -> Loss: 0.7961  Top1: 82.720  Top5: 95.280\n",
      "\n",
      "Epoch 116/120 - lr: 0.000117 - starting training...\n",
      "Epoch 116 Batch 50/703  AvgLoss: 0.8713  Time: 16.1s\n",
      "Epoch 116 Batch 100/703  AvgLoss: 0.8243  Time: 30.6s\n",
      "Epoch 116 Batch 150/703  AvgLoss: 0.8294  Time: 45.0s\n",
      "Epoch 116 Batch 200/703  AvgLoss: 0.8058  Time: 59.8s\n",
      "Epoch 116 Batch 250/703  AvgLoss: 0.8234  Time: 74.3s\n",
      "Epoch 116 Batch 300/703  AvgLoss: 0.8306  Time: 88.8s\n",
      "Epoch 116 Batch 350/703  AvgLoss: 0.8372  Time: 103.1s\n",
      "Epoch 116 Batch 400/703  AvgLoss: 0.8307  Time: 117.6s\n",
      "Epoch 116 Batch 450/703  AvgLoss: 0.8385  Time: 132.2s\n",
      "Epoch 116 Batch 500/703  AvgLoss: 0.8422  Time: 147.0s\n",
      "Epoch 116 Batch 550/703  AvgLoss: 0.8473  Time: 161.2s\n",
      "Epoch 116 Batch 600/703  AvgLoss: 0.8513  Time: 175.4s\n",
      "Epoch 116 Batch 650/703  AvgLoss: 0.8493  Time: 189.6s\n",
      "Epoch 116 Batch 700/703  AvgLoss: 0.8449  Time: 203.8s\n",
      "Epoch 116 Batch 703/703  AvgLoss: 0.8446  Time: 204.7s\n",
      "Epoch 116 VALID -> Loss: 0.7985  Top1: 82.660  Top5: 95.220\n",
      "\n",
      "Epoch 117/120 - lr: 0.000076 - starting training...\n",
      "Epoch 117 Batch 50/703  AvgLoss: 0.8226  Time: 15.8s\n",
      "Epoch 117 Batch 100/703  AvgLoss: 0.8043  Time: 30.3s\n",
      "Epoch 117 Batch 150/703  AvgLoss: 0.8241  Time: 44.6s\n",
      "Epoch 117 Batch 200/703  AvgLoss: 0.8273  Time: 59.1s\n",
      "Epoch 117 Batch 250/703  AvgLoss: 0.8293  Time: 73.6s\n",
      "Epoch 117 Batch 300/703  AvgLoss: 0.8359  Time: 87.9s\n",
      "Epoch 117 Batch 350/703  AvgLoss: 0.8356  Time: 102.0s\n",
      "Epoch 117 Batch 400/703  AvgLoss: 0.8365  Time: 116.3s\n",
      "Epoch 117 Batch 450/703  AvgLoss: 0.8378  Time: 131.0s\n",
      "Epoch 117 Batch 500/703  AvgLoss: 0.8413  Time: 145.6s\n",
      "Epoch 117 Batch 550/703  AvgLoss: 0.8382  Time: 160.2s\n",
      "Epoch 117 Batch 600/703  AvgLoss: 0.8404  Time: 174.7s\n",
      "Epoch 117 Batch 650/703  AvgLoss: 0.8410  Time: 189.1s\n",
      "Epoch 117 Batch 700/703  AvgLoss: 0.8481  Time: 203.6s\n",
      "Epoch 117 Batch 703/703  AvgLoss: 0.8470  Time: 204.5s\n",
      "Epoch 117 VALID -> Loss: 0.8009  Top1: 82.520  Top5: 95.100\n",
      "\n",
      "Epoch 118/120 - lr: 0.000043 - starting training...\n",
      "Epoch 118 Batch 50/703  AvgLoss: 0.8453  Time: 16.2s\n",
      "Epoch 118 Batch 100/703  AvgLoss: 0.8285  Time: 30.4s\n",
      "Epoch 118 Batch 150/703  AvgLoss: 0.8364  Time: 44.5s\n",
      "Epoch 118 Batch 200/703  AvgLoss: 0.8456  Time: 58.6s\n",
      "Epoch 118 Batch 250/703  AvgLoss: 0.8444  Time: 72.7s\n",
      "Epoch 118 Batch 300/703  AvgLoss: 0.8465  Time: 87.0s\n",
      "Epoch 118 Batch 350/703  AvgLoss: 0.8458  Time: 101.2s\n",
      "Epoch 118 Batch 400/703  AvgLoss: 0.8390  Time: 115.4s\n",
      "Epoch 118 Batch 450/703  AvgLoss: 0.8414  Time: 129.7s\n",
      "Epoch 118 Batch 500/703  AvgLoss: 0.8369  Time: 144.3s\n",
      "Epoch 118 Batch 550/703  AvgLoss: 0.8410  Time: 158.6s\n",
      "Epoch 118 Batch 600/703  AvgLoss: 0.8489  Time: 173.4s\n",
      "Epoch 118 Batch 650/703  AvgLoss: 0.8497  Time: 187.8s\n",
      "Epoch 118 Batch 700/703  AvgLoss: 0.8508  Time: 202.2s\n",
      "Epoch 118 Batch 703/703  AvgLoss: 0.8503  Time: 203.0s\n",
      "Epoch 118 VALID -> Loss: 0.8033  Top1: 82.460  Top5: 95.060\n",
      "\n",
      "Epoch 119/120 - lr: 0.000020 - starting training...\n",
      "Epoch 119 Batch 50/703  AvgLoss: 0.7979  Time: 15.7s\n",
      "Epoch 119 Batch 100/703  AvgLoss: 0.8349  Time: 29.9s\n",
      "Epoch 119 Batch 150/703  AvgLoss: 0.8438  Time: 44.3s\n",
      "Epoch 119 Batch 200/703  AvgLoss: 0.8423  Time: 59.0s\n",
      "Epoch 119 Batch 250/703  AvgLoss: 0.8483  Time: 73.6s\n",
      "Epoch 119 Batch 300/703  AvgLoss: 0.8492  Time: 88.1s\n",
      "Epoch 119 Batch 350/703  AvgLoss: 0.8521  Time: 102.5s\n",
      "Epoch 119 Batch 400/703  AvgLoss: 0.8647  Time: 117.0s\n",
      "Epoch 119 Batch 450/703  AvgLoss: 0.8652  Time: 131.4s\n",
      "Epoch 119 Batch 500/703  AvgLoss: 0.8662  Time: 146.2s\n",
      "Epoch 119 Batch 550/703  AvgLoss: 0.8717  Time: 160.6s\n",
      "Epoch 119 Batch 600/703  AvgLoss: 0.8699  Time: 175.1s\n",
      "Epoch 119 Batch 650/703  AvgLoss: 0.8704  Time: 189.6s\n",
      "Epoch 119 Batch 700/703  AvgLoss: 0.8737  Time: 203.8s\n",
      "Epoch 119 Batch 703/703  AvgLoss: 0.8734  Time: 204.7s\n",
      "Epoch 119 VALID -> Loss: 0.8060  Top1: 82.420  Top5: 94.960\n",
      "\n",
      "Epoch 120/120 - lr: 0.000006 - starting training...\n",
      "Epoch 120 Batch 50/703  AvgLoss: 0.8418  Time: 16.0s\n",
      "Epoch 120 Batch 100/703  AvgLoss: 0.8415  Time: 30.4s\n",
      "Epoch 120 Batch 150/703  AvgLoss: 0.8512  Time: 44.7s\n",
      "Epoch 120 Batch 200/703  AvgLoss: 0.8435  Time: 59.1s\n",
      "Epoch 120 Batch 250/703  AvgLoss: 0.8403  Time: 73.5s\n",
      "Epoch 120 Batch 300/703  AvgLoss: 0.8415  Time: 87.7s\n",
      "Epoch 120 Batch 350/703  AvgLoss: 0.8313  Time: 102.3s\n",
      "Epoch 120 Batch 400/703  AvgLoss: 0.8270  Time: 116.4s\n",
      "Epoch 120 Batch 450/703  AvgLoss: 0.8242  Time: 130.8s\n",
      "Epoch 120 Batch 500/703  AvgLoss: 0.8307  Time: 145.4s\n",
      "Epoch 120 Batch 550/703  AvgLoss: 0.8272  Time: 159.8s\n",
      "Epoch 120 Batch 600/703  AvgLoss: 0.8189  Time: 174.1s\n",
      "Epoch 120 Batch 650/703  AvgLoss: 0.8271  Time: 188.2s\n",
      "Epoch 120 Batch 700/703  AvgLoss: 0.8271  Time: 202.6s\n",
      "Epoch 120 Batch 703/703  AvgLoss: 0.8277  Time: 203.4s\n",
      "Epoch 120 VALID -> Loss: 0.8078  Top1: 82.320  Top5: 94.920\n",
      "Training finished. Best val top1: 86.84\n"
     ]
    }
   ],
   "source": [
    "best_val_top1 = 0.0\n",
    "min_lr = 1e-6\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # ---- compute LR for this epoch (linear warmup then cosine anneal) ----\n",
    "    if epoch < WARMUP_EPOCHS:\n",
    "        cur_lr = lr * float(epoch + 1) / float(max(1, WARMUP_EPOCHS))\n",
    "    else:\n",
    "        t = float(epoch - WARMUP_EPOCHS) / float(max(1, EPOCHS - WARMUP_EPOCHS))\n",
    "        cur_lr = min_lr + 0.5 * (lr - min_lr) * (1.0 + math.cos(math.pi * t))\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = cur_lr\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS} - lr: {cur_lr:.6f} - starting training...\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    seen = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    pbar = enumerate(train_loader)\n",
    "    for i, (imgs, targets) in pbar:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        # ----- MIXUP inline (or fall back to label smoothed one-hot) -----\n",
    "        if MIXUP_ALPHA > 0:\n",
    "            lam = np.random.beta(MIXUP_ALPHA, MIXUP_ALPHA)\n",
    "            idx = torch.randperm(imgs.size(0)).to(device)\n",
    "            imgs_mixed = lam * imgs + (1.0 - lam) * imgs[idx]\n",
    "            # build soft targets\n",
    "            y_a = one_hot_smooth(targets, NUM_CLASSES, 0.0, device)      # no smoothing in mixup construction\n",
    "            y_b = one_hot_smooth(targets[idx], NUM_CLASSES, 0.0, device)\n",
    "            soft_targets = lam * y_a + (1.0 - lam) * y_b\n",
    "        else:\n",
    "            imgs_mixed = imgs\n",
    "            soft_targets = one_hot_smooth(targets, NUM_CLASSES, LABEL_SMOOTHING, device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward with AMP\n",
    "        with torch.cuda.amp.autocast(enabled=(USE_AMP and device==\"cuda\")):\n",
    "            logits = model(imgs_mixed)\n",
    "            log_probs = F.log_softmax(logits, dim=1)\n",
    "            loss = -(soft_targets * log_probs).sum(dim=1).mean()\n",
    "\n",
    "        if USE_AMP and device==\"cuda\":\n",
    "            scaler.scale(loss).backward()\n",
    "            if GRAD_CLIP is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            if GRAD_CLIP is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "            optimizer.step()\n",
    "\n",
    "        # EMA update inline (if enabled)\n",
    "        if ema_model is not None:\n",
    "            with torch.no_grad():\n",
    "                decay = EMA_DECAY\n",
    "                msd = model.state_dict()\n",
    "                esd = ema_model.state_dict()\n",
    "                for k in esd.keys():\n",
    "                    target = esd[k]\n",
    "                    source = msd[k].to(target.device)\n",
    "                    # Only do EMA math for floating tensors; for integer/bool buffers just copy\n",
    "                    if target.dtype.is_floating_point:\n",
    "                        # ensure same dtype then ema update\n",
    "                        source = source.type_as(target)\n",
    "                        target.mul_(decay).add_(source, alpha=(1.0 - decay))\n",
    "                    else:\n",
    "                        # direct copy for non-float tensors (e.g., num_batches_tracked)\n",
    "                        target.copy_(source)\n",
    "\n",
    "        bs = imgs.size(0)\n",
    "        running_loss += loss.item() * bs\n",
    "        seen += bs\n",
    "\n",
    "        if (i + 1) % PRINT_FREQ == 0 or (i+1) == len(train_loader):\n",
    "            avg_loss = running_loss / seen\n",
    "            elapsed = time.time() - t0\n",
    "            print(f\"Epoch {epoch+1} Batch {i+1}/{len(train_loader)}  AvgLoss: {avg_loss:.4f}  Time: {elapsed:.1f}s\")\n",
    "\n",
    "    # ---- end of epoch: validation using EMA model if exists, else student model ----\n",
    "    eval_model = ema_model if (ema_model is not None) else model\n",
    "    eval_model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    total = 0\n",
    "    top1_count = 0\n",
    "    top5_count = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in val_loader:\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            logits = eval_model(imgs)\n",
    "            loss = F.cross_entropy(logits, targets)   # hard-label loss for reporting\n",
    "            bs = imgs.size(0)\n",
    "            val_running_loss += loss.item() * bs\n",
    "            total += bs\n",
    "            # top-k\n",
    "            _, pred = logits.topk(5, dim=1, largest=True, sorted=True)\n",
    "            correct = pred.eq(targets.view(-1,1).expand_as(pred))\n",
    "            top1_count += correct[:, :1].reshape(-1).float().sum().item()\n",
    "            top5_count += correct[:, :5].reshape(-1).float().sum().item()\n",
    "\n",
    "    val_avg_loss = val_running_loss / total\n",
    "    val_top1 = 100.0 * top1_count / total\n",
    "    val_top5 = 100.0 * top5_count / total\n",
    "    print(f\"Epoch {epoch+1} VALID -> Loss: {val_avg_loss:.4f}  Top1: {val_top1:.3f}  Top5: {val_top5:.3f}\")\n",
    "\n",
    "    # Save checkpoint (student weights; also keep best)\n",
    "    ckpt = {\n",
    "        \"epoch\": epoch+1,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"val_top1\": val_top1,\n",
    "        \"cfg\": {\n",
    "            \"MODEL_NAME\": MODEL_NAME, \"IMG_SIZE\": IMG_SIZE, \"BATCH_SIZE\": BATCH_SIZE\n",
    "        }\n",
    "    }\n",
    "    if epoch%5==0:\n",
    "        torch.save(ckpt, os.path.join(SAVE_DIR, f\"checkpoint_epoch_{epoch+1}.pth\"))\n",
    "    if val_top1 > best_val_top1:\n",
    "        best_val_top1 = val_top1\n",
    "        torch.save(ckpt, os.path.join(SAVE_DIR, \"best_checkpoint.pth\"))\n",
    "        print(\"Saved new best checkpoint.\")\n",
    "\n",
    "# End of epoch loop\n",
    "print(\"Training finished. Best val top1:\", best_val_top1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "302b23f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test -> Loss: 0.7919  Top1: 82.200  Top5: 95.200\n"
     ]
    }
   ],
   "source": [
    "final_model = ema_model if (ema_model is not None) else model\n",
    "final_model.eval()\n",
    "test_running_loss = 0.0\n",
    "test_total = 0\n",
    "test_top1 = 0\n",
    "test_top5 = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, targets in test_loader:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        logits = final_model(imgs)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        bs = imgs.size(0)\n",
    "        test_running_loss += loss.item() * bs\n",
    "        test_total += bs\n",
    "        _, pred = logits.topk(5, dim=1, largest=True, sorted=True)\n",
    "        correct = pred.eq(targets.view(-1,1).expand_as(pred))\n",
    "        test_top1 += correct[:, :1].reshape(-1).float().sum().item()\n",
    "        test_top5 += correct[:, :5].reshape(-1).float().sum().item()\n",
    "\n",
    "test_loss = test_running_loss / test_total\n",
    "test_top1 = 100.0 * test_top1 / test_total\n",
    "test_top5 = 100.0 * test_top5 / test_total\n",
    "print(f\"Final Test -> Loss: {test_loss:.4f}  Top1: {test_top1:.3f}  Top5: {test_top5:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
